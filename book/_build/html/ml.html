
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Maximum Likelihood &#8212; Data science and AI for Bio/medical applications using python</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Linear separable models" href="linearSeparable.html" />
    <link rel="prev" title="Logistic regression" href="logistic.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/dasl.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data science and AI for Bio/medical applications using python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome!
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="ds_python.html">
   Python background
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="basic_python.html">
   Python basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="python_programming.html">
   Python programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="functions.html">
   Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="python_practice.html">
   Python in practice
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data_cleaning.html">
   Data cleaning by example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="EDA.html">
   Exploratory data analysis in Seaborn
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="binary_classification.html">
   Introduction to binary classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regression_through_the_origin.html">
   Regression through the origin
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regression.html">
   Continuous prediction with regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="logistic.html">
   Logistic regression
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Maximum Likelihood
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linearSeparable.html">
   Linear separable models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linearSeparableSMF.html">
   Interpretation of linear regression coefficients.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regression_examples.html">
   Linear models: a classic example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regression_interpretation.html">
   Regression interpretation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dft.html">
   DFT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linearModels_FFTs.html">
   Regression and FFTs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="basic_regression_pytorch.html">
   Basic regression in pytorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="logistic_regression_pytorch.html">
   Logistic regression in pytorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pytorch_regression.html">
   Pytorch by example, linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="convnet_classifier_pytorch.html">
   Convnet example
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/ml.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/smart-stats/ds4bio_book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/smart-stats/ds4bio_book/issues/new?title=Issue%20on%20page%20%2Fml.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-regression">
   Linear regression
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p><a href="https://colab.research.google.com/github/smart-stats/ds4bio_book/blob/main/book/ml.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<div class="section" id="maximum-likelihood">
<h1>Maximum Likelihood<a class="headerlink" href="#maximum-likelihood" title="Permalink to this headline">¶</a></h1>
<p>How do we get the loss function that we use for logistic regression? It relies on a statistical argument called maximum likelihood (ML). Sadly, ML is used to be represent maximum likelihood and machine learning, both important topics in data science. So you’ll just kind of have to get used to which one is being used via the context.</p>
<p>To figure out maximum likelihood, let’s consider a bunch of coin flips, each with their own probability of a head. Say</p>
<div class="math notranslate nohighlight">
\[
p_i = P(Y_i = 1 | x_i) ~~~ 1 - p_i = P(Y_i = 0 | x_i)
\]</div>
<p>Here, we write <span class="math notranslate nohighlight">\(~| x_i\)</span> in the probability statement to denote that the probability may depend on the realized value of some variable that also depends on <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(X_i\)</span>, which is denoted as <span class="math notranslate nohighlight">\(x_i\)</span>. So, for a context, think <span class="math notranslate nohighlight">\(Y_i\)</span> is event that person <span class="math notranslate nohighlight">\(i\)</span> has hypertension and <span class="math notranslate nohighlight">\(x_i\)</span> their smoking consumption in pack years. We’d like to estimate the probability that someone has hypertension given their pack years.</p>
<p>We could write this more compactly as:</p>
<div class="math notranslate nohighlight">
\[
P(Y_i = j | x_i) = p_i ^ j (1 - p_i)^{1-j} ~~~ j \in \{0, 1\}.
\]</div>
<p>Consider a dataset, <span class="math notranslate nohighlight">\(Y_1, \ldots, Y_n\)</span> and <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span>. Consider a sequence of potential observed values of the <span class="math notranslate nohighlight">\(Y_i\)</span>, say <span class="math notranslate nohighlight">\(y_1, \ldots, y_n\)</span> where each <span class="math notranslate nohighlight">\(y_i\)</span> is 0 or 1. Then, using our formula:</p>
<div class="math notranslate nohighlight">
\[
P(Y_i = y_i | x_i) = p_i ^ {y_i} (1 - p_i)^{1-y_i}
\]</div>
<p>This is the (perhaps?) unfortunate notation that statisticians use, <span class="math notranslate nohighlight">\(Y_i\)</span> for the conceptual value of a variable and <span class="math notranslate nohighlight">\(y_i\)</span> for a realized value or number that we could plug in. This equation is just the probability of one <span class="math notranslate nohighlight">\(y_i\)</span>. This is why I’m using a lowercase <span class="math notranslate nohighlight">\(x_i\)</span> for the variables we’re conditioning on. Perhaps if I was being more correct, I would write something like <span class="math notranslate nohighlight">\(P(Y_i = y_i ~|~ X_i = x_i)\)</span>, but I find that adds too much notation.</p>
<p>What about all of them jointly? If the coin flips are independent, a statistical way of saying unrelated, then the probabilities multiply. So the <strong>joint</strong> probability of our data in this case is</p>
<div class="math notranslate nohighlight">
\[
P(Y_1 = y_1, \ldots, Y_n = y_n ~|~ x_1, \ldots, x_n)
= \prod_{i=1}^n p_i ^ {y_i} (1 - p_i)^{1-y_i}
\]</div>
<p>This model doesn’t say much, there’s nothing to tie these probabilities together. In our example, all we could do is estimate the probability of hypertension for a bunch of people with exactly the same pack years. There’s no <strong>parsimony</strong> so to speak. It seems logical that groups with nearly the same pack years should have similar probabilities, or even better that they vary smoothly with pack years. Our logistic regression model does this.</p>
<p>Consider again, our logistic regression model:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{logit}(p_i) = \beta_0 + \beta_1 x_i
\]</div>
<p>Now we have a model that relates the probabilities to the <span class="math notranslate nohighlight">\(x_i\)</span> in a smooth way. This implies that if we plot <span class="math notranslate nohighlight">\(x_i\)</span> versus <span class="math notranslate nohighlight">\(\mathrm{logit}(p_i)\)</span> we have a line and if we plot <span class="math notranslate nohighlight">\(x_i\)</span> versus <span class="math notranslate nohighlight">\(p_i\)</span> it looks like a sigmoid. So, under this model, what is our joint probability?</p>
<div class="math notranslate nohighlight">
\[
P(Y_1 = y_1, \ldots, Y_n = y_n ~|~ x_1, \ldots, x_n)
= \prod_{i=1}^n p_i ^ {y_i} (1 - p_i)^{1-y_i}
= \prod_{i=1}^n \left(\frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}}\right)^{y_i}
\left(\frac{1}{1 + e^{\beta_0 + \beta_1 x_i}}\right)^{1-y_i}
\]</div>
<p>We can work around with this a bit to get</p>
<div class="math notranslate nohighlight">
\[
\exp\left\{\beta_0 \sum_{i=1}^n y_i + \beta_1 \sum_{i=1}^n y_i x_i\right\}\times \prod_{i=1}^n \left(\frac{1}{1 + e^{\beta_0 + \beta_1 x_i}}\right)
\]</div>
<p>Notice, interestingly, this only depends on <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(\sum_{i=1}^n y_i\)</span> and <span class="math notranslate nohighlight">\(\sum_{i=1}^n y_i x_i\)</span>. These are called the <strong>sufficient statistics</strong>, since we don’t actually need to know the individual data points, just these quantities. (Effectively
these quantities can be turned into the proportion of Ys that are one and the correlation between the Ys and Xs.) Plug in these quantities and this equation spits out the joint probability of that particular sequence of 0s and 1s for the Ys given this particular collection of Xs. Of course, we can’t actually do this, because we don’t know <span class="math notranslate nohighlight">\(\beta_0\)</span> or <span class="math notranslate nohighlight">\(\beta_1\)</span>.</p>
<p>The statistician Fisher got around this problem using maximum likelihood. The principle is something like this. Pick the values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> that make the data that we actually observed most probable. This seems like a good idea, since the data that we observed must be at least somewhat probable (since we observed it). When you take the joint probability and plug in the actual Ys and Xs that we observed and view it as a function of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>, it’s called a <strong>likelihood</strong>. So a likelihood is the joint probability with the observed data plugged in and maximum likelihood finds the values of the parameters that makes the data
that we observed most likely.</p>
<p>Let’s consider that for our problem. Generally, since sums are more convenient than producs, we take the natural logarithm. Then, this works out to be:</p>
<div class="math notranslate nohighlight">
\[
\beta_0 \sum_{i=1}^n y_i + \beta_1 \sum_{i=1}^n y_i x_i - \sum_{i=1}^n \log\left(1 + e^{\beta_0 + \beta_1 x_i}\right)
\]</div>
<p>This is the function that sklearn maximizes over <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> to obtain the estimates. There’s quite a few
really good properties of maximum likelihood, which is why we use it.</p>
<div class="section" id="linear-regression">
<h2>Linear regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h2>
<p>You might be surprised to find out that linear regression can also be cast as a likelihood problem. Consider an instance where we assume that the <span class="math notranslate nohighlight">\(Y_i\)</span> are Gaussian with a mean equal to <span class="math notranslate nohighlight">\(\beta_0 + \beta_1 x_i\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. What that means is that the probability that <span class="math notranslate nohighlight">\(Y_i\)</span> lies betweens the points <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> is governed by the equation</p>
<div class="math notranslate nohighlight">
\[
P(Y_i \in [A, B) ~|~ x_i) = \int_A^B \exp\left\{ -(y_i - \beta_0 - \beta_1 x_i)^2 / 2\sigma^2 \right\} dy_i
\]</div>
<p>Letting <span class="math notranslate nohighlight">\(A=-\infty\)</span> and taking the derivative with respect to <span class="math notranslate nohighlight">\(B\)</span>, we obtain the density function, sort of the
probability on an infintessimally small interval:</p>
<div class="math notranslate nohighlight">
\[
\exp\left\{ -(y_i - \beta_0 - \beta_1 x_i)^2 / 2\sigma^2 \right\}
\]</div>
<p>Uses the density evaluated at the observed data, the joint likelihood assuming independence is:</p>
<div class="math notranslate nohighlight">
\[
\prod_{i=1}^n \exp\left\{ -(y_i - \beta_0 - \beta_1 x_i)^2 / 2\sigma^2 \right\}
= \exp\left\{ -\sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2 / 2\sigma^2 \right\}
\]</div>
<p>Since it’s more convenient to deal with logs we get that the joint log likelihood is</p>
<div class="math notranslate nohighlight">
\[
- \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2 / 2\sigma^2 
\]</div>
<p>Since minimizing the negative is the same as maximizing this, and the constants of proportionality are irrelevant for maximizing for <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span>, we get that maximum likelihood for these parameters minimizes</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2
\]</div>
<p>which is the same thing we minimized to obtain our least squares regression estimates.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="logistic.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Logistic regression</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="linearSeparable.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Linear separable models</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Brian Caffo<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>