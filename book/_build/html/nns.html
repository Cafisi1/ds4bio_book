
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Neural networks &#8212; Data science and AI for Bio/medical applications using python</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Pytorch by example" href="pytorch_regression.html" />
    <link rel="prev" title="Regression and FFTs" href="linearModels_FFTs.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/dasl.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data science and AI for Bio/medical applications using python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome!
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="git.html">
   Git, github
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ds_python.html">
   Python background
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="basic_python.html">
   Python basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="python_programming.html">
   Python programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="functions.html">
   Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="python_practice.html">
   Python in practice
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="virtual_environments.html">
   Virtual Environments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data_cleaning.html">
   Data cleaning by example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="EDA.html">
   Exploratory data analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data_structures.html">
   Data structures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sqlite.html">
   SQL via sqlite
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pysqlite.html">
   sqlite in python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rBasic.html">
   Base R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rTidyverse.html">
   R tidyverse quick example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rFromPython.html">
   R from python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pythonFromR.html">
   Python from R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="html.html">
   HTML, CSS and javascript
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="interactive.html">
   Interactive graphics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="webscraping.html">
   Webscraping
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="voila.html">
   Jupyterwidgets and voila
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="streamlit.html">
   Streamlit
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dash.html">
   Dash
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dash2.html">
   Dash callbacks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="binary_classification.html">
   Introduction to binary classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regression_through_the_origin.html">
   Regression through the origin
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regression.html">
   Continuous prediction with regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="numpy.html">
   Numpy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="logistic.html">
   Logistic regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml.html">
   Maximum Likelihood
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linearSeparable.html">
   Linear separable models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linearSeparableSMF.html">
   Interpretation of linear regression coefficients.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regression_examples.html">
   Linear models: a classic example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regression_interpretation.html">
   Regression interpretation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dft.html">
   DFT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linearModels_FFTs.html">
   Regression and FFTs
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pytorch_regression.html">
   Pytorch by example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="basic_regression_pytorch.html">
   Basic regression in pytorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="logistic_regression_pytorch.html">
   Logistic regression in pytorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="convnet_classifier_pytorch.html">
   Convnet classifier example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="convolutions.html">
   Convolutions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="autoencoder.html">
   Autoencoders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gan.html">
   GANs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gpu.html">
   Parallelism and GPU computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="explainability.html">
   Explainability
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/smart-stats/ds4bio_book/master?urlpath=tree/nns.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/smart-stats/ds4bio_book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/smart-stats/ds4bio_book/issues/new?title=Issue%20on%20page%20%2Fnns.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/nns.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basics">
   Basics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#more-layers">
   More layers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#activation-functions">
   Activation functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization">
   Optimization
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Neural networks</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basics">
   Basics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#more-layers">
   More layers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#activation-functions">
   Activation functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization">
   Optimization
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <p><a href="https://colab.research.google.com/github/smart-stats/ds4bio_book/blob/main/book/nns.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> <a class="reference external" href="https://mybinder.org/v2/gh/smart-stats/ds4bio_book/HEAD"><img alt="Binder" src="https://mybinder.org/badge_logo.svg" /></a></p>
<section class="tex2jax_ignore mathjax_ignore" id="neural-networks">
<h1>Neural networks<a class="headerlink" href="#neural-networks" title="Permalink to this headline">#</a></h1>
<section id="basics">
<h2>Basics<a class="headerlink" href="#basics" title="Permalink to this headline">#</a></h2>
<p>Let’s start by relating neural networks to regression. Consider a simple case where we have two nodes, <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(X\)</span> pointing to an outcome <span class="math notranslate nohighlight">\(Y\)</span>. What does this mean? Let’s first put some context around the problem. Imagine that we want to use a subject’s BMI <span class="math notranslate nohighlight">\(X\)</span> to predict their blood pressure, <span class="math notranslate nohighlight">\(Y\)</span>. This diagram represents that.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">sklearn</span> <span class="k">as</span> <span class="nn">skl</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="c1">#G = nx.Graph()</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>

<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="n">pos</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="n">pos</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;Y&quot;</span><span class="p">,</span> <span class="n">pos</span> <span class="o">=</span> <span class="p">(</span><span class="mf">.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;Y&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="s2">&quot;Y&quot;</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> 
        <span class="n">nx</span><span class="o">.</span><span class="n">get_node_attributes</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="s1">&#39;pos&#39;</span><span class="p">),</span> 
        <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
        <span class="n">font_weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> 
        <span class="n">node_size</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span>
        <span class="n">node_color</span> <span class="o">=</span> <span class="s2">&quot;lightblue&quot;</span><span class="p">,</span>
        <span class="n">linewidths</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_edgecolor</span><span class="p">(</span><span class="s2">&quot;#000000&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">.3</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">.3</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/nns_1_0.png" src="_images/nns_1_0.png" />
</div>
</div>
<p>To interpret this diagram as a neural network, consider the following rule:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Parent nodes that point to a child node are multiplied by weights then added together then operated on by an activation function to form the child node.</p>
</div>
<p>If the parent nodes point to the outcome, then the nodes are combined the operated on by a known function, called the <strong>activation function</strong> to form a prediction. So, in this case, this is saying that the intercept (node labeled <span class="math notranslate nohighlight">\(1\)</span>)times a weight plus BMI (node labeled <span class="math notranslate nohighlight">\(X\)</span>) times a different weight get combined to form a prediction for SBP <span class="math notranslate nohighlight">\(Y\)</span>. Or, in other words</p>
<div class="math notranslate nohighlight">
\[
\hat Y = g(w_0 \times 1 + w_1 \times X)
\]</div>
<p>where <span class="math notranslate nohighlight">\(g\)</span> is a function that we specify. So in this case, if <span class="math notranslate nohighlight">\(w_0 = 120\)</span>, <span class="math notranslate nohighlight">\(w_1 = .1\)</span> and <span class="math notranslate nohighlight">\(g\)</span> is an idenity function, <span class="math notranslate nohighlight">\(g(a) = a\)</span>, and a subject had a BMI of 30, then the prediction would be</p>
<div class="math notranslate nohighlight">
\[
\hat Y = g(120 + .1 * 30) = 120.3
\]</div>
<p>Note <span class="math notranslate nohighlight">\(g\)</span> is not shown in the diagram (though maybe you could with the shape of the child node) or something like that0. Also not shown in the daigram is:</p>
<ul class="simple">
<li><p>The loss function, i.e. how to measure the different between <span class="math notranslate nohighlight">\(\hat Y\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
<li><p>The way the loss function combines subjects; we have multiple BMIs and SBPs</p></li>
<li><p>How we obtain the weights, <span class="math notranslate nohighlight">\(W_0\)</span> and <span class="math notranslate nohighlight">\(W_1\)</span>; this is done by minmizing the loss function using an algorithm</p></li>
</ul>
<p>So, imagine the case where <span class="math notranslate nohighlight">\(g\)</span> is an identity function, our loss function for different subjects is squared error and we combine different losses by adding them up. Then, our weights are obtained by minmizing</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^N (Y_i - \hat Y_i)^2 
\]</div>
<p>and so, presuming our optimization algorithm works well, it should be idential to linear regression.</p>
<p>Consider a different setting. Imagine if our <span class="math notranslate nohighlight">\(Y\)</span> is 0 or 1 based on whether or not the subject is taking anti-hypertensive mediations. Further, let <span class="math notranslate nohighlight">\(g\)</span> be the sigmoid function, <span class="math notranslate nohighlight">\(g(a) = 1 / \{1 + \exp(-a)\}\)</span>. Our prediction is</p>
<div class="math notranslate nohighlight">
\[
\hat Y = \{1 + \exp(-W_0 - W_1 X)\}^{-1}
\]</div>
<p>which is the logistic regression prediction with intercept <span class="math notranslate nohighlight">\(W_0\)</span> and slope <span class="math notranslate nohighlight">\(W_1\)</span>. Consider a case where
<span class="math notranslate nohighlight">\(W_0 = -4\)</span>, <span class="math notranslate nohighlight">\(W_1 = .1\)</span> and <span class="math notranslate nohighlight">\(X=30\)</span>, then our <span class="math notranslate nohighlight">\(\hat Y = 1 / \{1 + \exp[-(-4 + .1\times 30)\}]\approx .27\)</span>. Thus, this model estimates a 27% probability that a subject with a BMI of 30 has hypertension.</p>
<p>Further, if we specify that the loss function is binary cross entropy</p>
<div class="math notranslate nohighlight">
\[
- \sum_{i=1}^n \{ Y_i \log(\hat Y_i) + (1 - Y_i) \log(1 - \hat Y_i)\} / N
\]</div>
<p>then minmizing our loss function is identical to maximizing the likelihood for logistic regression.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span> <span class="o">+</span> <span class="mf">.1</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.2689414213699951
</pre></div>
</div>
</div>
</div>
</section>
<section id="more-layers">
<h2>More layers<a class="headerlink" href="#more-layers" title="Permalink to this headline">#</a></h2>
<p>Of course, there’d be no point in using NNs for problems that we can just solve with generalized linear models. NNs get better when we add more layers, since then they can discover interactions and non-linearities. Consider the following model. Notice we quit explicitly adding the bias (intercept) term / node. In general assume the bias term is included unless otherwise specified.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#plt.figure(figsize=[2, 2])</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span>  <span class="n">pos</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;X2&quot;</span><span class="p">,</span>  <span class="n">pos</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;H11&quot;</span><span class="p">,</span> <span class="n">pos</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;H12&quot;</span><span class="p">,</span> <span class="n">pos</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;H21&quot;</span><span class="p">,</span> <span class="n">pos</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;H22&quot;</span><span class="p">,</span> <span class="n">pos</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;Y&quot;</span><span class="p">,</span> <span class="n">pos</span> <span class="o">=</span> <span class="p">(</span><span class="mf">.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edges_from</span><span class="p">([</span> <span class="p">(</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="s2">&quot;H11&quot;</span><span class="p">),</span>  <span class="p">(</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="s2">&quot;H12&quot;</span><span class="p">),</span>  <span class="p">(</span><span class="s2">&quot;X2&quot;</span><span class="p">,</span> <span class="s2">&quot;H11&quot;</span><span class="p">),</span>  <span class="p">(</span><span class="s2">&quot;X2&quot;</span><span class="p">,</span> <span class="s2">&quot;H12&quot;</span><span class="p">)])</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edges_from</span><span class="p">([(</span><span class="s2">&quot;H11&quot;</span><span class="p">,</span> <span class="s2">&quot;H21&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;H11&quot;</span><span class="p">,</span> <span class="s2">&quot;H22&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;H12&quot;</span><span class="p">,</span> <span class="s2">&quot;H21&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;H12&quot;</span><span class="p">,</span> <span class="s2">&quot;H22&quot;</span><span class="p">)])</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edges_from</span><span class="p">([(</span><span class="s2">&quot;H21&quot;</span><span class="p">,</span> <span class="s2">&quot;Y&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;H22&quot;</span><span class="p">,</span> <span class="s2">&quot;Y&quot;</span><span class="p">)])</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> 
        <span class="n">nx</span><span class="o">.</span><span class="n">get_node_attributes</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="s1">&#39;pos&#39;</span><span class="p">),</span> 
        <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
        <span class="n">font_weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> 
        <span class="n">node_size</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span>
        <span class="n">node_color</span> <span class="o">=</span> <span class="s2">&quot;lightblue&quot;</span><span class="p">,</span>
        <span class="n">linewidths</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_edgecolor</span><span class="p">(</span><span class="s2">&quot;#000000&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">.3</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">.3</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/nns_5_0.png" src="_images/nns_5_0.png" />
</div>
</div>
<p>Usually, the nodes are added in so called layers. <span class="math notranslate nohighlight">\((X_1, X_2)\)</span> is the input layer, <span class="math notranslate nohighlight">\((H_{11}, H_{12})\)</span> is the first hidden layer, <span class="math notranslate nohighlight">\((H_{21}, H_{22})\)</span> is the second hidden layer and <span class="math notranslate nohighlight">\(Y\)</span> is the output layer. Imagine plugging an <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> into this network. It would feed forward through the network as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
H_{11} = &amp; g_1(W_{011} + W_{111} X_1 + W_{211} X_2) \\
H_{12} = &amp; g_1(W_{012} + W_{112} X_1 + W_{212} X_2) \\
H_{21} = &amp; g_2(W_{021} + W_{121} H_{11} + W_{221} H_{12}) \\
H_{22} = &amp; g_2(W_{022} + W_{122} H_{12} + W_{222} H_{12}) \\
\hat Y = &amp; g_3(W_{031} + W_{131} H_{21} + W_{231} H_{22})
\end{align}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(g_k\)</span> are specified activation functions. Typically, we would have a different activation function for the output layer than the others, and the other would have the same activation function. So, for example, if <span class="math notranslate nohighlight">\(Y\)</span> was binary, like hypertension diagnosis, then <span class="math notranslate nohighlight">\(g_1=g_2\)</span> and <span class="math notranslate nohighlight">\(g_3\)</span> would be a sigmoid.</p>
</section>
<section id="activation-functions">
<h2>Activation functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">#</a></h2>
<p>The output activation function tends to be based on the structure of the outcome. For example, a binary outcome would likely have a sigmoidal, or other function from <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> to <span class="math notranslate nohighlight">\([0, 1]\)</span> so as to model a probability. Historically, the internal activation functions were binary thresholds. This was owning to the fact that neural networks were models of (biological) neurons and the threshold was a model of an action potential being propigated. However, modern neural networks have less of a direct connection to their biological motivation and other activation functions tend to be used. The most popular right now is the rectified linear unit (RELU) function. This is simply:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
RELU(a) = \left\{
\begin{array}{ll}
a &amp; \text{if $a&gt;0$} \\
0 &amp; \text{otherwise}
\end{array}
\right.
= a \times I(a &gt; 0)
\end{split}\]</div>
<p>Plotted, this is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">4</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/nns_8_0.png" src="_images/nns_8_0.png" />
</div>
</div>
<p>If a bias term is included, then the fact that the RELU is centered at zero isn’t important, since the intercept term effectively shifts the function around. These kinds of splin terms are incredibly flexible. Just to show you an example, let’s fit the sine function using a collection of shifted RELUs. This is just</p>
<div class="math notranslate nohighlight">
\[
Y = \sin(X) + \epsilon
\]</div>
<p>being fit with</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^N \left\{ Y_i - W_{021} - \sum_{j=1}^{d} W_{j21} g(W_{1j1} X_i- W_{0j1}) \right\}^2
\]</div>
<p>where the <span class="math notranslate nohighlight">\(W_{kj}\)</span> are the weights for layer <span class="math notranslate nohighlight">\(k\)</span>. Below, we’re just setting <span class="math notranslate nohighlight">\(W_{1j1} = 1\)</span> and specifying the <span class="math notranslate nohighlight">\(W_{0j1}\)</span> at a sequence of values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Generate some data, a sine function on 0,4*pi</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="n">n</span><span class="p">)</span>

<span class="c1">## Generate the spline regressors</span>
<span class="n">df</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">knots</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">df</span><span class="p">)</span>
<span class="n">xmat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">df</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span> <span class="n">xmat</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">knots</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="n">knots</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="c1">## Fit them</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xmat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xmat</span><span class="p">)</span>

<span class="c1">## Plot them versus the data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yhat</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/nns_10_0.png" src="_images/nns_10_0.png" />
</div>
</div>
<p>This corresponds to a network like depicted below if there were <span class="math notranslate nohighlight">\(d=3\)</span>  hidden nodes, there was a relu activation function at the first layer, then a identity activation function for the output layer and the weights for the first layer are specified.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">,</span>  <span class="n">pos</span> <span class="o">=</span>  <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;H11&quot;</span><span class="p">,</span> <span class="n">pos</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;H12&quot;</span><span class="p">,</span> <span class="n">pos</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;H13&quot;</span><span class="p">,</span> <span class="n">pos</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;Y&quot;</span><span class="p">,</span> <span class="n">pos</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edges_from</span><span class="p">([(</span><span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="s2">&quot;H11&quot;</span><span class="p">),</span>  <span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="s2">&quot;H12&quot;</span><span class="p">),</span>  <span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="s2">&quot;H13&quot;</span><span class="p">)])</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edges_from</span><span class="p">([(</span><span class="s2">&quot;H11&quot;</span><span class="p">,</span> <span class="s2">&quot;Y&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;H12&quot;</span><span class="p">,</span> <span class="s2">&quot;Y&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;H13&quot;</span><span class="p">,</span> <span class="s2">&quot;Y&quot;</span><span class="p">)])</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> 
        <span class="n">nx</span><span class="o">.</span><span class="n">get_node_attributes</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="s1">&#39;pos&#39;</span><span class="p">),</span> 
        <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
        <span class="n">font_weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> 
        <span class="n">node_size</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span>
        <span class="n">node_color</span> <span class="o">=</span> <span class="s2">&quot;lightblue&quot;</span><span class="p">,</span>
        <span class="n">linewidths</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_edgecolor</span><span class="p">(</span><span class="s2">&quot;#000000&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">.3</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">.3</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/nns_12_0.png" src="_images/nns_12_0.png" />
</div>
</div>
<p>We can actually fit this function way better using splines and a little bit more care. However, this helps show how even one layer of RELU activated nodes can start to fit complex shapes.</p>
</section>
<section id="optimization">
<h2>Optimization<a class="headerlink" href="#optimization" title="Permalink to this headline">#</a></h2>
<p>One of the last bits of the puzzle we have to figure out is how to obtain the weights. A good strategy would be to minimize the loss function. However, it’s hard to minmize. If we had a derivative, we could try the following. Let <span class="math notranslate nohighlight">\(L(W)\)</span> be the loss function for weights <span class="math notranslate nohighlight">\(W\)</span>. Note, we’re omitting the fact that this is a function of the data (predictors and outcome) as well, since that’s a set of fixed numbers. Consider updating parameters as</p>
<div class="math notranslate nohighlight">
\[
W^{(new)} = W^{(old}) - e * L'(W^{(old)})
\]</div>
<p>What does this do? It moves the parameters by a small amount, <span class="math notranslate nohighlight">\(e\)</span>, called the <strong>learning rate</strong>, in the direction the opposite of the gradient. Think of a one dimensional convex function. If the derivative at a point is positive, then that point is larger than where the minimum is. Similarily, if the derivative is negative, it’s smaller. So, the idea is to head a small amount in the opposite direction of the derivative. How much? How about along the line of the derivative? That’s all gradient descent does, just in more than one dimension.</p>
<p>How do we get the gradient? Consider the following. If <span class="math notranslate nohighlight">\(X\)</span> is our vector of predictors and <span class="math notranslate nohighlight">\(Y\)</span> is our vector of outputs, a neural network with 3 layers, can be thought of as, where <span class="math notranslate nohighlight">\(L_k\)</span> is layer <span class="math notranslate nohighlight">\(K\)</span> and <span class="math notranslate nohighlight">\(W_k\)</span> are the weights for that layer:</p>
<div class="math notranslate nohighlight">
\[
L_3(L_2(L_1(X, W_1), W_2) W_3)
\]</div>
<p>Or a series of function compositions. Recall from calculus, if we want the derivative of composed functions we have a really simple rule called the chain rule:</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{dx}f(g(x)) = f'(g(x)) g'(x)
\]</div>
<p>I.e. if <span class="math notranslate nohighlight">\(h=f(u)\)</span> and <span class="math notranslate nohighlight">\(u = g(x)\)</span> then <span class="math notranslate nohighlight">\(\frac{dh}{dx} = \frac{dh}{du}\frac{du}{dx}\)</span>. Thus, characterized this way, the chain rule formally acts like fractions (though this is a symbolic equivalence having entirely different underlying meanings).</p>
<p>If we use the chain rule on our composed loss functions, we wind up bookkeeping backwards through our neural network. That is why it’s called backwards propagation (backprop).</p>
<p>So, our algorithm goes something like this.
Given, <span class="math notranslate nohighlight">\(W^{(new)}\)</span>, network, <span class="math notranslate nohighlight">\(\phi(X, W)\)</span>, which depends on the predictors and the weights
and loss, <span class="math notranslate nohighlight">\(L(Y, \hat Y)\)</span>, which depends on the observed and predicted outputs.</p>
<ol class="simple">
<li><p>Set <span class="math notranslate nohighlight">\(W^{(old)}=W^{(new)}\)</span></p></li>
<li><p>Calculate <span class="math notranslate nohighlight">\(\hat Y = \phi(X, W^{(old)})\)</span> and loss <span class="math notranslate nohighlight">\(L(Y, \hat Y)\)</span>.</p></li>
<li><p>Use back propagation to get to get a numerical approximation to <span class="math notranslate nohighlight">\(\frac{d}{dW} L\{Y, \phi(X, W)\} |_{W=W^{(old)}} = L'(W^{(old)})\)</span></p></li>
<li><p>Update <span class="math notranslate nohighlight">\(W^{(new)} = W^{(old)} - e L'(W^{(old)})\)</span></p></li>
<li><p>Go to step 0.</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="linearModels_FFTs.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Regression and FFTs</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="pytorch_regression.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Pytorch by example</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Brian Caffo<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>