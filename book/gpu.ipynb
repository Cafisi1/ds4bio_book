{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7348d2-466b-45fa-9096-0b052f42b5b5",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/smart-stats/ds4bio_book/blob/main/book/gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/smart-stats/ds4bio_book/HEAD)\n",
    "\n",
    "# Parallelism and GPU computing\n",
    "\n",
    "AI calculations involve quite a few arithmetic calculations. Graphics processing units (GPUs) are computer chips that were designed to parallelize large numbers of small arithmetic calculations. They were designed as such because computer graphics involve rotations, shifts, scaling, etc. of images, are a matrix manipulations. \n",
    "\n",
    "Currently, I don't have a GPU in my personal computer. Some options for trying out GPUs include Google Colab and Paperspace to name two examples. I'll show \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c36c84d2-f951-4197-b765-42c50ae1efb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06decc32-1d48-471c-964b-c86c1b1dc558",
   "metadata": {},
   "source": [
    "By default, calculations will be on the CPU. You have to actually migrate calculations to the GPU. Here I write the code in such a way that it works on the GPU or CPU depending on whether a GPU is available. It's typical to create a variable called \"device\" that references the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c610caeb-4553-4dfd-85fa-e8d3585b2929",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126ca6d3-71ce-442a-a4f5-a02adcdb3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "test_matrix = torch.randn([100000, 10])\n",
    "test_matrix_cuda = test_matrix.to(device)\n",
    "\n",
    "start = time.time()\n",
    "test_matrix.sum()\n",
    "end = time.time()\n",
    "\n",
    "a = end - start\n",
    "\n",
    "start = time.time()\n",
    "test_matrix_cuda.sum()\n",
    "end = time.time()\n",
    "\n",
    "b = end - start\n",
    "\n",
    "print(\"The % reduction in runtime is: \", end = \"\")\n",
    "print([(1 - b / a) * 100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ds4bio]",
   "language": "python",
   "name": "conda-env-.conda-ds4bio-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
