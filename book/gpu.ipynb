{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ad7348d2-466b-45fa-9096-0b052f42b5b5",
      "metadata": {
        "id": "ad7348d2-466b-45fa-9096-0b052f42b5b5"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smart-stats/ds4bio_book/blob/main/book/gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/smart-stats/ds4bio_book/HEAD)\n",
        "\n",
        "# Parallelism and GPU computing\n",
        "\n",
        "AI calculations involve quite a few arithmetic calculations. Graphics processing units (GPUs) are computer chips that were designed to parallelize large numbers of small arithmetic calculations. They were designed as such because computer graphics involve rotations, shifts, scaling, etc. of images, are a matrix manipulations. \n",
        "\n",
        "Currently, I don't have a GPU in my personal computer. Some options for trying out GPUs include Google Colab and Paperspace to name two examples. I'll show \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c36c84d2-f951-4197-b765-42c50ae1efb9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c36c84d2-f951-4197-b765-42c50ae1efb9",
        "outputId": "12233b5d-7360-49dd-ea1b-771049220fa3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06decc32-1d48-471c-964b-c86c1b1dc558",
      "metadata": {
        "id": "06decc32-1d48-471c-964b-c86c1b1dc558"
      },
      "source": [
        "By default, calculations will be on the CPU. You have to actually migrate calculations to the GPU. Here I write the code in such a way that it works on the GPU or CPU depending on whether a GPU is available. It's typical to create a variable called \"device\" that references the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c610caeb-4553-4dfd-85fa-e8d3585b2929",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c610caeb-4553-4dfd-85fa-e8d3585b2929",
        "outputId": "9455e88c-5b34-4e4d-ca3c-770cff970a07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else :\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the reduction in runtime"
      ],
      "metadata": {
        "id": "xgX82fORnt0y"
      },
      "id": "xgX82fORnt0y"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "126ca6d3-71ce-442a-a4f5-a02adcdb3a9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "126ca6d3-71ce-442a-a4f5-a02adcdb3a9e",
        "outputId": "6bd61c71-99e4-4822-c858-4dd5b01463d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The % reduction in runtime is: [69.79166666666667]\n",
            "The % reduction in runtime is: [73.96593673965937]\n",
            "The % reduction in runtime is: [76.03406326034063]\n",
            "The % reduction in runtime is: [75.97633136094674]\n",
            "The % reduction in runtime is: [78.29638273045506]\n",
            "The % reduction in runtime is: [75.93023255813954]\n",
            "The % reduction in runtime is: [78.57974388824213]\n",
            "The % reduction in runtime is: [77.97468354430379]\n",
            "The % reduction in runtime is: [78.54588796185935]\n",
            "The % reduction in runtime is: [78.66354044548652]\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "for i in range(10):\n",
        "  test_matrix = torch.randn([100000, 10])\n",
        "  test_matrix_cuda = test_matrix.to(device)\n",
        "\n",
        "  start = time.time()\n",
        "  test_matrix.sum()\n",
        "  end = time.time()\n",
        "\n",
        "  a = end - start\n",
        "\n",
        "  start = time.time()\n",
        "  test_matrix_cuda.sum()\n",
        "  end = time.time()\n",
        "\n",
        "  b = end - start\n",
        "\n",
        "  print(\"The % reduction in runtime is: \", end = \"\")\n",
        "  print([(1 - b / a) * 100])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "import PIL\n",
        "\n",
        "## Read in and organize the data\n",
        "imgURL = \"https://raw.githubusercontent.com/larvalabs/cryptopunks/master/punks.png\"\n",
        "urllib.request.urlretrieve(imgURL, \"cryptoPunksAll.jpg\")\n",
        "img = PIL.Image.open(\"cryptoPunksAll.jpg\").convert(\"RGB\")\n",
        "imgArray = np.asarray(img)\n",
        "finalArray = np.empty((10000, 3, 24, 24))\n",
        "for i in range(100):\n",
        "  for j in range(100):\n",
        "    a, b = 24 * i, 24 * (i + 1)  \n",
        "    c, d = 24 * j, 24 * (j + 1) \n",
        "    idx = j + i * (100)\n",
        "    finalArray[idx,0,:,:] = imgArray[a:b,c:d,0]\n",
        "    finalArray[idx,1,:,:] = imgArray[a:b,c:d,1]\n",
        "    finalArray[idx,2,:,:] = imgArray[a:b,c:d,2]\n",
        "\n",
        "n = finalArray.shape[0]\n",
        "x_real = finalArray / 255\n",
        "x_real = torch.tensor(x_real.astype(np.float32))\n",
        "kernel_size = 5\n",
        "generator_input_dim = [16, 3, 3]\n",
        "\n",
        "class create_generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()        \n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(16, 128, 10, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 3, 4, 2, 1, bias=False), \n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        " \n",
        "## Use the discriminator from the convnet chapter\n",
        "class create_discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
        "        self.fc1 = nn.Linear(12 * 3 * 3, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "    \n",
        "        \n",
        "generator = create_generator()\n",
        "discriminator = create_discriminator()\n",
        "\n",
        "lr = 1e-4\n",
        "\n",
        "## y is n real images then n fake images\n",
        "y = torch.concat( (torch.ones(n), torch.zeros(n) ) ) \n",
        "\n",
        "## Set up optimizers\n",
        "optimizerD = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "optimizerG = optim.Adam(generator.parameters(), lr=lr)\n",
        "\n",
        "## Set up the loss function\n",
        "loss_function = nn.BCELoss()"
      ],
      "metadata": {
        "id": "4tV0_A_Zhnn6"
      },
      "id": "4tV0_A_Zhnn6",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randomBatchSize = .1\n",
        "n_epochs = 20\n",
        "trainFraction = .1\n",
        "\n",
        "for epoch in range(n_epochs):          \n",
        "    ## Generate the batch sample\n",
        "    sample = np.random.uniform(size = n) < trainFraction\n",
        "    n_batch = np.sum(sample)\n",
        "\n",
        "    ## Generate the simulated embedding    \n",
        "    embedding = torch.randn([n_batch]+generator_input_dim)\n",
        "    \n",
        "    \n",
        "    ## Generate new fake images\n",
        "    x_fake = generator(embedding)\n",
        "    \n",
        "    ## train the discriminator\n",
        "    ## zero out the gradient\n",
        "    discriminator.zero_grad()\n",
        "\n",
        "    ## run the generated and fake images through the discriminator\n",
        "    yhat_fake = discriminator(x_fake.detach())\n",
        "    yhat_real = discriminator(x_real[sample,:, :, :])\n",
        "    ## Note you have to concatenate them in the same order as \n",
        "    ## the previous cell. Remember we did real then fake\n",
        "    yhat = torch.concat( (yhat_real, yhat_fake) ).reshape(-1)\n",
        "\n",
        "    ## Calculate loss on all-real batch \n",
        "    y = torch.concat( (torch.ones(n_batch), torch.zeros(n_batch) ) ) \n",
        "\n",
        "    discriminator_error = loss_function(yhat, y)\n",
        "\n",
        "    # Calculate gradients for D in backward pass\n",
        "    discriminator_error.backward(retain_graph = True)\n",
        "\n",
        "    # Update the discriminator\n",
        "    optimizerD.step()\n",
        "\n",
        "    ## Train the generator\n",
        "    ## zero out the gradient\n",
        "    generator.zero_grad()\n",
        "    ## The discriminator has been udpated, so push the data through the \n",
        "    ## new discriminator\n",
        "    yhat_fake = discriminator(x_fake)\n",
        "    ## Note the outcome for the generator is all ones even\n",
        "    ## though we're classifying real as 1 and fake as 0\n",
        "    ## In other words, we want the loss for the generator to be\n",
        "    ## based on how real-like the generated data is\n",
        "    generator_error = loss_function( yhat_fake,  torch.ones( (n_batch, 1) ) )\n",
        "    ## Calculate the backwards error\n",
        "    generator_error.backward(retain_graph = False)\n",
        "    # Update the discriminator\n",
        "    optimizerG.step()\n",
        "    \n",
        "    if (epoch + 1) % 10 == 0:  \n",
        "      print(epoch, end = \",\")"
      ],
      "metadata": {
        "id": "CPvuMtxZjZVU"
      },
      "id": "CPvuMtxZjZVU",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "gpu.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}