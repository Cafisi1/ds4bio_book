{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Maximum Likelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "+ How do we get the loss function that we use for logistic regression? \n",
    "+ We rely on a statistical argument called maximum likelihood (ML). \n",
    "+ Sadly, ML is used to be represent maximum likelihood and machine learning, both important topics in data science. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Example, coin flipping \n",
    "\n",
    "+ Consider a coin flips, each with their own probability of a head\n",
    "\n",
    "$$\n",
    "p_i = P(Y_i = 1 | x_i) ~~~ 1 - p_i = P(Y_i = 0 | x_i)\n",
    "$$\n",
    "\n",
    "+ For example $Y_i$ could be the event that person $i$ has hypertension and $x_i$ their smoking consumption in pack years. \n",
    "+ We'd like to estimate the probability that someone has hypertension given their pack years.\n",
    "+ We could write this more compactly as:\n",
    "\n",
    "$$\n",
    "P(Y_i = j | x_i) = p_i ^ j (1 - p_i)^{1-j} ~~~ j \\in \\{0, 1\\}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## A coin flip\n",
    "\n",
    "+ Consider a dataset, $Y_1, \\ldots, Y_n$ and $x_1, \\ldots, x_n$. \n",
    "+ Every $y_1, \\ldots, y_n$  is either 0 or 1. \n",
    "+ Under our model, the probability of one observed data point is\n",
    "\n",
    "$$\n",
    "P(Y_i = y_i | x_i) = p_i ^ {y_i} (1 - p_i)^{1-y_i}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Multiple coin flips\n",
    "\n",
    "+ What about all of the data jointly? \n",
    "+ Assume the coin flips are independent, then the probabilities multiply. \n",
    "+ The **joint** probability of our data in this case is \n",
    "\n",
    "$$\n",
    "P(Y_1 = y_1, \\ldots, Y_n = y_n ~|~ x_1, \\ldots, x_n)\n",
    "= \\prod_{i=1}^n p_i ^ {y_i} (1 - p_i)^{1-y_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Tying the probaiblities together\n",
    "\n",
    "+ This model doesn't say much, there's nothing to tie these probabilities together. \n",
    "+ In our example, all we could do is estimate the probability of hypertension for a bunch of people with exactly the same pack years. + It seems logical that groups with nearly the same pack years should have similar probabilities, or even better that they vary smoothly with pack years. \n",
    "+ Our logistic regression model does this.\n",
    "\n",
    "$$\n",
    "\\mathrm{logit}(p_i) = \\beta_0 + \\beta_1 x_i\n",
    "$$\n",
    "\n",
    "+ Now we have a model that relates the probabilities to the $x_i$ in a smooth way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## PUtting it together\n",
    "\n",
    "$$\n",
    "P(Y_1 = y_1, \\ldots, Y_n = y_n ~|~ x_1, \\ldots, x_n)\n",
    "= \\prod_{i=1}^n p_i ^ {y_i} (1 - p_i)^{1-y_i}\n",
    "= \\prod_{i=1}^n \\left(\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\right)^{y_i}\n",
    "\\left(\\frac{1}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\right)^{1-y_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Simplyfing\n",
    "\n",
    "$$\n",
    "\\exp\\left\\{\\beta_0 \\sum_{i=1}^n y_i + \\beta_1 \\sum_{i=1}^n y_i x_i\\right\\}\\times \\prod_{i=1}^n \\left(\\frac{1}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Simplyfing\n",
    "\n",
    "$$\n",
    "\\exp\\left\\{\\beta_0 \\sum_{i=1}^n y_i + \\beta_1 \\sum_{i=1}^n y_i x_i\\right\\}\\times \\prod_{i=1}^n \\left(\\frac{1}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\right)\n",
    "$$\n",
    "\n",
    "+ Notice, interestingly, this only depends on $n$, $\\sum_{i=1}^n y_i$ and $\\sum_{i=1}^n y_i x_i$. \n",
    "+ These are called the **sufficient statistics**, since we don't actually need to know the individual data points, just these quantities. (Effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Maximum likelihood\n",
    "\n",
    "+ The principle of ML. \n",
    "\n",
    "*Pick the values of $\\beta_0$ and $\\beta_1$ that make the data that we actually observed most probable.* \n",
    "\n",
    "+ When you take the joint probability and plug in the actual Ys and Xs that we observed and view it as a function of $\\beta_0$ and $\\beta_1$, it's called a **likelihood**.\n",
    "+ So a likelihood is the joint probability with the observed data plugged in and maximum likelihood finds the values of the parameters that makes the data that we observed most likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Our example\n",
    "\n",
    "+ Generally, since sums are more convenient than products, we take the natural logarithm.\n",
    "\n",
    "$$\n",
    "\\beta_0 \\sum_{i=1}^n y_i + \\beta_1 \\sum_{i=1}^n y_i x_i - \\sum_{i=1}^n \\log\\left(1 + e^{\\beta_0 + \\beta_1 x_i}\\right)\n",
    "$$\n",
    "\n",
    "+ This is the function that sklearn maximizes over $\\beta_1$ and $\\beta_0$ to obtain the estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Second example, linear regression\n",
    "\n",
    "+ linear regression can also be cast as a likelihood problem. + Consider an instance where we assume that the $Y_i$ are Gaussian with a mean equal to $\\beta_0 + \\beta_1 x_i$ and variance $\\sigma^2$. \n",
    "+ The probability that $Y_i$ lies betweens the points $A$ and $B$ is governed by the equation\n",
    "\n",
    "$$\n",
    "P(Y_i \\in [A, B) ~|~ x_i) = \\int_A^B \\exp\\left\\{ -(y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2 \\right\\} dy_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Density functions\n",
    "\n",
    "+ Letting $A=-\\infty$ and taking the derivative with respect to $B$, we obtain the density function, sort of the probability on an infintessimally small interval:\n",
    "\n",
    "$$\n",
    "\\exp\\left\\{ -(y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2 \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Joint likelihood\n",
    "\n",
    "+ Uses the density evaluated at the observed data, the joint likelihood assuming independence is:\n",
    "\n",
    "$$\n",
    "\\prod_{i=1}^n \\exp\\left\\{ -(y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2 \\right\\}\n",
    "= \\exp\\left\\{ -\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2 \\right\\}\n",
    "$$\n",
    "\n",
    "Since it's more convenient to deal with logs we get that the joint log likelihood is\n",
    "\n",
    "$$\n",
    "- \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Loss function\n",
    "\n",
    "+ Since minimizing the negative is the same as maximizing this, and the constants of proportionality are irrelevant for maximizing for $\\beta_1$ and $\\beta_0$, we get that maximum likelihood for these parameters minimizes\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2\n",
    "$$\n",
    "\n",
    "+ which is the same thing we minimized to obtain our least squares regression estimates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ds4bio]",
   "language": "python",
   "name": "conda-env-.conda-ds4bio-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
