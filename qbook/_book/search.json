[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introductory and intermediate data science for the bio/medical sciences using python",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This is a book for the Data Science for Bio/Biostat/Public Health/medical classes."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "intro.html#markdown",
    "href": "intro.html#markdown",
    "title": "Introduction",
    "section": "Markdown",
    "text": "Markdown\nBefore getting started, you need to learn some markdown. Markdown is a markup language (like HTML) that is absurdly easy. Every data scientist needs to know markdown. Fortunately, you’re five minutes away from knowing it. A markdown file is a text file that needs to be rendered to look nice. If you want an example, this page was written in markdown. To try it, go to google colab or a jupyter lab, create a markdown cell and start editing. Try this cheat sheet."
  },
  {
    "objectID": "intro.html#some-basic-unix",
    "href": "intro.html#some-basic-unix",
    "title": "Introduction",
    "section": "Some basic unix",
    "text": "Some basic unix\nSome basic unix commands will go a long way in the course, especially for when you’re working on a remote server. On windows, you can actually install a unix environment on so-called services for unix. So, on windows, you have three options for working with the command line, i) install a linux subsystem and use that, ii) use the DOS command prompt or iii) use powershell. The commands here would only work for option i. However, when I work on a windows system, I tend to just use options ii or iii. Here, we’ll assume you’re working on a linux or unix system, or windows services for linux and you’ll have to read up elsewhere if you want to learn windows proper terminal commands.\nTo get a unix terminal, you have several options. Since we’re promoting jupyter and jupyterlab, just open up the terminal on there. (Again, assuming you’re working on a unix/linux system.)\nThe first thing you should try is figuring out where you’re at. Do this by typing\nprompt> pwd\nThis will show you where you are in the directory structure. If you want to see the contents of the directory try these\nprompt> ls\nprompt> ls -al\nprompt> ls -alh\nAdding the flags -a lists everything, including directories with a weird character in front. The l gives the long format, which gives more information and the h changes the filesize lists to a more human readable format. I also like the option --color. What you get with-alh` is as follows.\ntotal 36K\ndrwxrwxrwx+ 7 codespace root      4.0K Feb 14 14:24 .\ndrwxr-xrwx+ 5 codespace root      4.0K Oct 19 15:21 ..\ndrwxrwxrwx+ 6 codespace codespace 4.0K Feb 14 14:31 book\ndrwxrwxrwx+ 8 codespace root      4.0K Feb 15 21:34 .git\n-rw-rw-rw-  1 codespace codespace  171 Feb 14 14:24 .gitignore\n-rw-rw-rw-  1 codespace codespace    0 Feb 14 14:23 .nojekyll\n-rw-rw-rw-  1 codespace codespace  444 Feb 14 14:24 README.md\ndrwxrwxrwx+ 3 codespace codespace 4.0K Feb 14 14:24 slides\ndrwxrwxrwx+ 7 codespace codespace 4.0K Oct 19 15:21 .venv\ndrwxrwxrwx+ 2 codespace codespace 4.0K Oct 19 15:23 .vscode\nThe drwxrwxrwx+1 looking columns give permissions d=directory, r=read, w=write and x=execute, the groups are owner (you), group, everyone. So a file that is -rw------- can be read and written to by the owner, but cannot be executed by anyone and no one else can read or write to it (except the superuser, who gets to do everything).\nTo change a directory, try the following\nprompt> cd DIRECTORY\nwhere DIRECTORY is the name of the directory that you want to change into. You can hit TAB to autocomplete names. The command\nprompt> mv PATH_TO_INPUT_FILE PATH_TO_OUTPUT_FILE\nmoves the file. This is also how you rename a file, since you could just do mv FILENAME1 FILENAME2 and change the name.\nThe unix command for removing things is rm. So\nrm FILENAME\ndeletes the file. Note linux really deletes things, so do this with some care. You can’t remove directories this way, instead you could do rmdir DIRECTORY, but the directory has to be empty. If you want to use rm to remove a directory and its contents, you can do rm -rf DIRECTORY. However, use this with care.\nFinally, I find it very useful to use wget to grab files from the internet. So, for example,\nwget https://URL.../FILENAME\nwill grab the file from that link. Super useful.\nThat’s enough unix to get you started. You’ll find as you use the terminal more and more, you’ll like it better and better. Eventually, you’ll find GUIs kind of frustrating."
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "3  Git, github",
    "section": "",
    "text": "In the live versions of these classes, we use the version control system git and git hosting service github. If you work in data science should have a working knowledge of both git and at least one cloud hosting service (like github). For git, you work in a repository, which is basically a project directory on your computer with some extra files that help git work. Git is then used for version control so that you keep track of states of your project. Github, is a hosting service for git repositories. Typically, you have your repository on your computer and you coordinate it with the one on the server. Github is just one of several hosting services, bitbucket is another, or you could even relatively easily start your own. However, github has front end web services that allows you to interact with your remote repository easily. This is very convenient."
  },
  {
    "objectID": "git.html#the-least-you-need-to-know",
    "href": "git.html#the-least-you-need-to-know",
    "title": "3  Git, github",
    "section": "3.1 The least you need to know",
    "text": "3.1 The least you need to know\nI’m not going to fully recreate git / github tutorials here; here’s one I recommend by Sean Kross. Instead, I’m going to go through a typical git / github workflow.\n\nInitialization I almost always initialize my git repository on github with a readme.md file.\nClone I typically clone the repository to my local computer using the command line or a local git gui that works with github, like this one. Note that you only have to clone the repo once. After it’s cloned you have a full local copy of the repository.\nadd new files to track and stage them after I’ve worked with them.\ncommit the changes to the local repository with a meaningful commit message.\npush the changes to the repository.\nIf there’s changes on the remote repository not represented in my local repository, I pull those changes to my local repo.\n\nFor larger projects, you’re likely working with multiple people, some of whom you’ve given access to your remote repository and some of whom you have not. The ones who can’t directly push to the remote repo might have their own version of the code and their own version on github. If they think you should incorporate those changes, they might issue a pull request to you. You can then opt to pull their changes into your repo on github, then pull them from github to your local repo. One of the reasons why services like github and bitbucket are so popular is that they make this coordination fairly easy, along with having nice project messaging and management tools.\nIn our class, we use github classroom. For github classroom, you’ll get a link to a repo to put your submission files into. When you push to the remote repository, you’ll have submitted. But, up to the due date you can make changes an push again."
  },
  {
    "objectID": "git.html#a-little-more-detail",
    "href": "git.html#a-little-more-detail",
    "title": "3  Git, github",
    "section": "3.2 A little more detail",
    "text": "3.2 A little more detail\n\n3.2.1 Getting started\nGit is a version control system invented by Linus Torvalds, the invetor of the linux operating system. A github repository is a directory that the software git is using to keep track of versions. Install git, it’s already installed on most cloud services and many operaiting systems. You can initialize a repository in a directory with the command git init in that directory. Honestly, I almost never start a git repo this way, because I’ve connected my repo to a cloud server. But, more on that later. After you’ve initialized it, your git repo is a locally version controlled system.\n\n\n3.2.2 Adding files\nOK, now you want to add some of your files to your repo. You have a directory You can add files with the command\ngit add FILENAME\nThe file FILENAME is now staged to be part of the repository, but isn’t added yet! Files that you haven’t added won’t be tracked. This is good, since there’s many files, llike log files, that we don’t want to track. You only need to add the files once, then they are being tracked. Try it out with a readme markdown file in your repository.\n\n\n3.2.3 Checking status\nOne of the most useful git commands is\ngit status\nThis gives the status of which files are being tracked, which are not and which have changes.\n\n\n3.2.4 Commiting changes\nYou have to actually commit your changes to the repository to have them represented. Committing changes is a local operation, subsequently, we’ll show you how to coordinate with a remote repository. Here’s the command I use to commit most often\ngit commit -a -m \"COMMIT MESSAGE\"\nHere the flag -a means commit all files staged for commits. You can also commit individual files or a collection of specific files. The -m \"COMMIT MESSAGE\" is the message associated with your commit. Make your commit messages more informative. If you don’t add -m github will bring up an editor for you to enter your commit message.\n\n\n3.2.5 Remote repositories\nGit becomes much more useful when it’s coordinated with a remote repository. Github is a server for hosting remote git repositories, there are several others, bitbucket is an example. Github also includes a great deal of functionality for working with the remote repository through the website. There’s many git and github guis that you could use if you want. I always liked to github one and magit in emacs.\n\n3.2.5.1 Connecting your remote and local repos\nYou need to connect your local repo to the remote repo. First create a version of the repo on github (actually then it gives you instructions). Then you can either\n\nClone the repo from the server git clone REMOTE SERVER\nConnect the server to your local repository\n\nFor 2. you use a command like:\ngit remote add origin git@github.com:USERNAME/REPONAME.git\nPersonally, I find it easier just to create the repo on github and clone it. You can clone over ssh or https, if you don’t know what ssh is use https and then switch over to ssh as you learn how to use it.\n\n\n3.2.5.2 Pushing changes\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n#import numpy as np\n#import sklearn as skl\n\nplt.figure(figsize=[2, 2])\nG = nx.DiGraph()\n\nG.add_node(\"GH\",    pos = (.5, 1) )\nG.add_node(\"Local\", pos = (.5, 0))\n#G.add_edge(\"GH\", \"Local\")\nG.add_edge(\"Local\", \"GH\")\nnx.draw(G, \n        nx.get_node_attributes(G, 'pos'), \n        with_labels=True, \n        font_weight='bold', \n        node_size = 2000,\n        node_color = \"lightblue\",\n        linewidths = 3)\nax= plt.gca()\nax.collections[0].set_edgecolor(\"#000000\")\nax.set_xlim([-.3, 1.3])\nax.set_ylim([-.3, 1.3])\nplt.show()\n\n\n\n\nYou push from the repo you’re working on to another (remote repo). So, for me, this typically means pushing from my local computer repo to github or to a scientific computing cluster.\ngit push origin main\nThe above command could be read as “push my new committed changes from my origin to the main branch on my remote repo”. Older repos had the default branch master instead of main. But, github switched in 2020 or so. So, some older documentation may not have this switch.\n\n\n3.2.5.3 Pulling changes\n\nplt.figure(figsize=[2, 2])\nG = nx.DiGraph()\n\nG.add_node(\"GH\",    pos = (.5, 1) )\nG.add_node(\"Local\", pos = (.5, 0))\nG.add_edge(\"GH\", \"Local\")\n#G.add_edge(\"Local\", \"GH\")\nnx.draw(G, \n        nx.get_node_attributes(G, 'pos'), \n        with_labels=True, \n        font_weight='bold', \n        node_size = 2000,\n        node_color = \"lightblue\",\n        linewidths = 3)\nax= plt.gca()\nax.collections[0].set_edgecolor(\"#000000\")\nax.set_xlim([-.3, 1.3])\nax.set_ylim([-.3, 1.3])\nplt.show()\n\n\n\n\nThere might be changes that you want represented locally that were changed on the remote repository; for example, you might edit a readme file through github’s web interface. The command for this is really easy, it’s just pull\ngit pull\nThe pain occurs if you have made local changes that are in conflict with the remote changes. I’d write about merge conflicts, but I can’t, I just can’t (shudder). [Here’s some docs on managing merge conflicts] (https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/addressing-merge-conflicts/resolving-a-merge-conflict-using-the-command-line).\n\n\n3.2.5.4 Cloning\n\nplt.figure(figsize=[3, 2])\nG = nx.DiGraph()\n\nG.add_node(\"GH1\",    pos = (.5, 1) )\nG.add_node(\"Local1\", pos = (.5, 0))\nG.add_node(\"Local2\", pos = (1.5, 0))\n\nG.add_edge(\"GH1\", \"Local1\")\nG.add_edge(\"Local1\", \"GH1\")\nG.add_edge(\"GH1\", \"Local2\")\n\n#G.add_edge(\"Local\", \"GH\")\nnx.draw(G, \n        nx.get_node_attributes(G, 'pos'), \n        with_labels=True, \n        font_weight='bold', \n        node_size = 2500,\n        node_color = \"lightblue\",\n        linewidths = 3)\nax= plt.gca()\nax.collections[0].set_edgecolor(\"#000000\")\nax.set_xlim([-.3, 2.3])\nax.set_ylim([-.3, 1.3])\nplt.show()\n\n\n\n\nTo clone a remote repository is to simply create a local copy where you can pull changes from the remote. If a repository is public and you own it, then you can clone it and push changes back to the remote. If you don’t, you can clone and pull changes, but then can’t push back to the remote.\nTry with the book repo, clone it so you have access to the files. In the figure below the owner can pull and push to the repo. In the picture below, consider that you clone GH1 off of github to you. You can pull updates from GH1 but can’t push changes to GH1. The owner of GH1 of course can pull and push.\nAs an example, close this book (via https) with:\ngit clone https://github.com/smart-stats/ds4bio_book.git\n\n\n3.2.5.5 Forking\n\nplt.figure(figsize=[3, 2])\nG = nx.DiGraph()\n\nG.add_node(\"GH1\",    pos = (.5, 1) )\nG.add_node(\"GH2\",    pos = (1.5, 1) )\nG.add_node(\"Local1\", pos = (.5, 0))\nG.add_node(\"Local2\", pos = (1.5, 0))\n\nG.add_edge(\"GH1\", \"Local1\")\nG.add_edge(\"Local1\", \"GH1\")\nG.add_edge(\"GH1\", \"GH2\")\nG.add_edge(\"GH2\", \"Local2\")\nG.add_edge(\"Local2\", \"GH2\")\n\n\n#G.add_edge(\"Local\", \"GH\")\nnx.draw(G, \n        nx.get_node_attributes(G, 'pos'), \n        with_labels=True, \n        font_weight='bold', \n        node_size = 2500,\n        node_color = \"lightblue\",\n        linewidths = 3)\nax= plt.gca()\nax.collections[0].set_edgecolor(\"#000000\")\nax.set_xlim([-.3, 2.3])\nax.set_ylim([-.3, 1.3])\nplt.show()\n\n\n\n\nA common workflow for github is to “fork” the repository. This differs from cloning in the sense that your fork is an entirely separate new copy. In the picture above, GH2 is a fork of GH1. It can pull upstream changes from GH1, but can’t write to it, since you don’t own that repo. You can push and pull from Local2 which is connected to GH2.\nForking on github is easy. Just click the “fork” button in the upper right corner.\n\n\n3.2.5.6 Pull requests\n\nplt.figure(figsize=[3, 2])\nG = nx.DiGraph()\n\nG.add_node(\"GH1\",    pos = (.5, 1) )\nG.add_node(\"GH2\",    pos = (1.5, 1) )\nG.add_node(\"Local1\", pos = (.5, 0))\nG.add_node(\"Local2\", pos = (1.5, 0))\n\nG.add_edge(\"GH1\", \"Local1\")\nG.add_edge(\"Local1\", \"GH1\")\nG.add_edge(\"GH2\", \"GH1\")\nG.add_edge(\"GH2\", \"Local2\")\nG.add_edge(\"Local2\", \"GH2\")\n\n\n#G.add_edge(\"Local\", \"GH\")\nnx.draw(G, \n        nx.get_node_attributes(G, 'pos'), \n        with_labels=True, \n        font_weight='bold', \n        node_size = 2500,\n        node_color = \"lightblue\",\n        linewidths = 3)\nax= plt.gca()\nax.collections[0].set_edgecolor(\"#000000\")\nax.set_xlim([-.3, 2.3])\nax.set_ylim([-.3, 1.3])\nplt.show()\n\n\n\n\nImagine a case where you forked the book repo and have some great additions (maybe a section on merge conflicts?). You think that these changes you think should be represented for other students. You can’t push changes to my repo; the very idea would be perposterous! How do you get your great changes represented in my repo?\nYou issue a pull request in the sense of requesting GH1 to pull your changes from GH2. Then, I would get notified of the pull request and would decide whether or not to merge them into my repo. This system has many benefits, and particularly, is great for operating on open source projects.\nI like to do pull requests directly on github through the web interface."
  },
  {
    "objectID": "git.html#branching",
    "href": "git.html#branching",
    "title": "3  Git, github",
    "section": "3.3 Branching",
    "text": "3.3 Branching\nYou often want multiple versions of a repository. For example, you might want a development version, a working version and a stable version. This is what branches are for in git. To create and switch to a branch called dev, use:\ngit checkout -b dev\nThis creates a new branch called dev and switches to that branch. If you already have dev, use checkout without -b. Suppose you want to merge your change from dev into your default branch (main)\ngit checkout main\ngit merge dev\nStart very basic with branches, then when you get the hang of it you can use it like a pro. Here’s more documentation."
  },
  {
    "objectID": "git.html#clients",
    "href": "git.html#clients",
    "title": "3  Git, github",
    "section": "3.4 Clients",
    "text": "3.4 Clients\nYou can avoid the command line stuff with a git client, of which there are many. It’s still good to know the command line options, for remote work if nothing else. I’ve tried several git clients and my favorite is the github client software and magit in emacs."
  },
  {
    "objectID": "git.html#setting-up-ssh",
    "href": "git.html#setting-up-ssh",
    "title": "3  Git, github",
    "section": "3.5 Setting up ssh",
    "text": "3.5 Setting up ssh\nIf you want to connect to a remote repository to github on a unix or linux system, it’s probably easiest to set up using so called secure shell. You can also set it up a variety of ways on Windows, but the rules are a little less universal. If you find this very hard to follow, just use a git and github gui (like github desktop, which is what I like to recommend).\nSecure shell works as follows. It creates a public key (think like a lock, everyone can see it) and a private key (like the key for the lock, only you have it). When you connect to github, it has your public key and you use the private key to “unlock” it. First, you need a .ssh directory from your personal directory. If you don’t have that, try ssh username@server to a server that you know and it will automatically create one.\nwe’ll go through this tutorial.\nThen, cd into your .ssh director and type the following and follow the instructions\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\nThis generates your public/prviate key pair. Then you need to take the public key file, and upload it to github under settings. This is described here.\nIn some cases I’ve also had to add this to a file config in my .ssh directory:\nHost github.com\n     ForwardX11 no\n     HostName github.com\n     User git\n     IdentityFile ~/.ssh/YOUR_PRIVATE_KEY_FILENAME\nOnce this is setup, then you should be able to push and pull from github from the command line without passwords."
  },
  {
    "objectID": "git.html#github-pages",
    "href": "git.html#github-pages",
    "title": "3  Git, github",
    "section": "3.6 Github pages",
    "text": "3.6 Github pages\nIt’s possible to host web pages on github. By hosting I mean serving the web page on the internet so that others can view it. There’s a couple of steps to doing this. First, you need an html file to serve. If you look in the chapter on html, there’s the code for a basic wepage. Save a web page as say “index.html”. You can double check that it works by simply double clicking on the file on your local computer. It should display as a web page in your browser, but it’s only viewable by you. Notice the addres begins with file:///.... To serve the file so that others can view it, we need it to exist on a server.\nGithub will serve the file for us, but first we need a repository. Create a public repository; I typically do this on github. Then you need to add an empty file called .nojekyll. Don’t forget the period at the beginning. This prevents github from thinking you’re using the jekyll framework, which is the default. Since we’re serving a basic webpage, we don’t need this framework. I create this file on github just by clicking Add File then Create new file.\nNext we need to tell github that this repo serves web pages. Click on settings then pages on the left. Then, under source choose the branch that you want to serve the pages from. Add your html file, commit and push to github. Give it a few minutes, then your file will be served from\nhttps:///GITHUBUSERNAME.github.io/REPONAME/FILENAME.html\nwhere here GITHUBUSERNAME is either your github username or the organization. REPONAME is the name of your repository and FILENAME.html is your html file, with the path if it’s in subdirectories. As an example, here’s a version I did https://bcaffo.github.io/testHtmlRepo/index.html."
  },
  {
    "objectID": "markdown.html",
    "href": "markdown.html",
    "title": "1  Markdown",
    "section": "",
    "text": "Before getting started, you need to learn some markdown. Markdown is a markup language (like HTML) that is absurdly easy. Every data scientist needs to know markdown. Fortunately, you’re five minutes away from knowing it. A markdown file is a text file that needs to be rendered to look nice. If you want an example, this page was written in markdown. To try it, go to google colab or a jupyter lab, create a markdown cell and start editing. Try this cheat sheet."
  },
  {
    "objectID": "unix.html",
    "href": "unix.html",
    "title": "2  Unix",
    "section": "",
    "text": "Some basic unix commands will go a long way in the course, especially for when you’re working on a remote server. On windows, you can actually install a unix environment on so-called services for unix. So, on windows, you have three options for working with the command line, i) install a linux subsystem and use that, ii) use the DOS command prompt or iii) use powershell. The commands here would only work for option i. However, when I work on a windows system, I tend to just use options ii or iii. Here, we’ll assume you’re working on a linux or unix system, or windows services for linux and you’ll have to read up elsewhere if you want to learn windows proper terminal commands.\nTo get a unix terminal, you have several options. Since we’re promoting jupyter and jupyterlab, just open up the terminal on there. (Again, assuming you’re working on a unix/linux system.)\nThe first thing you should try is figuring out where you’re at. Do this by typing\nprompt> pwd\nThis will show you where you are in the directory structure. If you want to see the contents of the directory try these\nprompt> ls\nprompt> ls -al\nprompt> ls -alh\nAdding the flags -a lists everything, including directories with a weird character in front. The l gives the long format, which gives more information and the h changes the filesize lists to a more human readable format. I also like the option --color. What you get with-alh` is as follows.\ntotal 36K\ndrwxrwxrwx+ 7 codespace root      4.0K Feb 14 14:24 .\ndrwxr-xrwx+ 5 codespace root      4.0K Oct 19 15:21 ..\ndrwxrwxrwx+ 6 codespace codespace 4.0K Feb 14 14:31 book\ndrwxrwxrwx+ 8 codespace root      4.0K Feb 15 21:34 .git\n-rw-rw-rw-  1 codespace codespace  171 Feb 14 14:24 .gitignore\n-rw-rw-rw-  1 codespace codespace    0 Feb 14 14:23 .nojekyll\n-rw-rw-rw-  1 codespace codespace  444 Feb 14 14:24 README.md\ndrwxrwxrwx+ 3 codespace codespace 4.0K Feb 14 14:24 slides\ndrwxrwxrwx+ 7 codespace codespace 4.0K Oct 19 15:21 .venv\ndrwxrwxrwx+ 2 codespace codespace 4.0K Oct 19 15:23 .vscode\nThe drwxrwxrwx+1 looking columns give permissions d=directory, r=read, w=write and x=execute, the groups are owner (you), group, everyone. So a file that is -rw------- can be read and written to by the owner, but cannot be executed by anyone and no one else can read or write to it (except the superuser, who gets to do everything).\nTo change a directory, try the following\nprompt> cd DIRECTORY\nwhere DIRECTORY is the name of the directory that you want to change into. You can hit TAB to autocomplete names. The command\nprompt> mv PATH_TO_INPUT_FILE PATH_TO_OUTPUT_FILE\nmoves the file. This is also how you rename a file, since you could just do mv FILENAME1 FILENAME2 and change the name.\nThe unix command for removing things is rm. So\nrm FILENAME\ndeletes the file. Note linux really deletes things, so do this with some care. You can’t remove directories this way, instead you could do rmdir DIRECTORY, but the directory has to be empty. If you want to use rm to remove a directory and its contents, you can do rm -rf DIRECTORY. However, use this with care.\nFinally, I find it very useful to use wget to grab files from the internet. So, for example,\nwget https://URL.../FILENAME\nwill grab the file from that link. Super useful.\nThat’s enough unix to get you started. You’ll find as you use the terminal more and more, you’ll like it better and better. Eventually, you’ll find GUIs kind of frustrating."
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "Python",
    "section": "",
    "text": "First, you’ll need a place to program python for data analysis. Python has a dizzying array of options for its use. A first choice is whether you’ll use python locally (installed on your computer) or in the cloud. The cloud options take care of a lot of installation problems, in exchange for a loss in control and typically much less computing resources unless you pay for stronger cloud computing. A second choice is whether you’ll look program in notebook environments or in a straight code editors. Notebooks mix code and documentation and are especially useful for programming for data analyses. More pure code editors and integrated development environments are preferable for writing software. Here’s a list of some of things I’ve tried and liked."
  },
  {
    "objectID": "python.html#notebooks",
    "href": "python.html#notebooks",
    "title": "Python",
    "section": "Notebooks",
    "text": "Notebooks\nNotebooks are going to be especially useful for us, as they’re a great way to do data analyses. With notebooks, you can merge richer documentation together with analysis code. You can take this to the extreme, and have solutions that create reproducible final documents. This book is an example, where the entire thing is written in jupyter-book. We’ll discuss this idea a little more when we discuss reproducible research. Alternatively, you can use your notebook as a working document that\nMost notebook solutions have text blocks and code blocks. The text is marked up in a markup language called “Markdown”, which we discussed eariler.\nIf you’re very new to notebooks in python, I would suggest starting with colab. The colab documentation is useful."
  },
  {
    "objectID": "python.html#weaved-text-formats",
    "href": "python.html#weaved-text-formats",
    "title": "Python",
    "section": "Weaved text formats",
    "text": "Weaved text formats\nI wrote this book in a format called quarto (see https://quarto.org/ ). This is a slightly different approach than jupyter notebooks and are perhaps better at producing final document-style output. Other approaches similar to quarto documents include R markdown, sweave and org mode."
  },
  {
    "objectID": "basic_python.html",
    "href": "basic_python.html",
    "title": "4  Python basics",
    "section": "",
    "text": "In this section, we will cover some basic python concepts. Python is an extremely quick language to learn, but like most programming languages, can take a long time to master. In this class, we’ll focus on a different style of programming than typical software development, programming with data. This will but less of a burden on us to be expert software developers in python, but some amount of base language knowledge is unavoidable. So, let’s get started learning some of the python programming basics. I’m going to assume you’ve programmed before in some language. If that isn’t the case, consider starting with a basic programming course of study before trying this book.\nA great resource for learning basic python is the python.org documentation https://docs.python.org/3/tutorial/index.html. My favorite programming resource of all time is the “Learn X in Y” tutorials. Here’s one for python https://learnxinyminutes.com/docs/python/.\nSome of the basic programming types in python are ints, floats, complex and Boolean. The command type tells us which. Here’s an example with 10 represented in 4 ways (int, float, string and complex) and the logical value True. Note, we’re using print to print out the result of the type command. If you’re typing directly into the python command line (called the repl, for read, evaluate, print, loop), you won’t need the print statements. But, if you’re using a notebook you probably will.\nIf you want an easy repl environment to program in, try https://replit.com/. For an easy notebook solution to try out, look at google colab, https://colab.research.google.com .\nThese types are our basic building blocks. There’s some other important basic types that build on these. We’ll cover these later, but to give you a teaser:\nTypes can be converted from one to another. For example, we might want to change our 10 into different types. Here’s some examples of converting the integer 10 into a float. First, we use the float function. Next we define a variable a that takes the integer value 10, then use a method associated with a to convert the type. If you’re unfamiliar with the second notation, don’t worry about that now, you’ll get very used to it as we work more in python.\nPython’s repl does all of the basic numerical calculations that you’d like. It does dynamic typing so that you can do things like add ints and floats. Here we show the basic operators, and note # is a comment in python.\nStrings are easy to work with in python. Type print(\"Hello World\") in the repl just to get that out of the way. Otherwise, + concatenates strings and brackets reference string elements. Here’s some examples. Remember counting starts at 0 and negative numbers count from the back.\nThe strings True and False are reserved for the respective Boolean values. The operators ==, >, <, >=, <= and != are the testing operators while and, or and is are Boolean operators. Here are some examples.\nDon’t define new variables called TRUE or FALSE or tRuE or FaLsE, or whatever, even though you can. Just get used to typing True and False the way python likes and don’t use similar named things for other reasons. Als, python as bitwise logical operators |, & and ~. On Boolean values, they work the same but differ in other circumstances. So, if you are unfamliar with bitwise operations, it’s probably better to stick to the word logical operators above."
  },
  {
    "objectID": "basic_python.html#data-structures",
    "href": "basic_python.html#data-structures",
    "title": "4  Python basics",
    "section": "4.1 Data structures",
    "text": "4.1 Data structures\nPython has some more advanced data structures that build on its primitive types.\n\nLists: ordered collections of objects\nSets: like lists but only have unique elements\nTuples: like lists, but not mutable, i.e. need to create a new one to modify\nDictionaries: named elements that you can reference by their name rather than position\n\nFirst, let’s look at some list operations.\n\ndat = [1, 4, 8, 10] # define a list\nprint(dat[0])       # reference an element\nprint(dat[2 : 4])   # reference elements\nprint(dat[2 : ]) \nprint(dat[:2])\ndat2 = [dat, dat]        # creating a list of lists\nprint(dat2)\nprint(dat2[1][2])        # referencing an item in a nested list\ndat3 = [dat2, \"string1\"] # mixed types\nprint(dat3)\ndat4 = dat + dat         # list concatenation\nprint(dat4)\n\n1\n[8, 10]\n[8, 10]\n[1, 4]\n[[1, 4, 8, 10], [1, 4, 8, 10]]\n8\n[[[1, 4, 8, 10], [1, 4, 8, 10]], 'string1']\n[1, 4, 8, 10, 1, 4, 8, 10]\n\n\nNow, let’s look at dictionaries.\n\ndict = {\"a\" : 1, \"b\" : 2} # Create a dictionary of two elements named a and b taking values 1 and 2 respectively\nprint(dict)\nprint(dict['a'])          # reference the element named a\n\n{'a': 1, 'b': 2}\n1\n\n\nSets and tuples are similar to lists, however with some important distinctions. Sets, contain only unique elements and tuples are immutable lists.\n\nset1 = {\"a\", \"b\", \"c\"}\nset2 = {\"a\", 1, True}\nset3 = {\"a\", \"b\", \"c\", \"c\"}\nprint(set1)\nprint(set2)\nprint(set3)\n\n{'b', 'a', 'c'}\n{1, 'a'}\n{'b', 'a', 'c'}\n\n\nHere’s an example to illustrate a tuple.\n\nlist1 = [\"a\", \"b\", \"c\"]\ntuple1 = (\"a\", \"b\", \"c\")\nlist1[0] = \"aa\" #Works just fine\nlist1\n#tuple1[0] = \"aa\" #doesn't work\n\n['aa', 'b', 'c']\n\n\n\n4.1.1 Mutable in immutable entities\nWhen working with objects in python, mutable and immutable elements act differently. Lists are mutable. So, below, the element y gets appended along with x.\n\nx = [10]\ny = x\nx.append(20)\n## Notice y has the appended element\nprint(y)\n## let's try again, as of now x = [10, 20] \nx[0] = x[0] + 11\n## Now x = [21, 20], but did y change?\nprint(y)\n\n[10, 20]\n[21, 20]\n\n\nThings like numbers and strings are immutable. Notice that changing y does not change x.\n\nx = 10\ny = x\nx = x + 10\nprint((x, y))\n\n(20, 10)"
  },
  {
    "objectID": "python_programming.html",
    "href": "python_programming.html",
    "title": "5  Python programming",
    "section": "",
    "text": "# do this if you'd like to prompt for an input\n# x = input(\"are you mean (y/n)? > \")\n# Let's just assume the user input 'n'\nx = 'n'\nif x == 'y': \n print(\"Slytherine!\")\nelse:\n print(\"Gryffindor\")\n\nGryffindor\n\n\nJust to further describe white space useage in python, consider testing whether statementA is True. Below, statementB is executed as part of the if statement whereas statementC is outside of it because it’s not indented. This is often considered an eye rolling aspect of the language, but I think it’s nice in the sense that it bakes good code identation practices into the language.\n## Some more about white space\nif statementA:\n  statementB   # Executed if statementA is True\nstatementC     # Executed regardless since it's not indented\nThe generic structure of if statements in python are\nif statement1 :\n ...\nelif statement2 :\n ...\nelse \n ...\nHere’s an example (note this is just equal to the statement (a < 0) - (a > 0)\n\na = 5\n\nif a < 0 :\n  a = -1\nelif a > 0 :\n  a = 1\nelse :\n  a = 0\n\nprint(a)\n\n1\n\n\nfor and while loops can be used for iteration. Here’s some examples\n\nfor i in range(4) :\n print(i)\n\n0\n1\n2\n3\n\n\n\nx = 4\nwhile x > 0 :\n x = x - 1\n print(x)\n\n3\n2\n1\n0\n\n\nNote for loops can iterate over list-like structures.\n\nfor w in 'word':\n print(w)\n\nw\no\nr\nd\n\n\nThe range function is useful for creating a structure to loop over. It creates a data type that can be iterated over, but isn’t itself a list. So if you want a list out of it, you have to convert it.\n\na = range(3)\nprint(a)\nprint(list(a))\n\nrange(0, 3)\n[0, 1, 2]"
  },
  {
    "objectID": "python_practice.html",
    "href": "python_practice.html",
    "title": "9  Python in practice",
    "section": "",
    "text": "The kind of programming we’ve seen so far in python isn’t how typical data programmming in python goes. Instead, we tend to rely a lot of modules that add methods to our complex data science objects. Most python objects are class objects that come with a variety of convenient methods associated with them. If you’re working in a good coding environment, then it should have some method autocompletion for your objects, which helps prevent typos and can speed up work. Let’s look at methods associated with a list object. Note that some methods change the object itself while others return things without changing the object.\nA useful working example is working with imaginary numbers.\nLet’s create our own version of a complex number, adapted from here. Complex numbers have two parts, the real part and the “imaginary” part. (Note I put imaginary in quotes, since, IMHO, complex numbers and the associated operations are simply an algebraic system, no more or less imaginary than most other algebraic systems. They simply lack the direct real world counting analogy of squaring a whole number. But there are countless algenbraic systems created for a variety of uses, most lacking the direct analogy to counting whole numbers.) There’s two popular representations of complex numbers, Cartesian and polar. We’ll use Cartesian and not discuss polar.\nA complex number is represented as \\(a + bi\\) where \\(a\\) and \\(b\\) are numbers and \\(i\\) is the symbol for the complex root of -1, i.e. \\(i^2 = -1\\). If $a + b i $ is a complex number, \\(a - bi\\) is its so-called conjugate. Conjugates are useful since \\[ (a+bi)(a-bi) = a^2 + abi -\nabi - b^2 i^2 = a^2 + b^2 \\]\nLet’s create an object called mycomplex and give it a conjugation method and some other stuff.\nLet’s now create a version that doesn’t modify the object when we conjugate."
  },
  {
    "objectID": "python_practice.html#utilizing-python-libraries",
    "href": "python_practice.html#utilizing-python-libraries",
    "title": "6  Python in practice",
    "section": "6.1 Utilizing python libraries",
    "text": "6.1 Utilizing python libraries\nWe typically load libraries that create useful data structures for us and have useful functions. We’ll use pandas a lot next. So, let’s go through another very useful data science library, numpy.\nFirst, let’s load up numpy. Here’s three separate ways\n\nimport numpy\nimport numpy as np\nfrom numpy import *\n\nOption 1. imports numpy, but then you have to type numpy.FUNCTION to access FUNCTION. The second option (my preferred) shortens this to np.FUNCTION. The third loads the numpy functions into the global namespace. This is probably ok for really core packages like numpy. But, otherwise it’s an issue since you typically load many libraries and some may have the same function names.\nLet’s load up numpy and look at some of its capabilities.\n\nimport numpy as np\n\n## Numpy has constants\nprint(np.pi)\n## Numpy has a not a number placeholder\nprint(np.nan)\n## Numpy has 1 and 2d arrays\nvector = np.array([1, 2, 3, 8])\nprint(vector)\nprint(type(vector))\n## Numpy has N-Dimensional arrays\narray = np.array([ [1, 2, 3], [4, 5, 6]])\nprint(array)\n\n3.141592653589793\nnan\n[1 2 3 8]\n<class 'numpy.ndarray'>\n[[1 2 3]\n [4 5 6]]\n\n\n\n## Arrays have operator definitions\nprint(array + array)\nprint(array * array)\n\n[[ 2  4  6]\n [ 8 10 12]]\n[[ 1  4  9]\n [16 25 36]]\n\n\nLet’s look at a common use of a library object. Our vector, vector, has some data in it. What if we wanted the mean and standard deviation?\n\nprint(vector.mean())\nprint(vector.std())\n\n3.5\n2.692582403567252\n\n\nNumpy has a separate data structure for matrices (2D arrays). Let’s creat a matrix like our previous 2D array.\n\nmymat = np.matrix([ [1, 2], [4, 5] ] )\nprint( (type(array), type(mymat) ) )\nmymat\n\n(<class 'numpy.ndarray'>, <class 'numpy.matrix'>)\n\n\nmatrix([[1, 2],\n        [4, 5]])\n\n\nNumpy’s linear algebra functions are spread across sublibraries of numpy. linalg is one. Let’s suppose we want the matrix determinant of mymat. We have several choices\n\nnp.linalg.det(mymat)\nimport numpy.linalg as la then la.det(mymat)\nfrom numpy.linalg import det then det(mymat)\n\nThe first is a little long, but just calls the sublibrary then the function of the sublibrary. The second and third are shorter, where the second gives a named reference to the sublibrary methods while the third loads the function into the namespace.\n\nfrom numpy.linalg import det \ndet(mymat)\n\n-2.9999999999999996\n\n\n\nx = np.array([1.1, 2.1, 3.1, 3.2, 8.6])\nx[1:3]\n\narray([2.1, 3.1])"
  },
  {
    "objectID": "python_practice.html#building-python-modules",
    "href": "python_practice.html#building-python-modules",
    "title": "9  Python in practice",
    "section": "9.2 Building python modules",
    "text": "9.2 Building python modules\nWith very small effort, a set of python functions can be turned into a python library. Let’s start with creating a python module, then we’ll talk about distributing with pip"
  },
  {
    "objectID": "python_practice.html#example-utilizing-python-libraries-numpy",
    "href": "python_practice.html#example-utilizing-python-libraries-numpy",
    "title": "9  Python in practice",
    "section": "9.1 Example utilizing python libraries, numpy",
    "text": "9.1 Example utilizing python libraries, numpy\nWe typically load libraries that create useful data structures for us and have useful functions. We’ll use pandas a lot next. So, let’s go through another very useful data science library, numpy.\nFirst, let’s load up numpy. Here’s three separate ways\n\nimport numpy\nimport numpy as np\nfrom numpy import *\n\nOption 1. imports numpy, but then you have to type numpy.FUNCTION to access FUNCTION. The second option (my preferred) shortens this to np.FUNCTION. The third loads the numpy functions into the global namespace. This is probably ok for really core packages like numpy. But, otherwise it’s an issue since you typically load many libraries and some may have the same function names.\nLet’s load up numpy and look at some of its capabilities.\n\nimport numpy as np\n\n## Numpy has constants\nprint(np.pi)\n## Numpy has a not a number placeholder\nprint(np.nan)\n## Numpy has 1 and 2d arrays\nvector = np.array([1, 2, 3, 8])\nprint(vector)\nprint(type(vector))\n## Numpy has N-Dimensional arrays\narray = np.array([ [1, 2, 3], [4, 5, 6]])\nprint(array)\n\n3.141592653589793\nnan\n[1 2 3 8]\n<class 'numpy.ndarray'>\n[[1 2 3]\n [4 5 6]]\n\n\n\n## Arrays have operator definitions\nprint(array + array)\nprint(array * array)\n\n[[ 2  4  6]\n [ 8 10 12]]\n[[ 1  4  9]\n [16 25 36]]\n\n\nLet’s look at a common use of a library object. Our vector, vector, has some data in it. What if we wanted the mean and standard deviation?\n\nprint(vector.mean())\nprint(vector.std())\n\n3.5\n2.692582403567252\n\n\nNumpy has a separate data structure for matrices (2D arrays). Let’s creat a matrix like our previous 2D array.\n\nmymat = np.matrix([ [1, 2], [4, 5] ] )\nprint( (type(array), type(mymat) ) )\nmymat\n\n(<class 'numpy.ndarray'>, <class 'numpy.matrix'>)\n\n\nmatrix([[1, 2],\n        [4, 5]])\n\n\nNumpy’s linear algebra functions are spread across sublibraries of numpy. linalg is one. Let’s suppose we want the matrix determinant of mymat. We have several choices\n\nnp.linalg.det(mymat)\nimport numpy.linalg as la then la.det(mymat)\nfrom numpy.linalg import det then det(mymat)\n\nThe first is a little long, but just calls the sublibrary then the function of the sublibrary. The second and third are shorter, where the second gives a named reference to the sublibrary methods while the third loads the function into the namespace.\n\nfrom numpy.linalg import det \ndet(mymat)\n\n-2.9999999999999996\n\n\n\nx = np.array([1.1, 2.1, 3.1, 3.2, 8.6])\nx[1:3]\n\narray([2.1, 3.1])"
  },
  {
    "objectID": "intro_markdown.html",
    "href": "intro_markdown.html",
    "title": "1  Markdown",
    "section": "",
    "text": "As mentioned, markdown is markup language. So, you write in plain text and then it needs to be rendered into a pretty document or page. For example, all of these notes were written in markdown, but then converted to HTML. There are different flavors of markdown. So, syntax can change a bit. I’m using the one that works in quarto.\nHere’s how you section in markdown.\n# Top level heading\n## Second level heading\n### Third level heading\nand so on. It should look something like this, though the style can change depending on how it is being rendered.\n Top level heading \n Second level heading \n Third level heading \nYou can can bold text and italicize text like this:\n**bold** text and *italicize*\nIf you want an ordered list, like this:\n\nItem one.\nItem two.\nItem three.\n\nthen you can just do:\n1. Item one.\n2. Item two.\n3. Item three.\nNote, if you move around the order of the list in the markdown code, the markup engine will just reorder them starting at whatever the first number is. An unordered lists can be done with asterisks or plus signs or dashes. (I tend to use asterisks.) Also, putting brackets with an x makes for a check mark.\n* [ ] Pick up broccoli\n* [ ] Pick up oat milk\n* [x] Pick up golden berries\n* [x] Pick up tea\n\nPick up broccoli\nPick up oat milk\nPick up golden berries\nPick up tea\n\nInline code, like lambda x: x ** 2 can be written with backticks like this:\n`lambda x: x ** 2`\nBlock code is written in between three backticks.\n```\nlike this\n```\nLinks can be done like this:\n[Markdown cheat sheet](https://www.markdownguide.org/cheat-sheet/).\nwhich renders like this: Markdown cheat sheet. (Also, that’s a real link to a nice MD cheat sheet.) Images can be done like this\n![Image alt text](assets/images/book_graphic.png)\nIf your converter can use mathjax, or some other LaTeX math rendering library, you can insert LaTeX equations. For example,\n\\[\n\\int_{-\\infty}^\\infty (2\\pi\\sigma^2)^{-1/2} \\exp\\left\\{-\\frac{1}{2\\sigma^2} (x - \\mu)^2\\right\\} dx = 1\n\\]\ncan be written as\n$$\n\\int_{-\\infty}^\\infty (2\\pi\\sigma^2)^{-1/2} \\exp\\left\\{-\\frac{1}{2\\sigma^2} (x - \\mu)^2\\right\\} dx = 1\n$$\nI noticed the github markdown renderer doesn’t load mathjax, but most of the data science things do, like jupyter-lab, colab and quarto.\nThat’s plenty of markdown to start. Try it out. You’ll find that you pick it up really fast."
  },
  {
    "objectID": "intro_unix.html",
    "href": "intro_unix.html",
    "title": "2  Unix",
    "section": "",
    "text": "To get a unix terminal, you have several options. Since we’re promoting jupyter and jupyterlab, just open up the terminal on there. (Again, assuming you’re working on a unix/linux system.)\nThe first thing you should try is figuring out where you’re at. Do this by typing\nprompt> pwd\nThis will show you where you are in the directory structure. If you want to see the contents of the directory try these\nprompt> ls\nprompt> ls -al\nprompt> ls -alh\nAdding the flags -a lists everything, including directories with a weird character in front. The l gives the long format, which gives more information and the h changes the filesize lists to a more human readable format. I also like the option --color. What you get with-alh` is as follows.\ntotal 36K\ndrwxrwxrwx+ 7 codespace root      4.0K Feb 14 14:24 .\ndrwxr-xrwx+ 5 codespace root      4.0K Oct 19 15:21 ..\ndrwxrwxrwx+ 6 codespace codespace 4.0K Feb 14 14:31 book\ndrwxrwxrwx+ 8 codespace root      4.0K Feb 15 21:34 .git\n-rw-rw-rw-  1 codespace codespace  171 Feb 14 14:24 .gitignore\n-rw-rw-rw-  1 codespace codespace    0 Feb 14 14:23 .nojekyll\n-rw-rw-rw-  1 codespace codespace  444 Feb 14 14:24 README.md\ndrwxrwxrwx+ 3 codespace codespace 4.0K Feb 14 14:24 slides\ndrwxrwxrwx+ 7 codespace codespace 4.0K Oct 19 15:21 .venv\ndrwxrwxrwx+ 2 codespace codespace 4.0K Oct 19 15:23 .vscode\nThe drwxrwxrwx+1 looking columns give permissions d=directory, r=read, w=write and x=execute, the groups are owner (you), group, everyone. So a file that is -rw------- can be read and written to by the owner, but cannot be executed by anyone and no one else can read or write to it (except the superuser, who gets to do everything).\nTo change a directory, try the following\nprompt> cd DIRECTORY\nwhere DIRECTORY is the name of the directory that you want to change into. You can hit TAB to autocomplete names. The command\nprompt> mv PATH_TO_INPUT_FILE PATH_TO_OUTPUT_FILE\nmoves the file. This is also how you rename a file, since you could just do mv FILENAME1 FILENAME2 and change the name.\nThe unix command for removing things is rm. So\nrm FILENAME\ndeletes the file. Note linux really deletes things, so do this with some care. You can’t remove directories this way, instead you could do rmdir DIRECTORY, but the directory has to be empty. If you want to use rm to remove a directory and its contents, you can do rm -rf DIRECTORY. However, use this with care.\nFinally, I find it very useful to use wget to grab files from the internet. So, for example,\nwget https://URL.../FILENAME\nwill grab the file from that link. Super useful.\nThat’s enough unix to get you started. You’ll find as you use the terminal more and more, you’ll like it better and better. Eventually, you’ll find GUIs kind of frustrating."
  },
  {
    "objectID": "intro_git.html",
    "href": "intro_git.html",
    "title": "3  Git, github",
    "section": "",
    "text": "In the live versions of these classes, we use the version control system git and git hosting service github. If you work in data science should have a working knowledge of both git and at least one cloud hosting service (like github). For git, you work in a repository, which is basically a project directory on your computer with some extra files that help git work. Git is then used for version control so that you keep track of states of your project. Github, is a hosting service for git repositories. Typically, you have your repository on your computer and you coordinate it with the one on the server. Github is just one of several hosting services, bitbucket is another, or you could even relatively easily start your own. However, github has front end web services that allows you to interact with your remote repository easily. This is very convenient."
  },
  {
    "objectID": "intro_git.html#the-least-you-need-to-know",
    "href": "intro_git.html#the-least-you-need-to-know",
    "title": "3  Git, github",
    "section": "3.1 The least you need to know",
    "text": "3.1 The least you need to know\nI’m not going to fully recreate git / github tutorials here; here’s one I recommend by Sean Kross. Instead, I’m going to go through a typical git / github workflow.\n\nInitialization I almost always initialize my git repository on github with a readme.md file.\nClone I typically clone the repository to my local computer using the command line or a local git gui that works with github, like this one. Note that you only have to clone the repo once. After it’s cloned you have a full local copy of the repository.\nadd new files to track and stage them after I’ve worked with them.\ncommit the changes to the local repository with a meaningful commit message.\npush the changes to the repository.\nIf there’s changes on the remote repository not represented in my local repository, I pull those changes to my local repo.\n\nFor larger projects, you’re likely working with multiple people, some of whom you’ve given access to your remote repository and some of whom you have not. The ones who can’t directly push to the remote repo might have their own version of the code and their own version on github. If they think you should incorporate those changes, they might issue a pull request to you. You can then opt to pull their changes into your repo on github, then pull them from github to your local repo. One of the reasons why services like github and bitbucket are so popular is that they make this coordination fairly easy, along with having nice project messaging and management tools.\nIn our class, we use github classroom. For github classroom, you’ll get a link to a repo to put your submission files into. When you push to the remote repository, you’ll have submitted. But, up to the due date you can make changes an push again."
  },
  {
    "objectID": "intro_git.html#a-little-more-detail",
    "href": "intro_git.html#a-little-more-detail",
    "title": "3  Git, github",
    "section": "3.2 A little more detail",
    "text": "3.2 A little more detail\n\n3.2.1 Getting started\nGit is a version control system invented by Linus Torvalds, the invetor of the linux operating system. A github repository is a directory that the software git is using to keep track of versions. Install git, it’s already installed on most cloud services and many operaiting systems. You can initialize a repository in a directory with the command git init in that directory. Honestly, I almost never start a git repo this way, because I’ve connected my repo to a cloud server. But, more on that later. After you’ve initialized it, your git repo is a locally version controlled system.\n\n\n3.2.2 Adding files\nOK, now you want to add some of your files to your repo. You have a directory You can add files with the command\ngit add FILENAME\nThe file FILENAME is now staged to be part of the repository, but isn’t added yet! Files that you haven’t added won’t be tracked. This is good, since there’s many files, llike log files, that we don’t want to track. You only need to add the files once, then they are being tracked. Try it out with a readme markdown file in your repository.\n\n\n3.2.3 Checking status\nOne of the most useful git commands is\ngit status\nThis gives the status of which files are being tracked, which are not and which have changes.\n\n\n3.2.4 Commiting changes\nYou have to actually commit your changes to the repository to have them represented. Committing changes is a local operation, subsequently, we’ll show you how to coordinate with a remote repository. Here’s the command I use to commit most often\ngit commit -a -m \"COMMIT MESSAGE\"\nHere the flag -a means commit all files staged for commits. You can also commit individual files or a collection of specific files. The -m \"COMMIT MESSAGE\" is the message associated with your commit. Make your commit messages more informative. If you don’t add -m github will bring up an editor for you to enter your commit message.\n\n\n3.2.5 Remote repositories\nGit becomes much more useful when it’s coordinated with a remote repository. Github is a server for hosting remote git repositories, there are several others, bitbucket is an example. Github also includes a great deal of functionality for working with the remote repository through the website. There’s many git and github guis that you could use if you want. I always liked to github one and magit in emacs.\n\n3.2.5.1 Connecting your remote and local repos\nYou need to connect your local repo to the remote repo. First create a version of the repo on github (actually then it gives you instructions). Then you can either\n\nClone the repo from the server git clone REMOTE SERVER\nConnect the server to your local repository\n\nFor 2. you use a command like:\ngit remote add origin git@github.com:USERNAME/REPONAME.git\nPersonally, I find it easier just to create the repo on github and clone it. You can clone over ssh or https, if you don’t know what ssh is use https and then switch over to ssh as you learn how to use it.\n\n\n3.2.5.2 Pushing changes\n\n\n\n\n\nYou push from the repo you’re working on to another (remote repo). So, for me, this typically means pushing from my local computer repo to github or to a scientific computing cluster.\ngit push origin main\nThe above command could be read as “push my new committed changes from my origin to the main branch on my remote repo”. Older repos had the default branch master instead of main. But, github switched in 2020 or so. So, some older documentation may not have this switch.\n\n\n3.2.5.3 Pulling changes\n\n\n\n\n\nThere might be changes that you want represented locally that were changed on the remote repository; for example, you might edit a readme file through github’s web interface. The command for this is really easy, it’s just pull\ngit pull\nThe pain occurs if you have made local changes that are in conflict with the remote changes. I’d write about merge conflicts, but I can’t, I just can’t (shudder). [Here’s some docs on managing merge conflicts] (https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/addressing-merge-conflicts/resolving-a-merge-conflict-using-the-command-line).\n\n\n3.2.5.4 Cloning\n\n\n\n\n\nTo clone a remote repository is to simply create a local copy where you can pull changes from the remote. If a repository is public and you own it, then you can clone it and push changes back to the remote. If you don’t, you can clone and pull changes, but then can’t push back to the remote.\nTry with the book repo, clone it so you have access to the files. In the figure below the owner can pull and push to the repo. In the picture below, consider that you clone GH1 off of github to you. You can pull updates from GH1 but can’t push changes to GH1. The owner of GH1 of course can pull and push.\nAs an example, close this book (via https) with:\ngit clone https://github.com/smart-stats/ds4bio_book.git\n\n\n3.2.5.5 Forking\n\n\n\n\n\nA common workflow for github is to “fork” the repository. This differs from cloning in the sense that your fork is an entirely separate new copy. In the picture above, GH2 is a fork of GH1. It can pull upstream changes from GH1, but can’t write to it, since you don’t own that repo. You can push and pull from Local2 which is connected to GH2.\nForking on github is easy. Just click the “fork” button in the upper right corner.\n\n\n3.2.5.6 Pull requests\n\n\n\n\n\nImagine a case where you forked the book repo and have some great additions (maybe a section on merge conflicts?). You think that these changes you think should be represented for other students. You can’t push changes to my repo; the very idea would be perposterous! How do you get your great changes represented in my repo?\nYou issue a pull request in the sense of requesting GH1 to pull your changes from GH2. Then, I would get notified of the pull request and would decide whether or not to merge them into my repo. This system has many benefits, and particularly, is great for operating on open source projects.\nI like to do pull requests directly on github through the web interface."
  },
  {
    "objectID": "intro_git.html#branching",
    "href": "intro_git.html#branching",
    "title": "3  Git, github",
    "section": "3.3 Branching",
    "text": "3.3 Branching\nYou often want multiple versions of a repository. For example, you might want a development version, a working version and a stable version. This is what branches are for in git. To create and switch to a branch called dev, use:\ngit checkout -b dev\nThis creates a new branch called dev and switches to that branch. If you already have dev, use checkout without -b. Suppose you want to merge your change from dev into your default branch (main)\ngit checkout main\ngit merge dev\nStart very basic with branches, then when you get the hang of it you can use it like a pro. Here’s more documentation."
  },
  {
    "objectID": "intro_git.html#clients",
    "href": "intro_git.html#clients",
    "title": "3  Git, github",
    "section": "3.4 Clients",
    "text": "3.4 Clients\nYou can avoid the command line stuff with a git client, of which there are many. It’s still good to know the command line options, for remote work if nothing else. I’ve tried several git clients and my favorite is the github client software and magit in emacs."
  },
  {
    "objectID": "intro_git.html#setting-up-ssh",
    "href": "intro_git.html#setting-up-ssh",
    "title": "3  Git, github",
    "section": "3.5 Setting up ssh",
    "text": "3.5 Setting up ssh\nIf you want to connect to a remote repository to github on a unix or linux system, it’s probably easiest to set up using so called secure shell. You can also set it up a variety of ways on Windows, but the rules are a little less universal. If you find this very hard to follow, just use a git and github gui (like github desktop, which is what I like to recommend).\nSecure shell works as follows. It creates a public key (think like a lock, everyone can see it) and a private key (like the key for the lock, only you have it). When you connect to github, it has your public key and you use the private key to “unlock” it. First, you need a .ssh directory from your personal directory. If you don’t have that, try ssh username@server to a server that you know and it will automatically create one.\nwe’ll go through this tutorial.\nThen, cd into your .ssh director and type the following and follow the instructions\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\nThis generates your public/prviate key pair. Then you need to take the public key file, and upload it to github under settings. This is described here.\nIn some cases I’ve also had to add this to a file config in my .ssh directory:\nHost github.com\n     ForwardX11 no\n     HostName github.com\n     User git\n     IdentityFile ~/.ssh/YOUR_PRIVATE_KEY_FILENAME\nOnce this is setup, then you should be able to push and pull from github from the command line without passwords."
  },
  {
    "objectID": "intro_git.html#github-pages",
    "href": "intro_git.html#github-pages",
    "title": "3  Git, github",
    "section": "3.6 Github pages",
    "text": "3.6 Github pages\nIt’s possible to host web pages on github. By hosting I mean serving the web page on the internet so that others can view it. There’s a couple of steps to doing this. First, you need an html file to serve. If you look in the chapter on html, there’s the code for a basic wepage. Save a web page as say “index.html”. You can double check that it works by simply double clicking on the file on your local computer. It should display as a web page in your browser, but it’s only viewable by you. Notice the addres begins with file:///.... To serve the file so that others can view it, we need it to exist on a server.\nGithub will serve the file for us, but first we need a repository. Create a public repository; I typically do this on github. Then you need to add an empty file called .nojekyll. Don’t forget the period at the beginning. This prevents github from thinking you’re using the jekyll framework, which is the default. Since we’re serving a basic webpage, we don’t need this framework. I create this file on github just by clicking Add File then Create new file.\nNext we need to tell github that this repo serves web pages. Click on settings then pages on the left. Then, under source choose the branch that you want to serve the pages from. Add your html file, commit and push to github. Give it a few minutes, then your file will be served from\nhttps:///GITHUBUSERNAME.github.io/REPONAME/FILENAME.html\nwhere here GITHUBUSERNAME is either your github username or the organization. REPONAME is the name of your repository and FILENAME.html is your html file, with the path if it’s in subdirectories. As an example, here’s a version I did https://bcaffo.github.io/testHtmlRepo/index.html."
  },
  {
    "objectID": "python_basic.html",
    "href": "python_basic.html",
    "title": "5  Python basics",
    "section": "",
    "text": "In this section, we will cover some basic python concepts. Python is an extremely quick language to learn, but like most programming languages, can take a long time to master. In this class, we’ll focus on a different style of programming than typical software development, programming with data. This will but less of a burden on us to be expert software developers in python, but some amount of base language knowledge is unavoidable. So, let’s get started learning some of the python programming basics. I’m going to assume you’ve programmed before in some language. If that isn’t the case, consider starting with a basic programming course of study before trying this book.\nA great resource for learning basic python is the python.org documentation https://docs.python.org/3/tutorial/index.html. My favorite programming resource of all time is the “Learn X in Y” tutorials. Here’s one for python https://learnxinyminutes.com/docs/python/.\nSome of the basic programming types in python are ints, floats, complex and Boolean. The command type tells us which. Here’s an example with 10 represented in 4 ways (int, float, string and complex) and the logical value True. Note, we’re using print to print out the result of the type command. If you’re typing directly into the python command line (called the repl, for read, evaluate, print, loop), you won’t need the print statements. But, if you’re using a notebook you probably will.\nIf you want an easy repl environment to program in, try https://replit.com/. For an easy notebook solution to try out, look at google colab, https://colab.research.google.com .\nThese types are our basic building blocks. There’s some other important basic types that build on these. We’ll cover these later, but to give you a teaser:\nTypes can be converted from one to another. For example, we might want to change our 10 into different types. Here’s some examples of converting the integer 10 into a float. First, we use the float function. Next we define a variable a that takes the integer value 10, then use a method associated with a to convert the type. If you’re unfamiliar with the second notation, don’t worry about that now, you’ll get very used to it as we work more in python.\nPython’s repl does all of the basic numerical calculations that you’d like. It does dynamic typing so that you can do things like add ints and floats. Here we show the basic operators, and note # is a comment in python.\nStrings are easy to work with in python. Type print(\"Hello World\") in the repl just to get that out of the way. Otherwise, + concatenates strings and brackets reference string elements. Here’s some examples. Remember counting starts at 0 and negative numbers count from the back.\nThe strings True and False are reserved for the respective Boolean values. The operators ==, >, <, >=, <= and != are the testing operators while and, or and is are Boolean operators. Here are some examples.\nDon’t define new variables called TRUE or FALSE or tRuE or FaLsE, or whatever, even though you can. Just get used to typing True and False the way python likes and don’t use similar named things for other reasons. Als, python as bitwise logical operators |, & and ~. On Boolean values, they work the same but differ in other circumstances. So, if you are unfamliar with bitwise operations, it’s probably better to stick to the word logical operators above."
  },
  {
    "objectID": "python_basic.html#data-structures",
    "href": "python_basic.html#data-structures",
    "title": "5  Python basics",
    "section": "5.1 Data structures",
    "text": "5.1 Data structures\nPython has some more advanced data structures that build on its primitive types.\n\nLists: ordered collections of objects\nSets: like lists but only have unique elements\nTuples: like lists, but not mutable, i.e. need to create a new one to modify\nDictionaries: named elements that you can reference by their name rather than position\n\nFirst, let’s look at some list operations.\n\ndat = [1, 4, 8, 10] # define a list\nprint(dat[0])       # reference an element\nprint(dat[2 : 4])   # reference elements\nprint(dat[2 : ]) \nprint(dat[:2])\ndat2 = [dat, dat]        # creating a list of lists\nprint(dat2)\nprint(dat2[1][2])        # referencing an item in a nested list\ndat3 = [dat2, \"string1\"] # mixed types\nprint(dat3)\ndat4 = dat + dat         # list concatenation\nprint(dat4)\n\n1\n[8, 10]\n[8, 10]\n[1, 4]\n[[1, 4, 8, 10], [1, 4, 8, 10]]\n8\n[[[1, 4, 8, 10], [1, 4, 8, 10]], 'string1']\n[1, 4, 8, 10, 1, 4, 8, 10]\n\n\nNow, let’s look at dictionaries.\n\ndict = {\"a\" : 1, \"b\" : 2} # Create a dictionary of two elements named a and b taking values 1 and 2 respectively\nprint(dict)\nprint(dict['a'])          # reference the element named a\n\n{'a': 1, 'b': 2}\n1\n\n\nSets and tuples are similar to lists, however with some important distinctions. Sets, contain only unique elements and tuples are immutable lists.\n\nset1 = {\"a\", \"b\", \"c\"}\nset2 = {\"a\", 1, True}\nset3 = {\"a\", \"b\", \"c\", \"c\"}\nprint(set1)\nprint(set2)\nprint(set3)\n\n{'c', 'a', 'b'}\n{1, 'a'}\n{'c', 'a', 'b'}\n\n\nHere’s an example to illustrate a tuple.\n\nlist1 = [\"a\", \"b\", \"c\"]\ntuple1 = (\"a\", \"b\", \"c\")\nlist1[0] = \"aa\" #Works just fine\nlist1\n#tuple1[0] = \"aa\" #doesn't work\n\n['aa', 'b', 'c']\n\n\n\n5.1.1 Mutable in immutable entities\nWhen working with objects in python, mutable and immutable elements act differently. Lists are mutable. So, below, the element y gets appended along with x.\n\nx = [10]\ny = x\nx.append(20)\n## Notice y has the appended element\nprint(y)\n## let's try again, as of now x = [10, 20] \nx[0] = x[0] + 11\n## Now x = [21, 20], but did y change?\nprint(y)\n\n[10, 20]\n[21, 20]\n\n\nThings like numbers and strings are immutable. Notice that changing y does not change x.\n\nx = 10\ny = x\nx = x + 10\nprint((x, y))\n\n(20, 10)"
  },
  {
    "objectID": "python_functions.html",
    "href": "python_functions.html",
    "title": "6  Functions",
    "section": "",
    "text": "def pow(x, n = 2):\n  return x ** n\n\nprint(pow(5, 3))\n\n125\n\n\nNote our function has a mandatory arugment, x, and an optional arugment, n, that takes the default value 2. Consider this example to think about how python evaluates function arguments. These are all the same.\n\nprint(pow(3, 2))\nprint(pow(x = 3, n = 2))\nprint(pow(n = 2, x = 3))\n#pow(n = 2, 3) this returns an error, the second position is n, but it's a named argument too\n\n9\n9\n9\n\n\nYou can look here, https://docs.python.org/3/tutorial/controlflow.html, to study the rules. It doesn’t make a lot of sense to get to cute with your function calling arguments. I try to obey both the order and the naming. I argue that this is the way to go since usually functions are written with some sensible ordering of arguments and naming removes all doubt. Python has a special variable for variable length arguments. Here’s an example.\n\ndef concat(*args, sep=\"/\"):\n return sep.join(args)  \n\nprint(concat(\"a\", \"b\", \"c\"))\nprint(concat(\"a\", \"b\", \"c\", sep = \":\"))\n\na/b/c\na:b:c\n\n\nLambda can be used to create short, unnamed functions. This has a lot of uses that we’ll see later.\n\nf = lambda x: x ** 2\nprint(f(5))\n\n25\n\n\nHere’s an example useage where we use lambda to make specific “raise to the power” functions.\n\ndef makepow(n):\n return lambda x: x ** n\n\nsquare = makepow(2)\nprint(square(3))\ncube = makepow(3)\nprint(cube(2))\n\n9\n8"
  },
  {
    "objectID": "intro_markdown.html#sectioning",
    "href": "intro_markdown.html#sectioning",
    "title": "1  Markdown",
    "section": "1.1 Sectioning",
    "text": "1.1 Sectioning\nHere’s how you section in markdown.\n# Top level heading\n## Second level heading\n### Third level heading\nand so on. You can can bold text and italicize text like this:\n**bold** text and *italicize*\nIf you want an ordered list, like this:\n\nItem one.\nItem two.\nItem three.\n\nthen you can just do:\n1. Item one.\n2. Item two.\n3. Item three.\nNote, if you move around the order of the list in the markdown code, the markup engine will just reorder them starting at whatever the first number is. An unordered lists can be done with asterisks or plus signs. (I use asterisks)\n\nPick up broccoli\nPick up oat milk\nPick up golden berries\nPick up tea\n\n* Pick up broccoli\n* Pick up oat milk\n* Pick up golden berries\n+ Pick up tea\nTry this cheat sheet."
  },
  {
    "objectID": "python_virtual_environments.html",
    "href": "python_virtual_environments.html",
    "title": "7  Virtual Environments",
    "section": "",
    "text": "One of the nicer aspects of python is its virtual environment capabilities. Before we discuss virtual environments, we should discuss the Python installer program (pip). Pip is a little program to help you install python packages. In fact, it makes installing python packages trivial. The packages need to be hosted on the Python Package Index (PyPI).\nInstalling pip depends on your system and how you’d like to do it. It’s also best to know a little bit about virtual environments before you undertake this task.\nLet’s assume you have pip installed and you want to install pandas, usually something like\njust works. However, again, read below before you do this."
  },
  {
    "objectID": "python_virtual_environments.html#running-python",
    "href": "python_virtual_environments.html#running-python",
    "title": "7  Virtual Environments",
    "section": "7.1 Running python",
    "text": "7.1 Running python\nThere’s a few decisions that you have to make before you start running python. The first is, what sort of development environment will you choose? For me, there’s 3 realistic choices\n\nRunning python natively on the OS.\nRunning python within its virtual environment\nRunning python within a conda virtual environment\n\nOption 1 is to simply install (if not already installed) and run python. An example of this would be to install python from the Windows store. However, this approach has some limitations. Often you want different libraries installed for different, and maybe even different versions of python and other software. This is what virtual environments are created for. To create a virtual environment, simply do\npython -m venv myvenv\nHere python should be python 3. On some systems, the default python is python 3 and on others its python 2. If your default system is python 2, replace the above command with python3 -m .... This should have created a virtual environment in a directory. To activate it, you need to run the command\nmyvenv\\scripts\\activate.bat\non windows or\nsource myvenv/bin/activate\n\non unix type systems. Now your command or terminal prompt should look like this\n(myvenv) prompt>\nThe little (myvenv) reminds you that you’re in that enviroment. When you pip install programs now, they are installed in your environment, not in your general system. Here’s a nice tutorial on python venvs."
  },
  {
    "objectID": "python_virtual_environments.html#conda-and-anaconda",
    "href": "python_virtual_environments.html#conda-and-anaconda",
    "title": "7  Virtual Environments",
    "section": "7.2 Conda and anaconda",
    "text": "7.2 Conda and anaconda\nConda is my preferred solution for managing virtual environments. Conda platform containing miniconda, a command line version of conda, and Anaconda, a graphical variation. I’ll show the commands here, but you can do all of these things with Anaconda navigator if you would prefer. On windows, I would just use Anaconda navigator. When working remotely, you typically have to use the command line.\nWith conda, when you’re in a virtual environment, you’ll see it named at the prompt. For me, it looks something like this.\n(ds4bio) prompt>\nTo create a conda environment you can (either do it in the Anaconda GUI or) type\nconda create --name myvenv2 \nThen activate that environment by (either clicking on it in the Anaconda GUI or) typing\nconda activate myvenv2\nTo deactivate the environment (either select a different environment in Anaconda GUI or) type\nconda deactivate\nInstalls with conda are typically quite easy. Within an activated environment, you can use pip if you want. However, it’s probably easier to just use conda. For example\n(myvenv2) prompt> conda install pandas\ninstalls pandas into myenv2 (which I had already activated). In Anaconda navigator, just navigate to environments, activate the one you want, then search and click to checkmark the package you want to install. The nice thi1ng about conda, is that it installs non-python software as well. For example, if you want to install R into myenv2, try\n(myvenv2) prompt> conda install r-essentials r-base\n(or do it in Anaconda navigator). In fact, you can create an R environment from the start with\nprompt> conda create -n r-environment r-essentials r-base\nor Anaconda navigator has an R environment option on creation."
  },
  {
    "objectID": "python_virtual_environments.html#further-virtualization",
    "href": "python_virtual_environments.html#further-virtualization",
    "title": "7  Virtual Environments",
    "section": "7.3 Further virtualization",
    "text": "7.3 Further virtualization\nIf we think of pip < venv < conda in the terms of increasing virtualization and abstraction of your python environment, we should discuss further virtualization. Docker, for example, creates an entirely virtualized computational environment. In short, an entirely new operating system running within your existing operating system. Other variations of this, like virtualbox, create a nice user interface including the OS GUI.\nThese solutions virtualize the OS, but not the hardware. There are solutions to virtualize every aspect of the computer, qemu is an example. This could be useful if a sort of extreme variation of reproducibility, down to the bitwise operations, is required."
  },
  {
    "objectID": "data_cleaning_example.html",
    "href": "data_cleaning_example.html",
    "title": "9  Data cleaning, an example",
    "section": "",
    "text": "We’re going to cover data cleaning by an example. Primarily, you’re going to work in pandas, a library for manipulating tabular data."
  },
  {
    "objectID": "data_cleaning_example.html#imports-and-files",
    "href": "data_cleaning_example.html#imports-and-files",
    "title": "9  Data cleaning, an example",
    "section": "9.1 Imports and files",
    "text": "9.1 Imports and files\nThe first thing we’ll try is loading some data and plotting it. To do this, we’ll need some packages. Let’s load up pandas, a package for data management, matplotlib for plotting and numpy for numerical manipulations. The python command for this is import.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\n\nwe say as in order to not have to type out the entire module name to access its methods."
  },
  {
    "objectID": "data_cleaning_example.html#reading-data-in-with-pandas",
    "href": "data_cleaning_example.html#reading-data-in-with-pandas",
    "title": "9  Data cleaning, an example",
    "section": "9.2 Reading data in with pandas",
    "text": "9.2 Reading data in with pandas\nLet’s now read in an MRICloud dataset using pandas. We want to use the function read_csv within pandas. Notice we imported pandas as pd so the command is pd.read_csv. Also, pandas can accept URLs, so we just put the link to the file in the argument. The data we want to read in is in a github repo I created.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/kirby127a_3_1_ax_283Labels_M2_corrected_stats.csv\")\n\nYou can see the variables created with locals. However, this shows you everything and you usually have to text process it a little.\nLet’s look at the first 4 rows of our dataframe. The object dataset is a pandas object with associated methods. One is head which allows one to see the first few rows of data.\n\ndf.head(4)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      rawid\n      roi\n      volume\n      min\n      max\n      mean\n      std\n      type\n      level\n    \n  \n  \n    \n      0\n      1\n      kirby127a_3_1_ax.img\n      Telencephalon_L\n      531111\n      0\n      374\n      128.3013\n      51.8593\n      1\n      1\n    \n    \n      1\n      2\n      kirby127a_3_1_ax.img\n      Telencephalon_R\n      543404\n      0\n      300\n      135.0683\n      53.6471\n      1\n      1\n    \n    \n      2\n      3\n      kirby127a_3_1_ax.img\n      Diencephalon_L\n      9683\n      15\n      295\n      193.5488\n      32.2733\n      1\n      1\n    \n    \n      3\n      4\n      kirby127a_3_1_ax.img\n      Diencephalon_R\n      9678\n      10\n      335\n      193.7051\n      32.7869\n      1\n      1"
  },
  {
    "objectID": "data_eda.html",
    "href": "data_eda.html",
    "title": "10  Exploratory data analysis",
    "section": "",
    "text": "A picture is worth a 1,000 words\nOr saying how impactful intrer-ocular content is (i.e. when information hits you right between the eyes).\nI’m using Seaborn as the framework. There’s several plotting frameworks in python, but I find that seaborn has the nicest default plotting options. Also, it’s built on top of matplotlib, which is the main plotting library for DS for python.\nLet’s start with loading up some libraries.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()\n\nThe command sns.set sets the seaborn style. This sets the style for all matplotlib plots, even if not created in seaborn. I like the seaborn style, so I usually set it this way.\nFirst let’s download the data. Then we’ll read it in and drop some columns that aren’t needed for this analysis.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/kirby21.csv\")\ndf = df.drop(['Unnamed: 0', 'rawid', 'min', 'max', 'mean', 'std'], axis = 1)\ndf.head(4)\n\n\n\n\n\n  \n    \n      \n      roi\n      volume\n      type\n      level\n      id\n      icv\n      tbv\n    \n  \n  \n    \n      0\n      Telencephalon_L\n      531111\n      1\n      1\n      127\n      1378295\n      1268519\n    \n    \n      1\n      Telencephalon_R\n      543404\n      1\n      1\n      127\n      1378295\n      1268519\n    \n    \n      2\n      Diencephalon_L\n      9683\n      1\n      1\n      127\n      1378295\n      1268519\n    \n    \n      3\n      Diencephalon_R\n      9678\n      1\n      1\n      127\n      1378295\n      1268519\n    \n  \n\n\n\n\nLet’s look at the Type 1 Level 1 data and create a variable called comp which is brain composition, defined as the regional volumes over total brain volume. We’ll do this by selecting roi and comp then grouping by roi (region of interest) and taking the mean of the compostions.\n\n## Extract the Type 1 Level 1 data\nt1l1 = df.loc[(df['type'] == 1) & (df['level'] == 1)]\n\n\n## create a composition variable\nt1l1 = t1l1.assign(comp = t1l1['volume'] / t1l1['tbv'])\nt1l1 = t1l1.loc[t1l1['roi'] != 'CSF']\n\nLet’s get the mean of the composition variable across subjects by ROI. This is done by grouping by ROI then averaging over composition.\n\nsummary = t1l1[['roi', 'comp']].groupby('roi', as_index=False).mean()\nprint(summary)\n\n               roi      comp\n0   Diencephalon_L  0.007563\n1   Diencephalon_R  0.007634\n2    Mesencephalon  0.008647\n3    Metencephalon  0.124883\n4   Myelencephalon  0.003785\n5  Telencephalon_L  0.420305\n6  Telencephalon_R  0.427184\n\n\nOK, let’s try our first plot, a seaborn bar plot.\n\ng = sns.barplot(x='roi', y = 'comp', data = summary);\n## this is the matplotlib command for rotating \n## axis tick labels by 90 degrees.\nplt.xticks(rotation = 90);\n\n\n\n\nUnfortunately, seaborn doesn’t have a stakced bar chart. However, pandas does have one built in. To do this, however, we have to create a version of the data with ROIs as the columns. This can be done with a pivot statement. This converts our data from a “long” format to a “wide” format.\n\nt1l1pivot = t1l1.pivot(index = 'id', columns = 'roi', values = 'volume')\nt1l1pivot.head(4)\n\n\n\n\n\n  \n    \n      roi\n      Diencephalon_L\n      Diencephalon_R\n      Mesencephalon\n      Metencephalon\n      Myelencephalon\n      Telencephalon_L\n      Telencephalon_R\n    \n    \n      id\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      127\n      9683\n      9678\n      10268\n      159402\n      4973\n      531111\n      543404\n    \n    \n      142\n      9156\n      9071\n      10516\n      165803\n      4912\n      545603\n      552216\n    \n    \n      239\n      8937\n      9004\n      9070\n      124504\n      4023\n      483107\n      490805\n    \n    \n      346\n      8828\n      8933\n      9788\n      135090\n      4428\n      558849\n      568830\n    \n  \n\n\n\n\nNow that the data is in the right format, we can do our plot.\n\nt1l1pivot.plot(kind='bar', stacked=True, legend= False);\nplt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5));\n\n\n\n\nLet’s do some scatterplots. Let’s look at bilateral symmetry of the telencephalon. That is, let’s plot the right telencephalon versus the left telencephalon.\n\nsns.scatterplot(x = 'Telencephalon_L', y = 'Telencephalon_R', data = t1l1pivot);\nplt.xticks(rotation = 90);\n#plot an identity line from the data min to the data max\nx1 = min([t1l1pivot.Telencephalon_L.min(), t1l1pivot.Telencephalon_R.min()])\nx2 = max([t1l1pivot.Telencephalon_L.max(), t1l1pivot.Telencephalon_R.max()])\nplt.plot([x1, x2], [x1 , x2]);\n\n\n\n\nThis plot has the issue that there’s a lot of blank space. This is often addressed via a mean difference plot. This plot shows (X+Y) / 2 versus (X-y). This is basically just rotating the plot above by 45 degrees to get rid of all of the blank space around the diagonal line. Alternatively, you could plot (log(x) + log(y)) / 2 versus log(X) - log(Y). This plots the log of the geometric mean of the two observations versus the log of their ratio. Sometimes people use log base 2 or log base 10.\n\nt1l1pivot = t1l1pivot.assign(Tel_logmean = lambda x: (np.log(x.Telencephalon_L) * .5 +  np.log(x.Telencephalon_R)* .5))\nt1l1pivot = t1l1pivot.assign(Tel_logdiff = lambda x: (np.log(x.Telencephalon_R) -  np.log(x.Telencephalon_L)))\nsns.scatterplot(x = 'Tel_logmean', y = 'Tel_logdiff', data = t1l1pivot);\nplt.axhline(0, color='green');\nplt.xticks(rotation = 90);\n\n\n\n\nThus, apparently, the right side is always a little bigger than the left and the scale of the ratio is \\(e^{0.02}\\) while the scale of the geometric mean is \\(e^{13}\\). Note, \\(\\exp(x) \\approx 1 + x\\) for \\(x \\approx 0\\). So it’s about 2% larger. A note about right versus left in imaging. Often the labels get switched as there are different conventions (is it the right of the subject or the right of the viewer when looking straight at the subject?). Typically, it’s known that some of the areas of subject’s left hemisphere are larger and so it’s probably radiological (right of the viewer) convention here. Here’s a nicely done article about right versus left brain.\n(Also, in case you don’t believe me, here’s a plot of \\(e^x\\) versus \\(1+x\\) for values up to 0.1. This is the so-called Taylor expasion for \\(e^x\\) around 0. Notice the approximation gets worse, the curves diverge, as you get further away from 0.)\n\n## A sequence of numbers from 0 to .1 spaced by 0.001\nx = np.arange(0, .1, .001)\nex = np.exp(x)\n\nsns.lineplot(x = x, y = ex)\nplt.plot(x, x + 1)"
  },
  {
    "objectID": "data_sqlite.html",
    "href": "data_sqlite.html",
    "title": "11  SQL via sqlite",
    "section": "",
    "text": "In this page, we’ll cover some of the basics of SQL (structured querry language) by working through some examples. SQL is a set of language standards for databases, so we have to choose a specific implementation. We’ll use sqlite for this purpose. As its name implies, sqlite is a small implementation of SQL.\nIn my linux implementation, sqlite3 was pre-installed. Here’s a tutorial on installing for windows. Sqlite3 is a single file.\nWe’ll first create a database at the command line. Notice when we create a file\nPerforming an ls in the current working directory now shows the file class.db. Everything else we discuss below assumes working in the sqlite command prompt.\nTo work with sqlite, it’s nice to work with a development environment specifically created for sql. Specifically, one with nice highlighting and autocompletion. Since I’m writing these notes in jupyter, I’m just pasting code output.\nSqlite has SQL commands, which must be typed with a semicolon at the end, and sqlite specific commands, which begin with a period and the pragma commands, which are also sqlite specific. This is good to remember, since some things will be portable to other SQL implementations and others not. ]"
  },
  {
    "objectID": "data_sqlite.html#a-more-reaslistic-example",
    "href": "data_sqlite.html#a-more-reaslistic-example",
    "title": "11  SQL via sqlite",
    "section": "11.1 A more reaslistic example",
    "text": "11.1 A more reaslistic example\nLet’s create and work with a more realistic example. Consider the data Opiods in the US at Open Case Studies https://github.com/opencasestudies/ocs-bp-opioid-rural-urban as described here. Read over their writeup, as we’re mostly going to be showing how to duplicate a lot of their steps in sqlite.\nFirst, you need to download the data, which you could do by right clicking and saving the file or with a command:\nwget https://raw.githubusercontent.com/opencasestudies/ocs-bp-opioid-rural-urban/master/data/simpler_import/county_pop_arcos.csv\nwget https://raw.githubusercontent.com/opencasestudies/ocs-bp-opioid-rural-urban/master/data/simpler_import/land_area.csv\nwget https://raw.githubusercontent.com/opencasestudies/ocs-bp-opioid-rural-urban/master/data/simpler_import/county_annual.csv\nNext, let’s import them into sqlite\ncommand prompt> sqlite3 opioid.db\nsqlite> .mode csv\nsqlite> .import county_pop_arcos.csv population\nsqlite> .import county_annual.csv annual\nsqlite> .import land_area.csv land\nsqlite> .tables\nannual      land        population\nWhat variables do the tables include? The pragma command is unique to sqlite and contains a bunch of helper functions.\nsqlite> pragma table_info(population);\ncid  name          type  notnull  dflt_value  pk\n---  ------------  ----  -------  ----------  --\n0                  TEXT  0                    0 \n1    BUYER_COUNTY  TEXT  0                    0 \n2    BUYER_STATE   TEXT  0                    0 \n3    countyfips    TEXT  0                    0 \n4    STATE         TEXT  0                    0 \n5    COUNTY        TEXT  0                    0 \n6    county_name   TEXT  0                    0 \n7    NAME          TEXT  0                    0 \n8    variable      TEXT  0                    0 \n9    year          TEXT  0                    0 \n10   population    TEXT  0                    0 \nsqlite> pragma table_info(annual);\ncid  name          type  notnull  dflt_value  pk\n---  ------------  ----  -------  ----------  --\n0                  TEXT  0                    0 \n1    BUYER_COUNTY  TEXT  0                    0 \n2    BUYER_STATE   TEXT  0                    0 \n3    year          TEXT  0                    0 \n4    count         TEXT  0                    0 \n5    DOSAGE_UNIT   TEXT  0                    0 \n6    countyfips    TEXT  0                    0\nsqlite> pragma table_info(land)\ncid  name         type  notnull  dflt_value  pk\n---  -----------  ----  -------  ----------  --\n0                 TEXT  0                    0 \n1    Areaname     TEXT  0                    0 \n2    STCOU        TEXT  0                    0 \n3    LND010190F   TEXT  0                    0 \n4    LND010190D   TEXT  0                    0 \n5    LND010190N1  TEXT  0                    0\n(I truncated this latter output at 5.)"
  },
  {
    "objectID": "data_sqlite.html#working-with-data",
    "href": "data_sqlite.html#working-with-data",
    "title": "11  SQL via sqlite",
    "section": "11.2 Working with data",
    "text": "11.2 Working with data\nLet’s print out a few columns of the population data.\nsqlite> select BUYER_COUNTY, BUYER_STATE, STATE, COUNTY, year, population from population limit 5;\nBUYER_COUNTY  BUYER_STATE  STATE  COUNTY  year  population\n------------  -----------  -----  ------  ----  ----------\nAUTAUGA       AL           1      1       2006  51328     \nBALDWIN       AL           1      3       2006  168121    \nBARBOUR       AL           1      5       2006  27861     \nBIBB          AL           1      7       2006  22099     \nBLOUNT        AL           1      9       2006  55485   \nThe limit 5 prints out five rows. Let’s perform some of the tasks in the write up. For example, they want to print out some of the missing data in the annual dataset.\nsqlite> select * from annual where countyfips = \"NA\" limit 10;\n     BUYER_COUNTY  BUYER_STATE  year  count  DOSAGE_UNIT  countyfips\n---  ------------  -----------  ----  -----  -----------  ----------\n188  ADJUNTAS      PR           2006  147    102800       NA        \n189  ADJUNTAS      PR           2007  153    104800       NA        \n190  ADJUNTAS      PR           2008  153    45400        NA        \n191  ADJUNTAS      PR           2009  184    54200        NA        \n192  ADJUNTAS      PR           2010  190    56200        NA        \n193  ADJUNTAS      PR           2011  186    65530        NA        \n194  ADJUNTAS      PR           2012  138    57330        NA        \n195  ADJUNTAS      PR           2013  138    65820        NA        \n196  ADJUNTAS      PR           2014  90     59490        NA        \n197  AGUADA        PR           2006  160    49200        NA   \nHere, we used the condition “NA” to test for missingness, since the CSV files have the string NA values for missing data. Places other than Puerto Rico (PR)? Lets check some\nsqlite> select * from annual where countyfips = \"NA\" and BUYER_STATE != \"PR\" limit 10;\n       BUYER_COUNTY  BUYER_STATE  year  count  DOSAGE_UNIT  countyfips\n-----  ------------  -----------  ----  -----  -----------  ----------\n10072  GUAM          GU           2006  319    265348       NA        \n10073  GUAM          GU           2007  330    275600       NA        \n10074  GUAM          GU           2008  313    286900       NA        \n10075  GUAM          GU           2009  390    355300       NA        \n10076  GUAM          GU           2010  510    413800       NA        \n10077  GUAM          GU           2011  559    475600       NA        \n10078  GUAM          GU           2012  616    564800       NA        \n10079  GUAM          GU           2013  728    623200       NA        \n10080  GUAM          GU           2014  712    558960       NA        \n17430  MONTGOMERY    AR           2006  469    175390       NA     \nInspect the missing data further on your own. It looks like its the unincorporated territories and a handful of Arkansas values missing countyfips (Federal Information Processing Standard). Specifically, Montgomery county AR is missing FIPs codes. Since we want to look US states in specific, excluding territories, we will just set the Montgomery county ones to the correct value 05097 and ignore the other missing values.\nsqlite> update annual set countyfips = 05097 where BUYER_STATE = \"AR\" and BUYER_COUNTY = \"MONTGOMERY\"\nsqlite> select * from annual where BUYER_STATE = \"AR\" and BUYER_COUNTY = \"MONTGOMERY\"\n\n       BUYER_COUNTY  BUYER_STATE  year  count  DOSAGE_UNIT  countyfips\n-----  ------------  -----------  ----  -----  -----------  ----------\n17430  MONTGOMERY    AR           2006  469    175390       5097      \n17431  MONTGOMERY    AR           2007  597    241270       5097      \n17432  MONTGOMERY    AR           2008  561    251760       5097      \n17433  MONTGOMERY    AR           2009  554    244160       5097      \nNow lets delete rows from the annual table that have missing county data. Check on these counties before and verify that the’ve been deleted afterwards. Also, we want to grab just three columns from the land table, so let’s create a new one called land_area. Also, the column there is called STCOU, which we want to rename to coutyfips. (I’m going to stop printing out the results of every step, so make sure you’re checking your work as you go.)\nsqlite> delete from annual where BUYER_COUNTY = \"NA\"\nsqlite> create table land_area as select Areaname, STCOU, LND110210D from land;\nsqlite> alter table land_area rename column STCOU to countyfips;\nNext we want to start joining the tables, so let’s left join our table and print out the counts to make sure we accounted correctly.\nsqlite> create table county_info as select * from population left join land_area using(countyfips);\nsqlite> select count(*) from land;\n3198\nsqlite> select count(*) from land_area;\n3198\nsqlite> select count(*) from county_info;\n28265\nsqlite> select count(*) from population;"
  },
  {
    "objectID": "data_sqlite.html#notes",
    "href": "data_sqlite.html#notes",
    "title": "11  SQL via sqlite",
    "section": "11.3 Notes",
    "text": "11.3 Notes\nAt this point, hopefully you have enough of a background to finish doing the example from Open Case Studies. I have to say, that working with SQL is pleasant, but I prefer python as a home base. In addition, after working with the data, I want to use plotting and analysis tools. In the next chapter, we’ll look at using python as a base language to interact with an sqlite database."
  },
  {
    "objectID": "data_sqlite.html#sqlite-in-python",
    "href": "data_sqlite.html#sqlite-in-python",
    "title": "11  SQL via sqlite",
    "section": "11.4 sqlite in python",
    "text": "11.4 sqlite in python\nAn sqlite3 library ships with python. In this tutorial, we’ll discuss how to utilize this library and read sqlite tables into pandas. With this, you can generalize to other python APIs to other databases.\nFirst, let’s continue on with our work from the previous notebook. A nice little tutorial can be found here.\n\nimport sqlite3 as sq3\nimport pandas as pd\n\ncon = sq3.connect(\"sql/opioid.db\")\n# cursor() creates an object that can execute functions in the sqlite cursor\n\nsql = con.cursor()\n\nfor row in sql.execute(\"select * from county_info limit 5;\"):\n    print(row)\n\n    \n# you have to close the connection\ncon.close\n\n('1', 'AUTAUGA', 'AL', '01001', '1', '1', 'Autauga', 'Autauga County, Alabama', 'B01003_001', '2006', '51328', 'Autauga, AL', '594.44')\n('2', 'BALDWIN', 'AL', '01003', '1', '3', 'Baldwin', 'Baldwin County, Alabama', 'B01003_001', '2006', '168121', 'Baldwin, AL', '1589.78')\n('3', 'BARBOUR', 'AL', '01005', '1', '5', 'Barbour', 'Barbour County, Alabama', 'B01003_001', '2006', '27861', 'Barbour, AL', '884.88')\n('4', 'BIBB', 'AL', '01007', '1', '7', 'Bibb', 'Bibb County, Alabama', 'B01003_001', '2006', '22099', 'Bibb, AL', '622.58')\n('5', 'BLOUNT', 'AL', '01009', '1', '9', 'Blount', 'Blount County, Alabama', 'B01003_001', '2006', '55485', 'Blount, AL', '644.78')\n\n\n<function Connection.close()>"
  },
  {
    "objectID": "data_sqlite.html#reading-into-pandas",
    "href": "data_sqlite.html#reading-into-pandas",
    "title": "11  SQL via sqlite",
    "section": "11.5 Reading into pandas",
    "text": "11.5 Reading into pandas\nLet’s read our sqlite database into pandas. At this point, we can then work on the dataset entirely in pandas. This is closest to how I work. I’m typically more comfortable working in R or python and so get my data out of database formats and into tidyverse or pandas formats as soon as I can.\n\ncon = sq3.connect(\"sql/opioid.db\")\n\ncounty_info = pd.read_sql_query(\"SELECT * from county_info\", con)\n\n# you have to close the connection\ncon.close\n\ncounty_info.head\n\n<bound method NDFrame.head of                BUYER_COUNTY BUYER_STATE countyfips STATE COUNTY  \\\n0          1        AUTAUGA          AL      01001     1      1   \n1          2        BALDWIN          AL      01003     1      3   \n2          3        BARBOUR          AL      01005     1      5   \n3          4           BIBB          AL      01007     1      7   \n4          5         BLOUNT          AL      01009     1      9   \n...      ...            ...         ...        ...   ...    ...   \n28260  28261       WASHAKIE          WY      56043    56     43   \n28261  28262         WESTON          WY      56045    56     45   \n28262  28263        SKAGWAY          AK      02230     2    230   \n28263  28264  HOONAH ANGOON          AK      02105     2    105   \n28264  28265     PETERSBURG          AK      02195     2    195   \n\n         county_name                               NAME    variable  year  \\\n0            Autauga            Autauga County, Alabama  B01003_001  2006   \n1            Baldwin            Baldwin County, Alabama  B01003_001  2006   \n2            Barbour            Barbour County, Alabama  B01003_001  2006   \n3               Bibb               Bibb County, Alabama  B01003_001  2006   \n4             Blount             Blount County, Alabama  B01003_001  2006   \n...              ...                                ...         ...   ...   \n28260       Washakie           Washakie County, Wyoming  B01003_001  2014   \n28261         Weston             Weston County, Wyoming  B01003_001  2014   \n28262        Skagway       Skagway Municipality, Alaska  B01003_001  2014   \n28263  Hoonah Angoon  Hoonah-Angoon Census Area, Alaska  B01003_001  2014   \n28264     Petersburg         Petersburg Borough, Alaska  B01003_001  2014   \n\n      population           Areaname LND110210D  \n0          51328        Autauga, AL     594.44  \n1         168121        Baldwin, AL    1589.78  \n2          27861        Barbour, AL     884.88  \n3          22099           Bibb, AL     622.58  \n4          55485         Blount, AL     644.78  \n...          ...                ...        ...  \n28260       8444       Washakie, WY    2238.55  \n28261       7135         Weston, WY    2398.09  \n28262        996        Skagway, AK     452.33  \n28263       2126  Hoonah-Angoon, AK    7524.92  \n28264       3212     Petersburg, AK    3281.98  \n\n[28265 rows x 13 columns]>"
  },
  {
    "objectID": "data_advanced_databases.html",
    "href": "data_advanced_databases.html",
    "title": "13  Big data storage",
    "section": "",
    "text": "You’ve probably already learned about some variation of databases either sql, nosql, spark, a cloud db, … We covered sqlite last chapter. Often, the backend of these databases can be quite complicated, while the front end requires SQL querries or something similar. We’ll look at a non-relational database format that is specifically useful for scientific computing called hdf5. HDF5 has implementations in many languages, but we’ll look at python. This is a hierarchical data format specifically useful for large array calculations.\nLet’s create a basic h5py file. First, let’s load our stuff.\nNow, let’s create an empty hdf5 file. Here’s the basic code; the option w is open for writing. There’s also w-, r, r+, a for write protected, read only, read/write, read/write and create. The first time I ran it I used:\nThen, subsequently\nNow let’s populate it with some data. The hdf5 file works almost like a directory where we can store hierarchical data. For example, suppose that we want sensors stored in a superstructure called sensors and want to fill in the data for sensor1 and sensor1.\nNow we can do normal np stuff on this sensor. However, hdf5 is only bringing in the part that we are using into memory. This allows us to work with very large files. Also, as we show here, you can name the data to a variable since that’s more convenient."
  },
  {
    "objectID": "data_advanced_databases.html#blockwise-basic-statistical-calculations",
    "href": "data_advanced_databases.html#blockwise-basic-statistical-calculations",
    "title": "13  Big data storage",
    "section": "13.1 Blockwise basic statistical calculations",
    "text": "13.1 Blockwise basic statistical calculations\nNow, consider taking the mean of both variables. Imagine that the time series is so long it’s not feasible to load into memory. So, we want to read it in blocks. You want your blocks to be as big as possible, since that’s fastest. In our case, of course, none of this is necessary.\nOur goal in this section is to do the following: calculate the empirical mean and variance for each sensor, center and scale each sensor, and write those changes to those variables, calculate the sample correlation then calculate the residual for sensor1 given sensor2. (I think typically you wouldn’t want to overwrite the original data; but, this is for pedagogical purposes.) We want our data organized so sensors are stored in a hierarchical “folder” called sensors and processed data is in a different folder.\nWe’re just simulating iid standard normals. So, we have a rough idea of the answers we should get, since the the data are theoretically mean 0, variance 1 and uncorrelated. After our calculations, they will have empirical mean 0 and variance 1 and the empirical correlation between the residual and sensor 2 will be 0.\nLet’s consider a block variation of the inner product. \\[\n<a, b> = \\sum_{i=0}^{n-1} a_i b_i = \\sum_{i=0}^{n/B} \\sum_{j=0}^{B-1} a_{j + i B} b_{j + i B}\n\\] (if \\(n\\) is divisible by \\(B\\). Otherwise you have to figure out what to do with the final block, which isn’t hard but makes the notation messier.) So, for example, the (sample) mean is then \\(<x, J>/n\\) where \\(J\\) is a vector of ones.\nLet’s calculate the mean using blockwise calculations.\n\nn = s1.shape[0]\nB = 32\n## mean center the blocks\nmean1 = 0\nmean2 = 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    mean1 += s1[block_indices].sum() / n \n    mean2 += s2[block_indices].sum() / n\n\n[mean1, mean2]\n\n[0.02317472126421261, 0.018259887652036223]\n\n\nLet’s now center our time series.\n\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    s1[block_indices] -= mean1  \n    s2[block_indices] -= mean2\n\nNow the (unbiased, sample) variance of centered vector \\(a\\) is simply \\(<a, a>/(n-1)\\).\n\nv1, v2 = 0, 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    v1 += np.sum(s1[block_indices] ** 2) / (n - 1)\n    v2 += np.sum(s2[block_indices] ** 2) / (n - 1)\n[v1, v2]\n\n[0.9303145564806921, 0.9518095709661069]\n\n\nNow let’s scale our vectors as\n\nsd1 = np.sqrt(v1)\nsd2 = np.sqrt(v2)\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    s1[block_indices] /= v1  \n    s2[block_indices] /= v2\n\nNow that our vectors are centered and scaled, the empirical correlation is simply \\(<a, b>/(n-1)\\). Let’s do that\n\ncor = 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    cor += np.sum(s1[block_indices] * s2[block_indices]) / (n-1) \ncor\n\n-0.009267019800591662\n\n\nFinally, we want to “regress out” s2 from s1. Since we normalized our series, the correlation is slope coefficient from linear regression (regardless of the outcome and dependent variable) and the intercept is zero (since we centered). Thus, the residual we want is \\(e_{12} = s_1 - \\rho s_2\\) where \\(\\rho\\) is the correlation.\n\nf['processed/resid_s1_s2'] = np.empty(n)\ne12 = f['processed/resid_s1_s2']\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    e12[block_indices] += s1[block_indices] - cor * s2[block_indices] \n\nNow we have our new processed data stored in a vector. To close our database simply do:\n\nf.close()\n\nNow our processed data is stored on disk.\n\nf = h5py.File('sensor.hdf5', 'r')\nf['processed/resid_s1_s2']\n\n<HDF5 dataset \"resid_s1_s2\": shape (1024,), type \"<f8\">\n\n\n\nf.close()"
  },
  {
    "objectID": "data_advanced_databases.html#homework",
    "href": "data_advanced_databases.html#homework",
    "title": "13  Big data storage",
    "section": "13.2 Homework",
    "text": "13.2 Homework\n\nPerform lots of regressions. Suppose that you have a setting where you would like to perform the operation \\[\n(X'X)^{-1} X' Y\n\\] where \\(X\\) is \\(n\\times p\\) and \\(Y\\) is \\(n\\times v\\). Consider the case where \\(Y\\) is very large (so \\(V\\) is large). Simulate some data where you perform this linear model in block calculations.\nWrite a block matrix multiplication program that takes in two matrices with agreeable dimensions stored as HDF5 and multiplies them in block sizes specified by the user."
  },
  {
    "objectID": "data_webscraping.html",
    "href": "data_webscraping.html",
    "title": "14  Webscraping",
    "section": "",
    "text": "We’ll need some packages to start, requests, beautifulsoup4 and selenium. Requesting elements from a static web page is very straightforward. Let’s take an example by trying to grab and plot the table of multiple Olympic medalists from Wikipedia then create a barplot of which sports have the most multiple medal winners.\nFirst we have to grab the data from the url, then pass it to beautifulsoup4, which parses the html, then pass it to pandas. First let’s import the packages we need.\nWe then need to read the web page into data.\nNow let’s read the page into bs4. Then we want to find the tables in the page. We add the class and wikitable information to specify which tables that we want. If you want to find classes, you can use a web tool, like selectorgadget or viewing the page source.\nNow we should take the html that we’ve saved, then read it into pandas. Fortunately, pandas has a read_html method. So, we convert our tables to strings then read it in. Since there’s multiple tables, we grab the first one.\nNow we’re in a position to build our plot. Let’s look at the count of 4 or more medal winers by sport and games."
  },
  {
    "objectID": "data_webscraping.html#selenium",
    "href": "data_webscraping.html#selenium",
    "title": "14  Webscraping",
    "section": "14.1 Selenium",
    "text": "14.1 Selenium\nIf the page has javacript, your basic web scraping may not work. In this case, you not only need to get and parse the page, but also to interact with the javascript. For this, enter Selenium. This is a python browser that allows you to automate web navigation. We’ll cover that in the next chapter."
  },
  {
    "objectID": "data_advanced_webscraping.html",
    "href": "data_advanced_webscraping.html",
    "title": "15  Advanced web scraping",
    "section": "",
    "text": "Before you start webscraping make sure to consider what you’re doing. Does your scraping violate TOS? Will it inconvenience the site, other users? Per Uncle Ben: WGPCGR.\nAlso, before you begin web scraping, look for a download data option or existing solution. Probably someone has run up against the same problem and worked it out. For example, we’re going to scrape some wikipedia tables, which there’s a million other solutions for, including a wikipedia api."
  },
  {
    "objectID": "data_advanced_webscraping.html#basic-web-scraping",
    "href": "data_advanced_webscraping.html#basic-web-scraping",
    "title": "15  Advanced web scraping",
    "section": "15.2 Basic web scraping",
    "text": "15.2 Basic web scraping\nWe covered this last chapter. However, let’s do an example of static page parsing just to get started. Consider scraping the table of top 10 heat waves from wikipedia. First, we open the url, then parse it using BeautifulSoup, then load it into a pandas dataframe.\n\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup as bs\nimport pandas as pd\nurl = \"https://en.wikipedia.org/wiki/List_of_natural_disasters_by_death_toll\"\nhtml = urlopen(url)\nparsed = bs(html, 'html.parser').findAll(\"table\")\npd.read_html(str(parsed))[11]\n\n\n\n\n\n  \n    \n      \n      Rank\n      Death toll\n      Event\n      Location\n      Date\n    \n  \n  \n    \n      0\n      1.0\n      1300\n      The Daulatpur–Saturia tornado\n      Manikganj, Bangladesh\n      1989\n    \n    \n      1\n      2.0\n      695\n      The Tri-State tornado outbreak\n      United States (Missouri–Illinois–Indiana)\n      1925\n    \n    \n      2\n      3.0\n      681\n      1973 Dhaka tornado\n      Bangladesh\n      1973\n    \n    \n      3\n      4.0\n      660\n      1969 East Pakistan tornado\n      East Pakistan (now Bangladesh)\n      1969\n    \n    \n      4\n      5.0\n      600\n      The Valletta, Malta tornado\n      Malta\n      1551 or 1556\n    \n    \n      5\n      6.0\n      500\n      The 1851 Sicily tornadoes\n      Sicily, Two Sicilies (now Italy)\n      1851\n    \n    \n      6\n      6.0\n      500\n      Narail-Magura tornado\n      Jessore, East Pakistan, Pakistan (now Bangladesh)\n      1964\n    \n    \n      7\n      6.0\n      500\n      Madaripur-Shibchar tornado\n      Bangladesh\n      1977\n    \n    \n      8\n      9.0\n      400\n      The 1984 Soviet Union tornado outbreak\n      Soviet Union (now Russia)\n      1984\n    \n    \n      9\n      10.0\n      317\n      The Great Natchez Tornado\n      United States (Mississippi–Louisiana)\n      1840\n    \n  \n\n\n\n\nThe workflow as as follows:\n\nWe used the developer console on the webpage to inspect the page and its properties.\nWe opened the url with urlopen\nWe parsed the webpage with BeautifulSoup then used the method findAll on that to search for every table\nPandas has a utility that converts a html tables into a dataframe. In this case it creates a list of tables, where the 12th one is the heatwaves. Note it needs the data to be converted to a string before proceeding.\n\nThis variation of web scraping couldn’t be easier. However, what if the content we’re interested in only exists after interacting with the page? Then we need a more sophisticated solution."
  },
  {
    "objectID": "data_advanced_webscraping.html#form-filling",
    "href": "data_advanced_webscraping.html#form-filling",
    "title": "15  Advanced web scraping",
    "section": "15.3 Form filling",
    "text": "15.3 Form filling\nWeb scraping can require posting to forms, such as logins. This can be done directly with python / R without elaborate programming, for example using the requests library. However, make sure you aren’t violating a web site’s TOS and also make sure you’re not posting your password to github as you commit scraping code. In general, don’t create a security hole for your account by web scraping it. Again, also check to make sure that the site doesn’t have an API with an authentication solution already before writing the code to post authentication. Many websites that want you to programmatically grab the data build an API."
  },
  {
    "objectID": "data_advanced_webscraping.html#programmatically-web-browsing",
    "href": "data_advanced_webscraping.html#programmatically-web-browsing",
    "title": "15  Advanced web scraping",
    "section": "15.4 Programmatically web browsing",
    "text": "15.4 Programmatically web browsing\nSome web scraping requires us to interact with the webpage. This requires a much more advanced solution where we programmatically use a web browser to interact with the page. I’m using selenium and chromedriver. To do this, I had to download chromedriver and set it so that it was in my unix PATH.\n\nfrom selenium import webdriver\ndriver = webdriver.Chrome()\ndriver.quit()\n\nIf all went well, a chrome window appeared then closed. That’s the browser we’re going to program. If you look closely at the browser before you close it, there’s a banner up to that says “Chrome is being controlled by automated test software.” Let’s go through the example on the selenium docs here. First let’s vist a few pages. We’ll go to my totally awesome web page that I meticulously maintain every day then duckduckgo. We’ll wait a few seconds in between. My site is created and hosted by google sites, which seems reasonable that they would store a cookie so that I can log in and edit my site (which I almost never do). Duckduckgo is a privacy browser, so let’s check to see if they create a cookie. (Hint, I noticed that selenium doesn’t like redirects, so use the actual page url.)\n\ndriver = webdriver.Chrome()\ndriver.get(\"https://sites.google.com/view/bcaffo/home\")\nprint(driver.get_cookies())\ndriver.implicitly_wait(5)\n## Let's get rid of all cookies before we visit duckduckgo\ndriver.delete_all_cookies()\ndriver.get(\"https://duckduckgo.com/\")\nprint(driver.get_cookies())\n\nFor me, at least, this prints out the cookie info for my google site then nothing for ddg. (I’m not evaluating the code in quarto since I don’t want to bring up the browser when I compile the document.)\nNow let’s find the page elements that we’d like to interact with. There’s a text box that we want to submit a search command into and a button that we’ll need to press. When I go to ddg and press CTRL-I I find that the search box is:\n<input id=\"search_form_input_homepage\" class=\"js-search-input search__input--adv\" type=\"text\" autocomplete=\"off\" name=\"q\" tabindex=\"1\" value=\"\" autocapitalize=\"off\" autocorrect=\"off\" placeholder=\"Search the web without being tracked\">\nNotice, the name=\"q\" html name for the search form. When I dig around and find the submit button, it’s code is:\n<input id=\"search_button_homepage\" class=\"search__button  js-search-button\" type=\"submit\" tabindex=\"2\" value=\"S\">\nNotice its id is search_button_homepage. Let’s find these elements.\n\nsearch_box = driver.find_element(by=By.NAME, value=\"q\")\nsearch_button = driver.find_element(by=By.ID, value=\"search_button_homepage\")\n\nNow let’s send the info and press submit\n\nsearch_box.send_keys(\"Selenium\")\nsearch_button.click()\ndriver.implicitly_wait(10)\ndriver.save_screenshot(\"assets/images/webscraping.png\")\npage_source = driver.page_source\ndriver.close()\n\nHere, we saved the page_source as a variable that then can be parsed with other html parses (like bs4). Play around with the methods associated with driver and navigate the web. You’ll see that selenium is pretty incredible. Here’s the screenshot that we took:\n\n\n\nScreenshot of webscraping"
  },
  {
    "objectID": "data_advanced_webscraping.html#homework",
    "href": "data_advanced_webscraping.html#homework",
    "title": "15  Advanced web scraping",
    "section": "15.5 Homework",
    "text": "15.5 Homework\n\nWrite a function that takes a search term, enters it into this link and returns the number of characters from the output.\nWrite a function that solves THE MAZE and returns your current location at its solution"
  },
  {
    "objectID": "intro_html.html",
    "href": "intro_html.html",
    "title": "4  HTML, CSS and javascript",
    "section": "",
    "text": "HTML is a markup language used by web browsers. HTML stands for hypetext markup language. Like all markup languages, it gives a text set of instructions that get interpreted into a nicer looking document. Other markup languages include XML, LaTeX, Org and markdown. (Yes, mark”down” is named as such since it’s a ultra-simple mark”up” language.)\nWe’ll need a little html knowledge since so much data science output is web-page oriented. Also, we’ll need to know a little about html to scrape web content. A web page typically has three elements: the html which gives the page structure and markup, css (cascading style sheets) for style and javascript for interactivity. We’ll cover a little html and javascript so that we can better understand certain data science products. However, you should take a web development course if you want in depth treatments.\nWe won’t spend much time talking about CSS. CSS gives a set of standards for the style of a web page. With CSS one can take the skeleton (HTML/JS) and dramatically change the style in the same way you could choose to play some sheet music in different ways. A quick tutorial on CSS can be found here.\nBack to HTML. An HTML document looks something like this. Take a file, insert the following code and give it the extension .html. Then, open it up in a browser.\n<!DOCTYPE html>\n<HTML>\n    <HEAD>\n        <TITLE> This is the web page title</TITLE>\n    </HEAD>\n    <BODY>\n        <H1>Heading 1</H1>\n        <H2>Heading 2</H2>\n        <P> Paragraph </P>\n        <CODE> CODE </CODE>\n    </BODY>\n</HTML>\nThe resulting document will look like the following\n\n\n   \n      \n      \n       Paragraph \n      CODE \n   \n\nAs you probably noticed, a bit of markup is something like <COMMAND>CONTENT</COMMAND>. The latter command has a forward slash. You should close your commands, even if your browser still renders the page like you like just because it makes for bad code not to. Also, someone else’s browser may not be as forgiving. Good code editors will help remind you to close your commands."
  },
  {
    "objectID": "intro_html.html#browser-stuff",
    "href": "intro_html.html#browser-stuff",
    "title": "4  HTML, CSS and javascript",
    "section": "4.2 Browser stuff",
    "text": "4.2 Browser stuff\nNote, since we’ll be working a lot with files, probably in one directory, you can use file:///PATH TO YOUR DIRECTORY to open up files (maybe even bookmark that directory). Also, CTRL-R is probably faster than clicking refresh and (in chrome at least) CTRL-I brings up developer tools (javascript console). When we have a web server running locallly, you usually go to localhost. For example, my jupyter lab server sends me to http://localhost:8888/lab/tree/. Here 8888 is a port, localhost refers to the server running on the lcoal computer and lab/tree is the relative path to the root of my jupyter lab server.\nBrowsers make choices in how they render HTML and CSS and implement javascript. So, unless you’re a web developer by trade, don’t get too exotic in your design choices. Also, a lot of HTML is auto generated. So, your mileage may vary by looking at page sources."
  },
  {
    "objectID": "intro_html.html#hosting",
    "href": "intro_html.html#hosting",
    "title": "4  HTML, CSS and javascript",
    "section": "4.3 Hosting",
    "text": "4.3 Hosting\nWhen you double click on your html file, it’s being hosted locally. So, no one else can see it. To have a web page on the internet it has to be hsoted on a server running web hosting software. Fortunately, github will actually allow us to host web pages. Basically, put an empty .nojekyll file in your repository (this tells it that it’s not a jekyll based web site and follow the instructions here. This will be really useful for us, since many of our datascience programs output web pages. For example, RMarkdown documents get translated into web documents. Similarly, jupyter-lab will output reveal.js (javascript/html) slide decks from our jupyter lab notebooks. Note that some of our programs will require servers that also run python or R in the back end, so github pages won’t suffice for that. There we need servers specifically set up to run those kinds of scripts."
  },
  {
    "objectID": "intro_html.html#javascript",
    "href": "intro_html.html#javascript",
    "title": "4  HTML, CSS and javascript",
    "section": "4.4 Javascript",
    "text": "4.4 Javascript\nJavascript is what makes webpages interactive. We’ll need a little javascript to understand how interactive web graphics work. Consider the following where we use javscript to change an HTML element in a web page\n<H2 id=\"textToChange\">Preference ?</H2>\n\n<button type=\"button\" onclick='document.getElementById(\"textToChange\").innerHTML = \"You prefer 1\"'>1</button>\n<button type=\"button\" onclick='document.getElementById(\"textToChange\").innerHTML = \"You prefer 2\"'>2</button>\nHere’s the result:\nPreference ?\nClick 1\nClick 2\n\n4.4.1 JSON\nJSON is a data format used in javascript, and adopted elsewhere. It’s a fairly straightforward data structure. In fact, you might have edited some JSON data by editing your Jupyter Lab ipython notebook properties. It goes \"name\":value where objects are enclosed in curly braces and arrays in brackets. You have to separate distinct object or values with quotes.\n{\n    \"name\" : \"Bowie\",\n    \"species\" : \"dog\",\n    \"age\" : 6,\n    \"likes\" : [\"biscuits\", \"naps\", \"walks\"],\n    \"dislikes\" : [\"the cat\", \"getting groomed\"]    \n}\nLet’s define a variable in our JS console. Open up your JS console and type:\npet = {\n    \"name\" : \"Bowie\",\n    \"species\" : \"dog\",\n    \"age\" : 6,\n    \"likes\" : [\"biscuits\", \"naps\", \"walks\"],\n    \"dislikes\" : [\"the cat\", \"getting groomed\"]\n};\nNow try doing things like pet.likes and hit enter."
  },
  {
    "objectID": "intro_html.html#example-of-using-a-javascript-library",
    "href": "intro_html.html#example-of-using-a-javascript-library",
    "title": "4  HTML, CSS and javascript",
    "section": "4.5 Example of using a javascript library",
    "text": "4.5 Example of using a javascript library\nLet’s use a javscript library to plot some data. Unless you need really fine control over the javascript elements, we usually do this by calling a python or R api to the JS library. But, it’s useful to do once to see how the library does things.\nHere is some data that we’ll use a lot. It is brain regions of interest (ROIs) by the percentage of the brain that they make up.\n               roi       comp\n0              CSF   7.370845\n1   Diencephalon_L   0.756288\n2   Diencephalon_R   0.763409\n3    Mesencephalon   0.864718\n4    Metencephalon  12.488275\n5   Myelencephalon   0.378464\n6  Telencephalon_L  42.030477\n7  Telencephalon_R  42.718368\nLet’s use Vegalite. This package creates the plot via a JSON object that contains all of the data and instructions.\n{\n    \"data\" : {\n        \"values\" : [\n{\"roi\" : \"CSF\"            , \"comp\" :  7.370845},\n{\"roi\" : \"Diencephalon_L\" , \"comp\" :  0.756288},\n{\"roi\" : \"Diencephalon_R\" , \"comp\" :  0.763409},\n{\"roi\" : \"Mesencephalon\"  , \"comp\" :  0.864718},\n{\"roi\" : \"Metencephalon\"  , \"comp\" : 12.488275},\n{\"roi\" : \"Myelencephalon\" , \"comp\" :  0.378464},\n{\"roi\" : \"Telencephalon_L\", \"comp\" : 42.030477},\n{\"roi\" : \"Telencephalon_R\", \"comp\" : 42.718368}\n        ]\n    },\n    },\n   \"mark\": \"bar\",\n   \"encoding\": {\n    \"y\": {\"field\": \"roi\" , \"type\": \"nominal\"},\n    \"x\": {\"field\": \"comp\", \"type\": \"quantitative\"}\n}\nThis needs to be embedded into html, plus the vega JS libraries loaded to execute. I have an example here. The output looks like this\n\n\n\nGraphic\n\n\nTypically, one creates these graphics in one’s home analysis language (like python or R). There are several libraries for doing as such. Some of the popular ones include: bookeh, vega, D3js, leaflet, but there are many more. There’s also connections to large private efforts including tableau, power bi, google charts and plotly."
  },
  {
    "objectID": "graphics_advanced_interactive.html",
    "href": "graphics_advanced_interactive.html",
    "title": "18  Advanced interactive graphics: D3",
    "section": "",
    "text": "In your other DS courses, you’ve learned how to create static graphics uses R, ggplot, matplotlib, seaborn … You’ve probably also learned how to create client side interactive graphics using libraries like plotly and maybe also learned client-server interactivity with shiny, dash …\nIn this section we’re going to dig deeper into client side graphics, which are almost always done via html, css, javascript and a javascript plotting library. We’re going to focus on d3.js, a well known javascript library for creating interactive data visulalizations.\nTools like d3 are mostly for creating professional data web graphics. So, most of our daily graphics use will just use python/R/julia/matlab … or plotting libraries like plotly. Usually, you want to prototype graphics outside of d3. Here, we’ll give you a smidge of using d3 to get you started if your goal is to become a graphics expert."
  },
  {
    "objectID": "graphics_advanced_interactive.html#introduction-to-d3",
    "href": "graphics_advanced_interactive.html#introduction-to-d3",
    "title": "18  Advanced interactive graphics: D3",
    "section": "18.1 Introduction to D3",
    "text": "18.1 Introduction to D3\nLet’s get started. I’m going to assume that you have a basic knowledge of html, css and a little bit of javascript. D3 works by manipulating html elements. Let’s select every paragraph element in a document.\n<!DOCTYPE html>\n<html lang=\"en\">\n\n<head>\n    <script src=\"https://d3js.org/d3.v5.min.js\"></script>\n</head>\n\n<body>\n    <p> Advanced </p>\n    <p> Data science </p> \n        <script>\n            let pselect = d3.selectAll(\"p\")\n            //let pselect = d3.select(\"p\").style(\"color\", \"green\");\n            //let pselect = d3.selectAll(\"p\").style(\"color\", \"green\");\n        </script>\n    </body>\n</html>\nGoing forward, we’ll omit most of the html commands.\n\nThe command <script src=\"https://d3js.org/d3.v5.min.js\"></script> loads d3 from a CDN. You could also download it locally if you’d like.\nThe script let pselect = d3.selectAll(\"p\").style(\"color\", \"green\"); creates a variable pselect that is all of the html paragraph elements\nTry doing this, loading the web page, then try uncommenting each other script line in turn and refreshing\nIn chrome do Ctrl-shift-i to get the developer console and inspect the variable pselect.\nNesting select or selectAll will select elements within the selected elements.\nYou can also select by id or class."
  },
  {
    "objectID": "graphics_advanced_interactive.html#a-simple-example",
    "href": "graphics_advanced_interactive.html#a-simple-example",
    "title": "18  Advanced interactive graphics: D3",
    "section": "18.2 A simple example",
    "text": "18.2 A simple example\nLet’s go through an example where we plot brain volumetric ROI data on the log scale using D3.\n<style>\n    .bar {\n        background: #f5b634;\n        border: 4px solid #0769ad;\n        height: 20px;\n    }\n</style>\n<body>\n        <script>\n            let roiData = [\n                {\"roi\": \"Telencephalon_L\", \"volume\" : 531111},\n                {\"roi\": \"Telencephalon_R\", \"volume\" : 543404},\n                {\"roi\": \"Diencephalon_L\",  \"volume\" : 9683  },\n                {\"roi\": \"Diencephalon_R\",  \"volume\" : 9678  },\n                {\"roi\": \"Mesencephalon\",   \"volume\" : 10268 },\n                {\"roi\": \"Metencephalon\",   \"volume\" : 159402},\n                {\"roi\": \"Myelencephalon\",  \"volume\" : 4973  },\n                {\"roi\": \"CSF\",             \"volume\" : 109776}\n            ];\n    \n            let divSelection = d3.select(\"body\") \n                    .selectAll(\"div\")\n                    .data(roiData)\n                    .enter()\n                    .append('div')\n                    .attr(\"class\", \"bar\")\n                    .style(\"width\", (d) => {return Math.log(d.volume) * 20 + \"px\"; })\n                    .text(d => d.roi)\n                    .on(\"mouseover\", function(){\n                        d3.select(this)\n                        .style(\"background-color\", \"orange\");\n                    })\n                    .on(\"mouseout\", function(){\n                        d3.select(this)\n                        .style(\"background-color\",\"#33A2FF\" )\n                    })        </script>\n    </body>\n\nThe data(roiDat) selects our dataset\nThe enter() and append('div') commands add div elements to the html document, one per data element.\nThe attr method considers our bar stylesheet style\nThe style method changes the style so that the bars have the width of our data. The notation (d) => {return d.volume * .001 + \"px\"} is a function that selects the ROI element of the data, multiplies it by .001 then converts it to text with px at the end.\nThe text method at the end appends the text to our plot\nThe on methods say what to do when one mouses over and off the bars. You can see now that they turn orange then back. Remove the mouseout .on call and see what happens.\n\nThe output looks like this. Hover over a bar to test. (Look at the file in d3/roi1.html)"
  },
  {
    "objectID": "graphics_advanced_interactive.html#working-through-a-realistic-example",
    "href": "graphics_advanced_interactive.html#working-through-a-realistic-example",
    "title": "18  Advanced interactive graphics: D3",
    "section": "18.3 Working through a realistic example",
    "text": "18.3 Working through a realistic example\nUnder assets/kirby_pivot.csv is a dataset with the kirby 21 data pivoted to have regions as columns. Let’s work through a d3 example of ploting right versus left asymmetry in the telencephalon (the largest area of the brain including the cortex and central white matter).\nHere’s the scatterplot that I’ve got so far. For HW, add text labels to the point, or a tooltip that gives point information when you hover over it.\n\nThe code for the plot is in d3/roi2.html. Let’s go over some of the main parts of the d3 code here. First, we set up the graphic\nconst h = 500\nconst w = 500\n\n// create the background\nlet svg = d3.select(\"body\")\n    .append(\"svg\")\n    .attr(\"width\" , h)\n    .attr(\"height\", w);\nNext we load in the data. First, we create a function that does a little row processing for us. Honestly, it’s probably better to just do this in python/R/julia … beforehand, but it’s worth showing here. We create variables for the log ratio between the right and left hemispheres and the log of the geometric mean. We’ll use this to create a Tukey mean/difference plot of the log of the volumes.\n//create the variables we're interested in\nlet rowConverter = function(d) {\n    return {\n        id : d.id,\n        //y is going to be the log difference R-L\n        logratio : Math.log(parseFloat(d.Telencephalon_R)) - Math.log(parseFloat(d.Telencephalon_L)),\n        //x is going to be the average log \n        loggm : (Math.log(parseFloat(d.Telencephalon_L)) + Math.log(parseFloat(d.Telencephalon_R))) * .5\n    };\n    }\n\n//the location where I'm pulling the csv from\nlet dataloc = \"https://raw.githubusercontent.com/smart-stats/advanced_ds4bio_book/main/qbook/assets/kirby_pivot.csv\"\n\n//read in the data and parse the rows \nkirby_pivot = d3.csv(dataloc, rowConverter)\nModern js uses something called ‘promises’, which alllows for asynchronous evaluation. When we read in our csv file, it gets created as a promise and not an array like we need. The result is that our plotting commands need to then be called as a method from the promise object. The reason for this is so that it only uses the data when the data is actually loaded (i.e. promise fulfilled.) So, the plotting commmands for us look like this.\nkirby_pivot.then(dat => {\n    PLOTTING COMMANDS\n})\nJust a reminder that the notation d => g(d) is JS shorthand for function(d) {return g(d);} and is used heavily in d3 coding. Now let’s fill in PLOTTING COMMANDS. First, let’s fill in some utility functions. We get the range of our x and y values to help set up our axes. d3 scales map our function values to a range we want. So let’s create scale maps for x, y and color and then also set up axes using those scales. We’ll also go ahead on plot our axes so they’re on the bottom.\nmaxx = d3.max(dat, d => d.loggm)\nminx = d3.min(dat, d => d.loggm)\nmaxy = d3.max(dat, d => d.logratio)\nminy = d3.min(dat, d => d.logratio)\n\n//fudge is the boundary otherwise points get chopped off\nlet fudge = 50\n\nlet yScale = d3.scaleLinear()\n    .domain([miny, maxy])\n    .range([h-fudge, fudge])\n\nlet pointScale = d3.scaleLinear()\n    .domain([miny, maxy])\n    .range([5, 10])\n\nlet colorScale = d3.scaleLinear()\n    .domain([miny, maxy])\n    .range([0, 1])\n\n\nlet xScale = d3.scaleLinear()\n    .domain([minx, maxx])\n    .range([w-fudge, fudge]);\n\n// define the axes\nlet xaxis = d3.axisBottom().scale(xScale)\nlet yaxis = d3.axisLeft().scale(yScale)\nsvg.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", \"translate(0,\" + (h - fudge) + \")\")\n    .call(xaxis)\n\nsvg.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", \"translate(\" + fudge + \",0)\")\n    .call(yaxis)\nNow let’s create the plot. We’re going to add circles at each location, which is attributes cx and cy. Notice we use our previous defined scales to give their locations. Also, we’ll set the color and size relative to the logratio. Finally, when we mouseover a point, let’s change the radius then change it back when we mouseoff.\nsvg.selectAll(\"circle\")\n    .data(dat)\n    .enter()\n    .append(\"circle\")\n    .attr(\"cy\", d => yScale(d.logratio))\n    .attr(\"cx\", d => xScale(d.loggm))\n    .attr(\"r\",  d => pointScale(d.logratio))\n    .attr(\"fill\", d => d3.interpolateWarm(colorScale(d.logratio)))\n    .attr(\"stroke\", \"black\")\n    .on(\"mouseover\", function() {\n        d3.select(this)\n            .attr(\"r\", 30)\n        })\n    .on(\"mouseout\", function() {\n        d3.select(this)\n        .attr(\"r\",  d => pointScale(d.logratio))\n    })\nObviously, this is a lot of work for a simple scatterplot. The difference is that here you have total control over plotting and interactivity elements."
  },
  {
    "objectID": "graphics_advanced_interactive.html#observable-and-observable-plot",
    "href": "graphics_advanced_interactive.html#observable-and-observable-plot",
    "title": "18  Advanced interactive graphics: D3",
    "section": "18.4 Observable and Observable Plot",
    "text": "18.4 Observable and Observable Plot\nObserverable is a notebook for working with d3. It’s quite neat since mixing javascript coding in a web notebook, which itself is written in javascript, makes for an interesting setup. Typically, one would do the data preprocessing in R, python, julia … then do the advanced graphing in d3. In addition to accepting d3 as inputs, observable has a slightly higher set of utility functions called observable plot. (Quarto, which this document is in, allows for observable cells.) So, let’s read in some ROI data and plot it in observable plot. Note this is the average of the Type I Level I ROIs. Notice this is much easier than using d3 directly.\n\ndata = FileAttachment(\"assets/kirby_avg.csv\").csv();\nPlot.plot({\nmarks: [Plot.barY(data, {x: \"roi\", y: \"volume\", fill : 'roi'})],\n    x: {tickRotate: 45},\n    color: {scheme: \"spectral\"},    \n    height: 400,\n    width: 400,\n    marginBottom: 100\n\n})"
  },
  {
    "objectID": "graphics_advanced_interactive.html#links",
    "href": "graphics_advanced_interactive.html#links",
    "title": "18  Advanced interactive graphics: D3",
    "section": "18.5 Links",
    "text": "18.5 Links\n\nObservable is not javascript\nd3 tutorial.\nd3 gallery"
  },
  {
    "objectID": "graphics_advanced_interactive.html#homework",
    "href": "graphics_advanced_interactive.html#homework",
    "title": "18  Advanced interactive graphics: D3",
    "section": "18.6 Homework",
    "text": "18.6 Homework\n\nCreate a D3 graphic web page that displays a scatterplot of your chosing. Show point information on hover.\nOn the same web page, create a D3 graphic web page that displays a stacked bar chart for the Kirby 21 data. Hover data should show subject information and increase the size of the bar. Here’s a plotly version to get a sense.\n\n\nimport pandas as pd\nimport plotly.express as px\nimport numpy as np\ndat = pd.read_csv(\"https://raw.githubusercontent.com/smart-stats/ds4bio_book/main/book/assetts/kirby21.csv\").drop(['Unnamed: 0'], axis = 1)\ndat = dat.assign(id_char = dat.id.astype(str))\nfig = px.bar(dat, x = \"id_char\", y = \"volume\", color = \"roi\")\nfig.show()\n\n\n                                                \n\n\n\nSubmit your webpages and all supporting code to your assignment repo\nHere’s a hint to the HW in d3/hwHint.html"
  },
  {
    "objectID": "graphics_interactive.html",
    "href": "graphics_interactive.html",
    "title": "17  Interactive graphics",
    "section": "",
    "text": "Javascript graphics allows one to put data oriented graphics into web documents (like this book), apps and other reproducible research documents. As mentioned, several well developed APIs have been developed to use Python, R … as the base language where graphics are output as javascript. Here, we’ll go through some examples using plotly, both because it’s a nice library of graphics functions, but also it’s what I know sort of well. However, if there’s another graphics platform you like, likely there’s a python and/or R API written for it."
  },
  {
    "objectID": "graphics_interactive.html#using-plotly",
    "href": "graphics_interactive.html#using-plotly",
    "title": "17  Interactive graphics",
    "section": "17.1 Using plotly",
    "text": "17.1 Using plotly\nConsider a dataset that has regional volumes for 20 subjects in a long dataset. I wrote some R code for reading in this dataset which you can follow along here.\n\nimport pandas as pd\nimport plotly.express as px\nimport numpy as np\n\n\ndat = pd.read_csv(\"assets/kirby21.csv\").drop(['Unnamed: 0'], axis = 1)\ndat.head()\n\n\n\n\n\n  \n    \n      \n      id\n      roi\n      volume\n    \n  \n  \n    \n      0\n      127\n      Telencephalon_L\n      531111\n    \n    \n      1\n      127\n      Telencephalon_R\n      543404\n    \n    \n      2\n      127\n      Diencephalon_L\n      9683\n    \n    \n      3\n      127\n      Diencephalon_R\n      9678\n    \n    \n      4\n      127\n      Mesencephalon\n      10268\n    \n  \n\n\n\n\nLet’s vew individual subjects. The id variable is a numeric variable, so let’s create a string version.\n\ndat = dat.assign(id_char = dat.id.astype(str))\nfig = px.bar(dat, x = \"id_char\", y = \"volume\", color = \"roi\")\nfig.show()\n\n\n                                                \n\n\nLet’s add an intra-cranial volume column by grouping by id, summing all volumes, then merging that data back into the main data. We’ll then add a composition variable, which is the regional volumes divided by the intra-cranial volume.\n\nicv = dat.groupby(['id']).volume.sum().reset_index().rename(columns = {'volume' : 'icv'})\ndat = pd.merge(dat, icv, on = 'id')\ndat = dat.assign(comp = dat.volume / dat.icv)\ndat.head()\n\n\n\n\n\n  \n    \n      \n      id\n      roi\n      volume\n      id_char\n      icv\n      comp\n    \n  \n  \n    \n      0\n      127\n      Telencephalon_L\n      531111\n      127\n      1378295\n      0.385339\n    \n    \n      1\n      127\n      Telencephalon_R\n      543404\n      127\n      1378295\n      0.394258\n    \n    \n      2\n      127\n      Diencephalon_L\n      9683\n      127\n      1378295\n      0.007025\n    \n    \n      3\n      127\n      Diencephalon_R\n      9678\n      127\n      1378295\n      0.007022\n    \n    \n      4\n      127\n      Mesencephalon\n      10268\n      127\n      1378295\n      0.007450\n    \n  \n\n\n\n\nLet’s now replot our compositional data (but now normalized to have height 1).\n\nfig = px.bar(dat, x = \"id_char\", y = \"comp\", color = \"roi\")\nfig.show()\n\n\n                                                \n\n\nLet’s look at the subject level means. Therefore, we have to average across id.\n\nroi_mean = dat.drop([\"id\", \"id_char\", \"icv\"], axis = 1).groupby([\"roi\"]).mean().reset_index()\nfig = px.bar(roi_mean, x = \"roi\", y = \"comp\")\nfig.show()\n\n\n                                                \n\n\nThere’s a hierarchy of regions in this dataset. Let’s visualize a subject’s type 1 level 5 data as it exists in the hierarchy. First, let’s load in the hierarchy information:\n\nurl = \"https://raw.githubusercontent.com/bcaffo/MRIcloudT1volumetrics/master/inst/extdata/multilevel_lookup_table.txt\"\nmultilevel_lookup = pd.read_csv(url, sep = \"\\t\").drop(['Level5'], axis = 1)\nmultilevel_lookup = multilevel_lookup.rename(columns = {\n    \"modify\"   : \"roi\", \n    \"modify.1\" : \"level4\",\n    \"modify.2\" : \"level3\", \n    \"modify.3\" : \"level2\",\n    \"modify.4\" : \"level1\"})\nmultilevel_lookup = multilevel_lookup[['roi', 'level4', 'level3', 'level2', 'level1']]\nmultilevel_lookup.head()\n\n\n\n\n\n  \n    \n      \n      roi\n      level4\n      level3\n      level2\n      level1\n    \n  \n  \n    \n      0\n      SFG_L\n      SFG_L\n      Frontal_L\n      CerebralCortex_L\n      Telencephalon_L\n    \n    \n      1\n      SFG_R\n      SFG_R\n      Frontal_R\n      CerebralCortex_R\n      Telencephalon_R\n    \n    \n      2\n      SFG_PFC_L\n      SFG_L\n      Frontal_L\n      CerebralCortex_L\n      Telencephalon_L\n    \n    \n      3\n      SFG_PFC_R\n      SFG_R\n      Frontal_R\n      CerebralCortex_R\n      Telencephalon_R\n    \n    \n      4\n      SFG_pole_L\n      SFG_L\n      Frontal_L\n      CerebralCortex_L\n      Telencephalon_L\n    \n  \n\n\n\n\nNow load in the subject data and merge it with the hierarchy data.\n\nid = 127\nsubjectData = pd.read_csv(\"assets/kirby21AllLevels.csv\")\nsubjectData = subjectData.loc[(subjectData.type == 1) & (subjectData.level == 5) & (subjectData.id == id)]\nsubjectData = subjectData[['roi', 'volume']]\n## Merge the subject data with the multilevel data\nsubjectData = pd.merge(subjectData, multilevel_lookup, on = \"roi\")\nsubjectData = subjectData.assign(icv = \"ICV\")\nsubjectData = subjectData.assign(comp = subjectData.volume / np.sum(subjectData.volume))\nsubjectData.head()\n\n\n\n\n\n  \n    \n      \n      roi\n      volume\n      level4\n      level3\n      level2\n      level1\n      icv\n      comp\n    \n  \n  \n    \n      0\n      SFG_L\n      12926\n      SFG_L\n      Frontal_L\n      CerebralCortex_L\n      Telencephalon_L\n      ICV\n      0.009350\n    \n    \n      1\n      SFG_R\n      10050\n      SFG_R\n      Frontal_R\n      CerebralCortex_R\n      Telencephalon_R\n      ICV\n      0.007270\n    \n    \n      2\n      SFG_PFC_L\n      12783\n      SFG_L\n      Frontal_L\n      CerebralCortex_L\n      Telencephalon_L\n      ICV\n      0.009247\n    \n    \n      3\n      SFG_PFC_R\n      11507\n      SFG_R\n      Frontal_R\n      CerebralCortex_R\n      Telencephalon_R\n      ICV\n      0.008324\n    \n    \n      4\n      SFG_pole_L\n      3078\n      SFG_L\n      Frontal_L\n      CerebralCortex_L\n      Telencephalon_L\n      ICV\n      0.002227\n    \n  \n\n\n\n\nHere’s a sunburst plot of a subject’s brain volumetrics.\n\nfig = px.sunburst(subjectData, path=['icv', 'level1', 'level2', 'level3', 'level4', 'roi'], \n                  values='comp', width=800, height=800)\nfig.show()\n\n\n                                                \n\n\nSimilarly, we can make a treemap.\n\nfig = px.treemap(subjectData, \n                 path = ['icv', 'level1', 'level2', 'level3', 'level4', 'roi'], \n                 values='comp',\n                 color='comp', \n                 color_continuous_scale = 'RdBu',\n                 color_continuous_midpoint = .005,\n                 width=800, height=800\n                )\nfig.show()"
  },
  {
    "objectID": "graphics_interactive.html#interactive-maps-using-folium-and-leaflet",
    "href": "graphics_interactive.html#interactive-maps-using-folium-and-leaflet",
    "title": "17  Interactive graphics",
    "section": "17.2 Interactive maps using folium and leaflet",
    "text": "17.2 Interactive maps using folium and leaflet\nA common form of interactive graphic is a map. There are several mapping libraries for python, including some in plotly. folium is another option that connects to the well known leaflet javascript library. Let’s create a quick plot of the Bloomberg School of Public Health Building, which is at latitude and longitude 39.298, -76.590. If you haven’t already, pip or conda install folium.\n\nimport folium \n\nm = folium.Map(location = [39.298, -76.590], zoom_start = 15)\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\nYou can then add elements to the map. For example, suppose we want a marker on the building saying “my office”. It’s just that easy! This is truly just the tip of the iceberg of using folium/leaflet.\n\nfolium.Marker([39.298, -76.590], popup = \"What it says when you click\",  tooltip = \"What it says when you hover\").add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "graphics_theory.html",
    "href": "graphics_theory.html",
    "title": "18  Advanced: theory of graphical display",
    "section": "",
    "text": "One of the main design arguments for the graphical display of information is data / ink maximization (tufte1990data?). This is the idea that idea that as much of the “ink” (non-background pixels) of the plot as possible should be displaying data.\nData/ink maximalization has been criticized empirically. For example, (inbar2007minimalism?) conducted a study with 87 undergraduates and found a clear preference for the non-maximized variations. Another line of argument discusses the “paradox of simplicity” (norman2007simplicity?), (eytam2017paradox?), whereby we have a strong aesthetic preference for simplicity, but also want flexibility and maximum utility.\n\n(bertin1983semiology?)\n\n\n\n\n(wickham2010graphical?)\n\n\n\n\n\n(cleveland1987research?)\n(cleveland1984many?)\n(cleveland1980calendar?)\n(carswell1992choosing?)\n(cleveland1986experiment?)\nMagical thinking (diaconis2006theories?)"
  },
  {
    "objectID": "graphics_theory.html#implementation",
    "href": "graphics_theory.html#implementation",
    "title": "18  Advanced: theory of graphical display",
    "section": "18.2 Implementation",
    "text": "18.2 Implementation\n\n18.2.1 Grammar of graphics\n\n(wilkinson2012grammar?)\n(wilkinson2013grammar?)\n(wickham2010layered?)\n\n\n\n18.2.2 Narative storytelling\nEdward and Jeffrey ((segel2010narrative?)) argue regarding the use of modern interactive tools in data narrative storytelling. They give seven canonical genres of narrative visulation."
  },
  {
    "objectID": "graphics_theory.html#graph-galleries-and-further-reading",
    "href": "graphics_theory.html#graph-galleries-and-further-reading",
    "title": "18  Advanced: theory of graphical display",
    "section": "18.3 Graph galleries and further reading",
    "text": "18.3 Graph galleries and further reading\n\n18.3.1 Further reading\n\nKarl Broman on How to display data badly\nKarl Broman Data Vizualization\nKarl Broman 10 worst plots\nKarl Broman Data visualization\n\n\n\n18.3.2 Graph galleries\n\nR graph gallery\nMatplotlib graph gallery\nPlotly\nD3 gallery\nVega gallery\nSeaborn\n\n\n\n18.3.3 Historically famous graphics\n\nhttps://medium.com/stotle-inc/the-greatest-graph-in-history-1155e0c25671Z\nhttps://plotlygraphs.medium.com/seven-modern-remakes-of-the-most-famous-graphs-ever-made-8ef30da1ab00\nhttps://www.datavis.ca/gallery/historical.php\nhttps://towardsdatascience.com/a-short-history-of-data-visualisation-de2f81ed0b23\nhttps://www.tableau.com/learn/articles/best-beautiful-data-visualization-examples\n\n\n\n18.3.4 Infographics in the media\n\nhttps://www.nytimes.com/spotlight/graphics\nJHU covid map"
  },
  {
    "objectID": "graphics_eda.html",
    "href": "graphics_eda.html",
    "title": "15  Exploratory data analysis example",
    "section": "",
    "text": "A picture is worth a 1,000 words\nOr saying how impactful intrer-ocular content is (i.e. when information hits you right between the eyes).\nI’m using Seaborn as the framework. There’s several plotting frameworks in python, but I find that seaborn has the nicest default plotting options. Also, it’s built on top of matplotlib, which is the main plotting library for DS for python.\nLet’s start with loading up some libraries.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()\n\nThe command sns.set sets the seaborn style. This sets the style for all matplotlib plots, even if not created in seaborn. I like the seaborn style, so I usually set it this way.\nFirst let’s download the data. Then we’ll read it in and drop some columns that aren’t needed for this analysis.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/kirby21.csv\")\ndf = df.drop(['Unnamed: 0', 'rawid', 'min', 'max', 'mean', 'std'], axis = 1)\ndf.head(4)\n\n\n\n\n\n  \n    \n      \n      roi\n      volume\n      type\n      level\n      id\n      icv\n      tbv\n    \n  \n  \n    \n      0\n      Telencephalon_L\n      531111\n      1\n      1\n      127\n      1378295\n      1268519\n    \n    \n      1\n      Telencephalon_R\n      543404\n      1\n      1\n      127\n      1378295\n      1268519\n    \n    \n      2\n      Diencephalon_L\n      9683\n      1\n      1\n      127\n      1378295\n      1268519\n    \n    \n      3\n      Diencephalon_R\n      9678\n      1\n      1\n      127\n      1378295\n      1268519\n    \n  \n\n\n\n\nLet’s look at the Type 1 Level 1 data and create a variable called comp which is brain composition, defined as the regional volumes over total brain volume. We’ll do this by selecting roi and comp then grouping by roi (region of interest) and taking the mean of the compostions.\n\n## Extract the Type 1 Level 1 data\nt1l1 = df.loc[(df['type'] == 1) & (df['level'] == 1)]\n\n\n## create a composition variable\nt1l1 = t1l1.assign(comp = t1l1['volume'] / t1l1['tbv'])\nt1l1 = t1l1.loc[t1l1['roi'] != 'CSF']\n\nLet’s get the mean of the composition variable across subjects by ROI. This is done by grouping by ROI then averaging over composition.\n\nsummary = t1l1[['roi', 'comp']].groupby('roi', as_index=False).mean()\nprint(summary)\n\n               roi      comp\n0   Diencephalon_L  0.007563\n1   Diencephalon_R  0.007634\n2    Mesencephalon  0.008647\n3    Metencephalon  0.124883\n4   Myelencephalon  0.003785\n5  Telencephalon_L  0.420305\n6  Telencephalon_R  0.427184\n\n\nOK, let’s try our first plot, a seaborn bar plot.\n\ng = sns.barplot(x='roi', y = 'comp', data = summary);\n## this is the matplotlib command for rotating \n## axis tick labels by 90 degrees.\nplt.xticks(rotation = 90);\n\n\n\n\nUnfortunately, seaborn doesn’t have a stakced bar chart. However, pandas does have one built in. To do this, however, we have to create a version of the data with ROIs as the columns. This can be done with a pivot statement. This converts our data from a “long” format to a “wide” format.\n\nt1l1pivot = t1l1.pivot(index = 'id', columns = 'roi', values = 'volume')\nt1l1pivot.head(4)\n\n\n\n\n\n  \n    \n      roi\n      Diencephalon_L\n      Diencephalon_R\n      Mesencephalon\n      Metencephalon\n      Myelencephalon\n      Telencephalon_L\n      Telencephalon_R\n    \n    \n      id\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      127\n      9683\n      9678\n      10268\n      159402\n      4973\n      531111\n      543404\n    \n    \n      142\n      9156\n      9071\n      10516\n      165803\n      4912\n      545603\n      552216\n    \n    \n      239\n      8937\n      9004\n      9070\n      124504\n      4023\n      483107\n      490805\n    \n    \n      346\n      8828\n      8933\n      9788\n      135090\n      4428\n      558849\n      568830\n    \n  \n\n\n\n\nNow that the data is in the right format, we can do our plot.\n\nt1l1pivot.plot(kind='bar', stacked=True, legend= False);\nplt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5));\n\n\n\n\nLet’s do some scatterplots. Let’s look at bilateral symmetry of the telencephalon. That is, let’s plot the right telencephalon versus the left telencephalon.\n\nsns.scatterplot(x = 'Telencephalon_L', y = 'Telencephalon_R', data = t1l1pivot);\nplt.xticks(rotation = 90);\n#plot an identity line from the data min to the data max\nx1 = min([t1l1pivot.Telencephalon_L.min(), t1l1pivot.Telencephalon_R.min()])\nx2 = max([t1l1pivot.Telencephalon_L.max(), t1l1pivot.Telencephalon_R.max()])\nplt.plot([x1, x2], [x1 , x2]);\n\n\n\n\nThis plot has the issue that there’s a lot of blank space. This is often addressed via a mean difference plot. This plot shows (X+Y) / 2 versus (X-y). This is basically just rotating the plot above by 45 degrees to get rid of all of the blank space around the diagonal line. Alternatively, you could plot (log(x) + log(y)) / 2 versus log(X) - log(Y). This plots the log of the geometric mean of the two observations versus the log of their ratio. Sometimes people use log base 2 or log base 10.\n\nt1l1pivot = t1l1pivot.assign(Tel_logmean = lambda x: (np.log(x.Telencephalon_L) * .5 +  np.log(x.Telencephalon_R)* .5))\nt1l1pivot = t1l1pivot.assign(Tel_logdiff = lambda x: (np.log(x.Telencephalon_R) -  np.log(x.Telencephalon_L)))\nsns.scatterplot(x = 'Tel_logmean', y = 'Tel_logdiff', data = t1l1pivot);\nplt.axhline(0, color='green');\nplt.xticks(rotation = 90);\n\n\n\n\nThus, apparently, the right side is always a little bigger than the left and the scale of the ratio is \\(e^{0.02}\\) while the scale of the geometric mean is \\(e^{13}\\). Note, \\(\\exp(x) \\approx 1 + x\\) for \\(x \\approx 0\\). So it’s about 2% larger. A note about right versus left in imaging. Often the labels get switched as there are different conventions (is it the right of the subject or the right of the viewer when looking straight at the subject?). Typically, it’s known that some of the areas of subject’s left hemisphere are larger and so it’s probably radiological (right of the viewer) convention here. Here’s a nicely done article about right versus left brain.\n(Also, in case you don’t believe me, here’s a plot of \\(e^x\\) versus \\(1+x\\) for values up to 0.1. This is the so-called Taylor expasion for \\(e^x\\) around 0. Notice the approximation gets worse, the curves diverge, as you get further away from 0.)\n\n## A sequence of numbers from 0 to .1 spaced by 0.001\nx = np.arange(0, .1, .001)\nex = np.exp(x)\n\nsns.lineplot(x = x, y = ex)\nplt.plot(x, x + 1)"
  },
  {
    "objectID": "graphics_images.html",
    "href": "graphics_images.html",
    "title": "19  Working with images",
    "section": "",
    "text": "Images broadly come in two types, vector and raster. Vector graphics are in formats like pdf, eps, svg and raster graphics are like jpeg, gif, png. Vector graphics store the image constructs and shapes. So, a vector graphics renderer can zoom in indefinitely on a shape and its edges will appear sharp. Vector fonts work this way. Raster graphics basically store a matrix and the pixels on the screen show the values of that matrix. Bitmapped fonts work this way. Of course, vector graphics have to be converted to raster to be actually displayed by the computer. Finally, some rater graphics formats have compression, which we won’t really discuss."
  },
  {
    "objectID": "graphics_images.html#working-with-raster-graphics",
    "href": "graphics_images.html#working-with-raster-graphics",
    "title": "19  Working with images",
    "section": "19.2 Working with raster graphics",
    "text": "19.2 Working with raster graphics\nRaster images are typically stored as an array. Grayscale images are matrices with the image intensity as the value and color pictures are stored as 3D arrays with the two main dimensions and color channels. A library for working with regular images in python is called PIL.\nThere are different raster specifications. RGB has 3 color channels, red, green and blue. CMYK has four: cyan, magenta, yellow and black. It’s interesting to note that the use of color channels existed before color cameras, when photographers would use different filters and additive and subtractive processes. The photograph below was created in 1877 by Louis Ducos du Hauron.\n\n\n\nColor image\n\n\nReading and working with images in python is quite easy because of the Python Image Library (PIL).\n\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nimg = Image.open(\"assets/images/Duhauron1877.jpg\")\n## You can see it with this, or img.show()\nimg\n\n\n\n\nPIL objects come with a ton of methods. For example, if we want to know whether we have an RGB or CMYK image, just print its mode.\n\nprint(img.mode)\n\nRGB\n\n\n\nr, g, b = img.split()\n\nplt.figure(figsize=(10,4));\nplt.subplot(1, 3, 1);\nplt.axis('off');\nplt.imshow(r);\n\nplt.subplot(1, 3, 2);\nplt.axis('off');\nplt.imshow(g);\n\nplt.subplot(1, 3, 3);\nplt.axis('off');\nplt.imshow(b);\n\n\n\n\nIf you’re tired of working with the image as a PIL object, it’s easy to convert to a np array.\n\nimg_array = np.array(img)\nimg_array.shape\n\n(1132, 1548, 3)\n\n\nBefore we leave PIL, it should be said that most image operations can be done in it. For example, cropping.\n\nbbox = [500, 630, 700, 760]\ncropped = img.crop(bbox)\ncropped\n\n\n\n\nWe can rotate the house and put it back\n\nrot = cropped.transpose(Image.Transpose.ROTATE_180)\nrot\n\n\n\n\n\n##Note this overwrites the image\nimg.paste(rot, bbox)\nimg"
  },
  {
    "objectID": "graphics_images.html#image-mathematics",
    "href": "graphics_images.html#image-mathematics",
    "title": "19  Working with images",
    "section": "19.3 Image mathematics",
    "text": "19.3 Image mathematics\n\n19.3.1 Convolutions\n\n19.3.1.1 1D transforms\nConvolutions are an important topic in mathematics, statistics, signal processing … Let’s discuss 1D convolutions first. A real valued convolution of two continuous signals, \\(X(t)\\) and \\(K(t)\\) is defined as \\(X* K\\)\n\\[\n(X* K)(t) = \\int_{-\\infty}^{\\infty} X(u) K(t-u) du\n= \\int_{-\\infty}^{\\infty} X(t-v) K(v) dv,\n\\]\nwhere the equality is determined by a simple change of variable argument. The discrete analog is\n\\[\n(X* K)(t) = \\sum_{u = -\\infty}^{\\infty} X(u) K(t-u)\n= \\sum_{v = -\\infty}^{\\infty} X(t-v) K(v)\n\\]\nThe convolution has many, many uses in data science and statistics. For example, the convolution of densities or mass functions is the respective density or mass function for the sum of random variables from those distributions. In applied data analysis, you can think of the convolution between \\(X\\) and \\(K\\) as smearing the function \\(K\\) over the function \\(X\\). Thus, it plays a key role in smoothing. Let’s try an example using the covid data and a box kernel. We take \\(K(t) = I\\{0 \\leq t < M\\} / M\\) (i.e. is 1 for times 0 to \\(M-1\\), then rescaled so it sums to 1). Assume that \\(N\\geq M\\) and that \\(X(t)\\) and \\(K(t)\\) are \\(0\\) and for \\(t < 0\\) or \\(t > N\\). Then, our convolution works out to be\n\\[\n(X* K)(t)\n= \\sum_{u = -\\infty}^{\\infty} X(u) K(t-u)\n= \\sum_{u = 0}^{N} X(u) K(t-u)\n= \\sum_{u = t}^{t + M - 1} X(u) K(t -u)\n= \\sum_{u = t}^{t + M - 1} X(u) / M\n\\]\nThat is, our convolution is a moving average of \\(X\\) where the convolution at point \\(t\\) is the average of the points between \\(t\\) and \\(t + M - 1\\). So, the convolution, as we’ve defined it, at point \\(t\\) is the moving average at point \\(t + (M-1)/2\\) (ie. it’s shifted by \\((M-1)/2\\)). Also, at the end (\\(t \\geq N - M + 1\\)), we’re averaging in the assumed zero values of the \\(X\\). This might be reasonable to do, or maybe not. The fact that we’re padding the end and not the beginning is just because of the range of index values we defined the kernel on. We’d have the same problem only on the other end if \\(K(t) = I(-M < t \\leq 0)/M\\). Of course, the computer will start summing things at index 0 regardless. However, it can shift the kernel relative to the signal arbitrarily by zero padding one end or the other or both. A reasonable strategy is to set it so that it averages in \\((M-1)/2\\) on both ends. Numpy allows you to only look at the range of \\(N - M\\) middle values where this isn’t an issue (argument mode = \"valid\").\nNote we could make the kernel weight points differently than just a box kernel. A popular choice is a Gaussian distribution.\nAlso, the convolution has \\(N+M-1\\) points. So, it has more time points than the original signal. Numpy has options to shift the convolution back into the same space as the original signal for you (i.e. has \\(N\\) points, mode = \"same\"). Or, you can just do it yourself if you do mode = \"full\", just shift by \\((M-1)/2\\). Similarly shift for mode = \"valid\" (but the convolution has fewer points in this case, so it won’t have corresponding points with \\(X\\) at the very beginning and end).\nHere’s an example using Italy’s daily covid case count data. We plot the data and the convolution smoothed data. In the bottom panels, we show the residuals to highlight the difference.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\ndat = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n## Get Italy, drop everyrthing except dates, convert to long (unstack converts to tuple)\nX = dat[dat['Country/Region'] == 'Italy'].drop([\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"], axis=1).unstack()\n## convert from tuple to array\nX = np.asarray(X)  \n## get case counts instead of cumulative counts\nX = X[1 : X.size] - X[0 : (X.size - 1)]\n## get the first non zero entry\nX =  X[np.min(np.where(X !=  0)) : X.size]\nplt.plot(X)\n\n\n\n\nNow let’s plot the convolutions with different options in np.convolve.\n\n## 41 day moving average\nN = len(X)\nM = 41\n\nfig, axes = plt.subplots(2, 3, figsize = [12.4, 12.4])\naxes[0,0].plot(X)\naxes[0,1].plot(X)\naxes[0,2].plot(X)\n\nK = np.ones(M) / M\n\n## Plot the convolution with the argument 'same'\n## this gives N (assumed greater than M) points\nXC = np.convolve(X, K, 'same')\naxes[0,0].plot(XC)\naxes[1,0].plot(X - XC)\n\n## Plot the convolution with the argument 'full'\n## which gives N+M-1 total pionts\nXC = np.convolve(X, K, 'full')\ntemp = np.pad(X, (M-1, 0), 'constant') \naxes[0,1].plot(XC)\naxes[1,1].plot(temp- XC)\n\n\n## Plot the convolution with the convolution shifted back by (M-1)/2\nXCshifted = XC[ (int((M - 1)/2)) : int(len(XC) - (M - 1)/2) ]\naxes[0, 2].plot(XCshifted)\naxes[1, 2].plot(X - XCshifted)\n## 41 day moving average\nN = len(X)\nM = 41\n\nfig, axes = plt.subplots(2, 3, figsize = [12.4, 12.4])\naxes[0,0].plot(X)\naxes[0,1].plot(X)\naxes[0,2].plot(X)\n\nK = np.ones(M) / M\n\n## Plot the convolution with the argument 'same'\n## this gives N (assumed greater than M) points\nXC = np.convolve(X, K, 'same')\naxes[0,0].plot(XC)\naxes[1,0].plot(X - XC)\n\n## Plot the convolution with the argument 'full'\n## which gives N+M-1 total pionts\nXC = np.convolve(X, K, 'full')\ntemp = np.pad(X, (M-1, 0), 'constant') \naxes[0,1].plot(XC)\naxes[1,1].plot(temp- XC)\n\n\n## Plot the convolution with the convolution shifted back by (M-1)/2\nXCshifted = XC[ (int((M - 1)/2)) : int(len(XC) - (M - 1)/2) ]\naxes[0, 2].plot(XCshifted)\naxes[1, 2].plot(X - XCshifted)\n\n\n\n\n\n\n\nLet’s show that the first point and end point of the convolution are the averages of \\((M-1)/2\\) points and and \\((M-1)/2+1\\) zeros at the beginning or end of the original signal just to show that our intuition is correct.\n\ntemp = np.convolve(X, K, 'same')\n[\n  # the first convolution point (temp[0]) and the average of the\n  # the first (M-1) / 2 X points and (M-1)/2 + 1 zeros\n  [temp[0],     X[0 : int(    (M - 1) / 2)].sum() / M],\n  # the last convolution point (temp[N-1]) and the average of the\n  # the last (M-1) / 2 X points and (M-1)/2 + 1 zeros\n  [temp[N - 1], X[int(N - (M - 1) / 2 - 1)  : N].sum() / M]\n \n]\n\n[[0.07317073170731708, 0.07317073170731707],\n [2163.0731707317073, 2163.0731707317073]]\n\n\nAlso, I averaged a lot (41 days) in order to make the shift very apparent. Let’s look at the performance for less wide of a kernel.\n\n## 21 day moving average\nM = 21\nK = np.ones(M) / M\n\nfig, axes = plt.subplots(1, 2, figsize = [12.4, 6.2])\nXC = np.convolve(X, K, 'same')\naxes[0].plot(X)\naxes[0].plot(XC)\naxes[1].plot(X - XC)\n\n\n\n\nIt should be stated that the convolution operation is multiplication in Fourier space. So, functions like np.convolve are performing FFTs in the background. However, if you’re going to do this yourself, make sure to keep track of indices and zero padding. (I.e. the bookkeeping.) Otherwise, the FFT wraps around and you get a little of the end averaged in with the beginning and vice versa. I work out getting the same answer as mode = “same” below.\n\nfig, axes = plt.subplots(1, 2, figsize = [12.4, 6.2])\n\n## Pad the X with zeros in the back, need at least M-1 \npad_width = (0, M - 1)\nXpadded = np.pad(X, pad_width, \"constant\")\n## Pad the kernel in the back with N-1, so both the kernel\n## and the X are of length, N+M-1\nKpadded = np.pad(K, (0, N - 1))\n\n## Note we take the real part b/c the complex part is all effectively \n## machine 0\nconvolution = np.fft.ifft(np.fft.fft(Xpadded) * np.fft.fft(Kpadded)).real\n\n## At this point the convolution is of length N + M - 1\n## To get it comparable with the original X, subtract (M-1)/2 indices\n## from each end\nconvolution = convolution[ int((M-1)/2) : int(N+(M-1)/2)]\n\n## Let's see how we did\naxes[0].plot(X)\naxes[0].plot(convolution)\n\n#Show they're the same by plotting the subtraction\naxes[1].plot(convolution - XC)\n\n\n\n\n\n\n19.3.1.2 2D transforms\nFor two dimensions, the convolution is similar\n\\[\n(X ** K)(i,j) = \\sum_{u=-\\infty}^{\\infty} \\sum_{v=-\\infty}^{\\infty}\nX(u, v)  K(i -u, k - v) = \\sum_{u=-\\infty}^{\\infty} \\sum_{v=-\\infty}^{\\infty}\nK(u, v)  X(i -u, k - v)  \n\\]\nOnce again, let’s think where \\(X\\) is of dimension \\((N_1, N_2)\\) and 0 outside of that range, and\n\\[\nK(u, v) = I(0 \\leq u < M_1, 0 \\leq v < M_2) / (M_1 M_2)\n\\]\n(i.e. \\(K\\) is a box on \\(M_1 \\leq N_1\\), \\(M_2 < N_2\\)). Then, applying the exact same argument as before, the convolution is:\n\\[\n(X ** K)(i,j) = \\sum_{u=i}^{M_1 + i - 1} \\sum_{v=j}^{M_2 + j - 1}\nX(u, v) / (M_1 M_2)\n\\]\nThat is, the convolution at point \\((i,j)\\) is the average of the neighboring points. Also, all of the same bookkeeping, zero padding and Fourier transform stuff apply (using the 2D FFT).\nFor regular kernels (box kernels, 2D Gaussians), convolution smooths the image, which has the efffect of making it blurrier. The kernel width determines how blurry the image will then be. This is typically done to denoise an image (to blur out the noise). Let’s try it on a cartoon image of Brian. We’ll just stick to a black and white image so that it’s 2D. A color image has 3 color channels, so is a 3D array. (However, you see the patten; you should be able to extend this to 3D with little problem.)\n\nimport PIL\nimport scipy.signal as sp\nimport urllib.request\n\n\nimgURL = \"https://github.com/smart-stats/ds4bio_book/raw/main/book/bcCartoon.png\"\nurllib.request.urlretrieve(imgURL, \"bcCartoon.png\")\nimg = np.asarray(PIL.Image.open(\"bcCartoon.png\").convert(\"L\"))\n\nplt.xticks([])\nplt.yticks([])\nplt.imshow(img, cmap='gray', vmin=0, vmax=255)\n\n<matplotlib.image.AxesImage at 0x7e54524b3d60>\n\n\n\n\n\nNow let’s take this image and convolve it with different kernels of different window lengths.\n\ndef kernel(i, j):\n  return np.ones((i, j)) / np.prod([i, j])\n\nplt.figure(figsize=[12.4, 12.4])\nimgC = sp.convolve2d(img, kernel(4, 4))\nplt.subplot(2, 2, 1)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"4x4\")\n\nimgC = sp.convolve2d(img, kernel(8, 8))\nplt.subplot(2, 2, 2)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"8x8\")\n\nimgC = sp.convolve2d(img, kernel(16, 16))\nplt.subplot(2, 2, 3)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"16x16\")\n\nboxsize = (5, 5)\nimgC = sp.convolve2d(img, kernel(32,32))\nplt.subplot(2, 2, 4)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"32x32\")\n\nText(0.5, 1.0, '32x32')\n\n\n\n\n\n\n\n19.3.1.3 Convolutional neural networks\nOf course, your kernel doesn’t have to be a box, or a truncated, discretized bivariate Gaussian density or even be non-negative. It’s helpful for smoothers to have non-negative kernels, since they’re just taking a generalized variation of a moving average that way. But, we want to use convolutions\nmore generally. Here, let’s take a kernel that is part of the image (left eye) and convolve it. I’ll make the kernel super peaked at eye features by extracting the eye and raising it to the 4th power.\nSo a relu activation function plus a bias term would then be able to highlight different thresheld variations of this convolution image. For example, here I add a bias term to the convolution then apply a leaky relu. You can see it just highlights the one area where the eye is. A leaky relu is\n\\[\nlrelu(x, c) = \\left\\{\n  \\begin{array}{ll}\n  x & \\text{if $x > 0$} \\\\\n  x * c & \\text{otherwise}\n  \\end{array}\n  \\right.\n\\]\nwhere \\(c\\) is usually set to a small value. If \\(c=0\\) the leaky relu is just the relu. I set \\(c\\) to be 0.05 so that we can see the background image.\n\nplt.figure(figsize=[12.4, 6.2])\n\nK = img[200 : 270,225 : 322]\nplt.subplot(1, 3, 1)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(K,  cmap='gray', vmin=0, vmax=255)\n## I normalized it this way so that the convolution\n## numbers wouldn't be so big\n## Also, I put it to the 4th power, so it exactly finds \n## the eye.\nK = K ** 4\nK = K / K.sum()\nK = K - K.mean()\n\nimgC = sp.convolve2d(img, K)\nplt.subplot(1, 3, 2)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC)\nplt.title(\"Convolution\")\n\ntemp = imgC.copy()\n## Add a bias term of -15\ntemp -= 15\n## Perform a leaky relu\ntemp[np.where(temp < 0)] = temp[np.where(temp < 0)] * .05\n\nplt.subplot(1, 3, 3)\nplt.imshow(temp)\nplt.xticks([])\nplt.yticks([])\nplt.title(\"LRELU of convolution + bias\")\n\nText(0.5, 1.0, 'LRELU of convolution + bias')\n\n\n\n\n\nBecause of how convolutions work, this will find this eye anywhere in the image. Here we just add another eye somewhere else and repeat the convolution.\n\nplt.figure(figsize=[12.4, 6.2])\n\n#put another eye in the image\nimgCopy = img.copy()\nimgCopy[60 : 130, 85 : 182] = img[200 : 270,225 : 322]\nplt.subplot(1, 2, 1)\nplt.imshow(imgCopy,  cmap='gray', vmin=0, vmax=255)\nplt.xticks([])\nplt.yticks([])\n\nimgC = sp.convolve2d(imgCopy, K)\n\nplt.subplot(1, 2, 2)\ntemp = imgC.copy()\n## Add a bias term of -15\ntemp -= 15\n## Perform a leaky relu\ntemp[np.where(temp < 0)] = temp[np.where(temp < 0)] * .05\n\nplt.subplot(1, 2, 2)\nplt.imshow(temp)\nplt.xticks([])\nplt.yticks([])\nplt.title(\"LRELU of convolution + bias\")\n\nText(0.5, 1.0, 'LRELU of convolution + bias')\n\n\n\n\n\nSo, we found a custom kernel that highlights this specific feature in images. Convnets layers learn the kernel. That is, CNNs learn the image that gets convolved with the previous layer to produce the next one. Here’s a really great pictorial guide by Sumit Saha.\nNow, let’s discuss some specific vocabulary used in CNNs.\n\nPadding zero padding just like we discussed for 1D transformations\nPooling pooling, often max pooling, is a dimension reduction technique, taking the max in little blocks.\nstride length instead of sliding the kernel by moving it one pixel at a time, move it more to increase computational efficiency and reduce the size of the output convolution."
  },
  {
    "objectID": "nns_intro.html",
    "href": "nns_intro.html",
    "title": "23  Neural networks, introduction",
    "section": "",
    "text": "Let’s start by relating neural networks to regression. Consider a simple case where we have two nodes, \\(1\\) and \\(X\\) pointing to an outcome \\(Y\\). What does this mean? Let’s first put some context around the problem. Imagine that we want to use a subject’s BMI \\(X\\) to predict their blood pressure, \\(Y\\). This diagram represents that.\n\n\n\n\n\nTo interpret this diagram as a neural network, consider the following rule:\n\n\n\n\n\n\nNote\n\n\n\nParent nodes that point to a child node are multiplied by weights then added together then operated on by an activation function to form the child node.\n\n\nIf the parent nodes point to the outcome, then the nodes are combined the operated on by a known function, called the activation function to form a prediction. So, in this case, this is saying that the intercept (node labeled \\(1\\))times a weight plus BMI (node labeled \\(X\\)) times a different weight get combined to form a prediction for SBP \\(Y\\). Or, in other words\n\\[\n\\hat Y = g(w_0 \\times 1 + w_1 \\times X)\n\\]\nwhere \\(g\\) is a function that we specify. So in this case, if \\(w_0 = 120\\), \\(w_1 = .1\\) and \\(g\\) is an idenity function, \\(g(a) = a\\), and a subject had a BMI of 30, then the prediction would be\n\\[\n\\hat Y = g(120 + .1 * 30) = 120.3\n\\]\nNote \\(g\\) is not shown in the diagram (though maybe you could with the shape of the child node) or something like that0. Also not shown in the daigram is:\n\nThe loss function, i.e. how to measure the different between \\(\\hat Y\\) and \\(Y\\).\nThe way the loss function combines subjects; we have multiple BMIs and SBPs\nHow we obtain the weights, \\(W_0\\) and \\(W_1\\); this is done by minmizing the loss function using an algorithm\n\nSo, imagine the case where \\(g\\) is an identity function, our loss function for different subjects is squared error and we combine different losses by adding them up. Then, our weights are obtained by minmizing\n\\[\n\\sum_{i=1}^N (Y_i - \\hat Y_i)^2\n\\]\nand so, presuming our optimization algorithm works well, it should be idential to linear regression.\nConsider a different setting. Imagine if our \\(Y\\) is 0 or 1 based on whether or not the subject is taking anti-hypertensive mediations. Further, let \\(g\\) be the sigmoid function, \\(g(a) = 1 / \\{1 + \\exp(-a)\\}\\). Our prediction is\n\\[\n\\hat Y = \\{1 + \\exp(-W_0 - W_1 X)\\}^{-1}\n\\]\nwhich is the logistic regression prediction with intercept \\(W_0\\) and slope \\(W_1\\). Consider a case where \\(W_0 = -4\\), \\(W_1 = .1\\) and \\(X=30\\), then our \\(\\hat Y = 1 / \\{1 + \\exp[-(-4 + .1\\times 30)\\}]\\approx .27\\). Thus, this model estimates a 27% probability that a subject with a BMI of 30 has hypertension.\nFurther, if we specify that the loss function is binary cross entropy\n\\[\n- \\sum_{i=1}^n \\{ Y_i \\log(\\hat Y_i) + (1 - Y_i) \\log(1 - \\hat Y_i)\\} / N\n\\]\nthen minmizing our loss function is identical to maximizing the likelihood for logistic regression.\n\n1 / (1 + np.exp(-(-4 + .1 * 30)))\n\n0.2689414213699951"
  },
  {
    "objectID": "nns_intro.html#more-layers",
    "href": "nns_intro.html#more-layers",
    "title": "23  Neural networks, introduction",
    "section": "23.2 More layers",
    "text": "23.2 More layers\nOf course, there’d be no point in using NNs for problems that we can just solve with generalized linear models. NNs get better when we add more layers, since then they can discover interactions and non-linearities. Consider the following model. Notice we quit explicitly adding the bias (intercept) term / node. In general assume the bias term is included unless otherwise specified.\n\n\n\n\n\nUsually, the nodes are added in so called layers. \\((X_1, X_2)\\) is the input layer, \\((H_{11}, H_{12})\\) is the first hidden layer, \\((H_{21}, H_{22})\\) is the second hidden layer and \\(Y\\) is the output layer. Imagine plugging an \\(X_1\\) and \\(X_2\\) into this network. It would feed forward through the network as\n\\[\n\\begin{align}\nH_{11} = & g_1(W_{011} + W_{111} X_1 + W_{211} X_2) \\\\\nH_{12} = & g_1(W_{012} + W_{112} X_1 + W_{212} X_2) \\\\\nH_{21} = & g_2(W_{021} + W_{121} H_{11} + W_{221} H_{12}) \\\\\nH_{22} = & g_2(W_{022} + W_{122} H_{12} + W_{222} H_{12}) \\\\\n\\hat Y = & g_3(W_{031} + W_{131} H_{21} + W_{231} H_{22})\n\\end{align}\n\\]\nwhere \\(g_k\\) are specified activation functions. Typically, we would have a different activation function for the output layer than the others, and the other would have the same activation function. So, for example, if \\(Y\\) was binary, like hypertension diagnosis, then \\(g_1=g_2\\) and \\(g_3\\) would be a sigmoid."
  },
  {
    "objectID": "nns_intro.html#activation-functions",
    "href": "nns_intro.html#activation-functions",
    "title": "23  Neural networks, introduction",
    "section": "23.3 Activation functions",
    "text": "23.3 Activation functions\nThe output activation function tends to be based on the structure of the outcome. For example, a binary outcome would likely have a sigmoidal, or other function from \\(\\mathbb{R}\\) to \\([0, 1]\\) so as to model a probability. Historically, the internal activation functions were binary thresholds. This was owning to the fact that neural networks were models of (biological) neurons and the threshold was a model of an action potential being propigated. However, modern neural networks have less of a direct connection to their biological motivation and other activation functions tend to be used. The most popular right now is the rectified linear unit (RELU) function. This is simply:\n\\[\nRELU(a) = \\left\\{\n\\begin{array}{ll}\na & \\text{if $a>0$} \\\\\n0 & \\text{otherwise}\n\\end{array}\n\\right.\n= a \\times I(a > 0)\n\\]\nPlotted, this is:\n\nplt.plot( [-1, 0, 1], [0, 0, 1], linewidth = 4);\n\n\n\n\nIf a bias term is included, then the fact that the RELU is centered at zero isn’t important, since the intercept term effectively shifts the function around. These kinds of splin terms are incredibly flexible. Just to show you an example, let’s fit the sine function using a collection of shifted RELUs. This is just\n\\[\nY = \\sin(X) + \\epsilon\n\\]\nbeing fit with\n\\[\n\\sum_{i=1}^N \\left\\{ Y_i - W_{021} - \\sum_{j=1}^{d} W_{j21} g(W_{1j1} X_i- W_{0j1}) \\right\\}^2\n\\]\nwhere the \\(W_{kj}\\) are the weights for layer \\(k\\). Below, we’re just setting \\(W_{1j1} = 1\\) and specifying the \\(W_{0j1}\\) at a sequence of values.\n\n## Generate some data, a sine function on 0,4*pi\nn = 1000\nx = np.linspace(0, 4 * np.pi, n)\ny = np.sin(x) + .2 * np.random.normal(size = n)\n\n## Generate the spline regressors\ndf = 30\nknots = np.linspace(x.min(), x.max(), df)\nxmat = np.zeros((n, df))\nfor i in range(0, df): xmat[:,i] = (x - knots[i]) * (x > knots[i])\n\n## Fit them\nfrom sklearn.linear_model import LinearRegression\nyhat = LinearRegression().fit(xmat, y).predict(xmat)\n\n## Plot them versus the data\nplt.plot(x, y);\nplt.plot(x, yhat);\n\n\n\n\nThis corresponds to a network like depicted below if there were \\(d=3\\) hidden nodes, there was a relu activation function at the first layer, then a identity activation function for the output layer and the weights for the first layer are specified.\n\n\n\n\n\nWe can actually fit this function way better using splines and a little bit more care. However, this helps show how even one layer of RELU activated nodes can start to fit complex shapes."
  },
  {
    "objectID": "nns_intro.html#optimization",
    "href": "nns_intro.html#optimization",
    "title": "23  Neural networks, introduction",
    "section": "23.4 Optimization",
    "text": "23.4 Optimization\nOne of the last bits of the puzzle we have to figure out is how to obtain the weights. A good strategy would be to minimize the loss function. However, it’s hard to minmize. If we had a derivative, we could try the following. Let \\(L(W)\\) be the loss function for weights \\(W\\). Note, we’re omitting the fact that this is a function of the data (predictors and outcome) as well, since that’s a set of fixed numbers. Consider updating parameters as\n\\[\nW^{(new)} = W^{(old}) - e * L'(W^{(old)})\n\\]\nWhat does this do? It moves the parameters by a small amount, \\(e\\), called the learning rate, in the direction the opposite of the gradient. Think of a one dimensional convex function. If the derivative at a point is positive, then that point is larger than where the minimum is. Similarily, if the derivative is negative, it’s smaller. So, the idea is to head a small amount in the opposite direction of the derivative. How much? How about along the line of the derivative? That’s all gradient descent does, just in more than one dimension.\nHow do we get the gradient? Consider the following. If \\(X\\) is our vector of predictors and \\(Y\\) is our vector of outputs, a neural network with 3 layers, can be thought of as, where \\(L_k\\) is layer \\(K\\) and \\(W_k\\) are the weights for that layer:\n\\[\nL_3(L_2(L_1(X, W_1), W_2) W_3)\n\\]\nOr a series of function compositions. Recall from calculus, if we want the derivative of composed functions we have a really simple rule called the chain rule:\n\\[\n\\frac{d}{dx}f(g(x)) = f'(g(x)) g'(x)\n\\]\nI.e. if \\(h=f(u)\\) and \\(u = g(x)\\) then \\(\\frac{dh}{dx} = \\frac{dh}{du}\\frac{du}{dx}\\). Thus, characterized this way, the chain rule formally acts like fractions (though this is a symbolic equivalence having entirely different underlying meanings).\nIf we use the chain rule on our composed loss functions, we wind up bookkeeping backwards through our neural network. That is why it’s called backwards propagation (backprop).\nSo, our algorithm goes something like this. Given, \\(W^{(new)}\\), network, \\(\\phi(X, W)\\), which depends on the predictors and the weights and loss, \\(L(Y, \\hat Y)\\), which depends on the observed and predicted outputs.\n\nSet \\(W^{(old)}=W^{(new)}\\)\nCalculate \\(\\hat Y = \\phi(X, W^{(old)})\\) and loss \\(L(Y, \\hat Y)\\).\nUse back propagation to get to get a numerical approximation to \\(\\frac{d}{dW} L\\{Y, \\phi(X, W)\\} |_{W=W^{(old)}} = L'(W^{(old)})\\)\nUpdate \\(W^{(new)} = W^{(old)} - e L'(W^{(old)})\\)\nGo to step 0."
  },
  {
    "objectID": "nns_basic_regression.html",
    "href": "nns_basic_regression.html",
    "title": "24  Basic regression as a NN",
    "section": "",
    "text": "sns.scatterplot(x = dat['T2'], y = dat['PD'])\n\n<AxesSubplot:xlabel='T2', ylabel='PD'>\n\n\n\n\n\n\nfit = smf.ols('PD ~ T2', data = dat).fit()\nfit.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:           PD          R-squared:             0.661\n\n\n  Model:                   OLS         Adj. R-squared:        0.657\n\n\n  Method:             Least Squares    F-statistic:           190.9\n\n\n  Date:             Sun, 26 Feb 2023   Prob (F-statistic): 9.77e-25\n\n\n  Time:                 17:45:21       Log-Likelihood:      -57.347\n\n\n  No. Observations:         100        AIC:                   118.7\n\n\n  Df Residuals:              98        BIC:                   123.9\n\n\n  Df Model:                   1                                    \n\n\n  Covariance Type:      nonrobust                                  \n\n\n\n\n               coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept     0.3138     0.052     6.010  0.000     0.210     0.417\n\n\n  T2            0.7832     0.057    13.815  0.000     0.671     0.896\n\n\n\n\n  Omnibus:        1.171   Durbin-Watson:         1.501\n\n\n  Prob(Omnibus):  0.557   Jarque-Bera (JB):      0.972\n\n\n  Skew:           0.241   Prob(JB):              0.615\n\n\n  Kurtosis:       2.995   Cond. No.               1.89\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# The in sample predictions\nyhat = fit.predict(dat['T2'])\n\n# Make sure that it's adding the intercept\n#test = 0.3138 + dat['T2'] * 0.7832\n#sns.scatterplot(yhat,test)\n\n## A plot of the in sample predicted values\n## versus the actual outcomes\nsns.scatterplot(x = yhat, y = dat['PD'])\nplt.plot([-1, 3], [-1, 3], linewidth=2)\n\n\n\n\n\nn = dat.shape[0]\n\n## Get the y and x from \nxtraining = torch.from_numpy(dat['T2'].values)\nytraining = torch.from_numpy(dat['PD'].values)\n\n## PT wants floats\nxtraining = xtraining.float()\nytraining = ytraining.float()\n\n## Dimension is 1xn not nx1\n## squeeze the second dimension\nxtraining = xtraining.unsqueeze(1)\nytraining = ytraining.unsqueeze(1)\n\n## Show that everything is the right size\n[xtraining.shape, \n ytraining.shape,\n [n, 1]\n ]\n\n[torch.Size([100, 1]), torch.Size([100, 1]), [100, 1]]\n\n\n\n## Show that linear regression is a pytorch \nmodel = torch.nn.Sequential(\n    torch.nn.Linear(1, 1)\n)\n\n## MSE is the loss function\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\n## Set the optimizer\n## There are lots of choices\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n\n## Loop over iterations\nfor t in range(10000):\n\n    ## Forward propagation\n  y_pred = model(xtraining)\n    \n  ## the loss for this interation\n  loss = loss_fn(y_pred, ytraining)\n\n  #print(t, loss.item() / n)\n\n  ## Zero out the gradients before adding them up \n  optimizer.zero_grad()\n  \n  ## Backprop\n  loss.backward()\n  \n  ## Optimization step\n  optimizer.step()\n\n\nytest = model(xtraining).detach().numpy().reshape(-1)\nsns.scatterplot(x = ytest, y = yhat)\nplt.plot([-1, 3], [-1, 3], linewidth=2)\n\n\n\n\n\nfor param in model.parameters():\n  print(param.data)\n\ntensor([[0.7831]])\ntensor([0.3138])"
  },
  {
    "objectID": "nns_logistic_regression.html",
    "href": "nns_logistic_regression.html",
    "title": "26  Logistic regression as a NN",
    "section": "",
    "text": "import pandas as pd\nimport torch\nimport statsmodels.formula.api as smf\nimport statsmodels as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n## Read in the data and display a few rows\ndat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv\")\ndat.head(4)\n\n## Create a binary outcome variable (people will use gold lesions in HW)\nm = np.median(dat.T2)\ndat = dat.assign(y = (dat.T2 > m) * 1 )\n## Create a normalized regression variable\ndat = dat.assign(x = (dat.PD - np.mean(dat.PD)) / np.std(dat.PD))\ndat.head()\n\n\n\n\n\n  \n    \n      \n      FLAIR\n      PD\n      T1\n      T2\n      FLAIR_10\n      PD_10\n      T1_10\n      T2_10\n      FLAIR_20\n      PD_20\n      T1_20\n      T2_20\n      GOLD_Lesions\n      y\n      x\n    \n  \n  \n    \n      0\n      1.143692\n      1.586219\n      -0.799859\n      1.634467\n      0.437568\n      0.823800\n      -0.002059\n      0.573663\n      0.279832\n      0.548341\n      0.219136\n      0.298662\n      0\n      1\n      1.181648\n    \n    \n      1\n      1.652552\n      1.766672\n      -1.250992\n      0.921230\n      0.663037\n      0.880250\n      -0.422060\n      0.542597\n      0.422182\n      0.549711\n      0.061573\n      0.280972\n      0\n      1\n      1.426453\n    \n    \n      2\n      1.036099\n      0.262042\n      -0.858565\n      -0.058211\n      -0.044280\n      -0.308569\n      0.014766\n      -0.256075\n      -0.136532\n      -0.350905\n      0.020673\n      -0.259914\n      0\n      0\n      -0.614749\n    \n    \n      3\n      1.037692\n      0.011104\n      -1.228796\n      -0.470222\n      -0.013971\n      -0.000498\n      -0.395575\n      -0.221900\n      0.000807\n      -0.003085\n      -0.193249\n      -0.139284\n      0\n      0\n      -0.955175\n    \n    \n      4\n      1.580589\n      1.730152\n      -0.860949\n      1.245609\n      0.617957\n      0.866352\n      -0.099919\n      0.384261\n      0.391133\n      0.608826\n      0.071648\n      0.340601\n      0\n      1\n      1.376909\n    \n  \n\n\n\n\n\nfit = smf.logit('y ~ x', data = dat).fit()\nfit.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.427855\n         Iterations 7\n\n\n\n\nLogit Regression Results\n\n  Dep. Variable:           y          No. Observations:       100  \n\n\n  Model:                 Logit        Df Residuals:            98  \n\n\n  Method:                 MLE         Df Model:                 1  \n\n\n  Date:            Sun, 26 Feb 2023   Pseudo R-squ.:       0.3827  \n\n\n  Time:                17:46:11       Log-Likelihood:      -42.785 \n\n\n  converged:             True         LL-Null:             -69.315 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        3.238e-13\n\n\n\n\n               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept     0.0367     0.269     0.136  0.892    -0.491     0.565\n\n\n  x             2.2226     0.436     5.095  0.000     1.368     3.078\n\n\n\n\n\n# The in sample predictions\nyhat = 1 / (1 + np.exp(-fit.fittedvalues))\n\n\nn = dat.shape[0]\n\n## Get the y and x from \nxtraining = torch.from_numpy(dat['x'].values)\nytraining = torch.from_numpy(dat['y'].values)\n\n## PT wants floats\nxtraining = xtraining.float()\nytraining = ytraining.float()\n\n## Dimension is 1xn not nx1\n## squeeze the second dimension\nxtraining = xtraining.unsqueeze(1)\nytraining = ytraining.unsqueeze(1)\n\n## Show that everything is the right size\n[xtraining.shape, \n ytraining.shape,\n [n, 1]\n ]\n\n[torch.Size([100, 1]), torch.Size([100, 1]), [100, 1]]\n\n\n\n## Doing it more now the pytorch docs recommend\n## Example taken from \n## https://medium.com/biaslyai/pytorch-linear-and-logistic-regression-models-5c5f0da2cb9\n\n## They recommend creating a class that defines\n## the model\nclass LogisticRegression(torch.nn.Module):\n     def __init__(self):\n        super(LogisticRegression, self).__init__()\n        self.linear = torch.nn.Linear(1, 1, bias = True)\n     def forward(self, x):\n        y_pred = torch.sigmoid(self.linear(x))\n        return y_pred\n\n## Then the model is simply  \nmodel = LogisticRegression()\n\n## MSE is the loss function\nloss_fn = torch.nn.BCELoss()  \n\n## Set the optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n\n## Loop over iterations\nfor t in range(100000):\n\n  ## Forward propagation\n  y_pred = model(xtraining)\n\n  ## the loss for this interation\n  loss = loss_fn(y_pred, ytraining)\n\n  #print(t, loss.item() / n)\n\n  ## Zero out the gradients before adding them up \n  optimizer.zero_grad()\n  \n  ## Backprop\n  loss.backward()\n  \n  ## Optimization step\n  optimizer.step()\n\n\nytest = model(xtraining)\nytest = ytest.detach().numpy().reshape(-1)\nplt.plot(yhat, ytest,  \".\")\nplt.plot([0, 1], [0, 1], linewidth=2)\n\n\n\n\n\nfor param in model.parameters():  \n  print(param.data)\n\ntensor([[1.3181]])\ntensor([0.0870])"
  },
  {
    "objectID": "statistics_logistic.html#classification-with-one-continuous-variable",
    "href": "statistics_logistic.html#classification-with-one-continuous-variable",
    "title": "21  Logistic regression",
    "section": "21.1 Classification with one continuous variable",
    "text": "21.1 Classification with one continuous variable\nSuppose now that we want to predict the gold standard from the FLAIR values. Fitting a line seems weird, since the outcome can only be 0 or 1. A line would allow for arbitrarily small or large predictions. Similiarly, forcing the prediction to be exactly 0 or 1 leads to difficult optimization problems. A clever solution is to instead model\n\\[\nP(Y_i = 1 ~|~ X_i)\n\\]\nwhere \\(Y_i\\) is the gold standard value (0 or 1 for no lesion or lesion at that voxel, respectively) and \\(X_i\\) is the FLAIR value for voxel \\(i\\). This solves the problem somewhat nicely, but it still leaves some issues unresolved. For example, what does probability even mean in this context? And also probabilities are between 0 and 1, that’s better than exactly 0 or 1, but still would create problems.\nFor the probability, it’s generally a good idea to think about what you’re modeling as random in the context. In this case, we’re thinking of our voxels as a random sample of FLAIR and gold standard voxel values from some population. This is a meaningful benchmark even if it’s not true. We’ll find that often in statistics we model data as if it comes from a probability distribution when we know it didn’t. We simply know that the probability distribution is a useful model for thinking about the problem.\nAs for getting the probabilities from \\([0,1]\\) to \\((-\\infty, \\infty)\\), we need a function, preferably a monotonic one. The generally agreed upon choice is the logit (natural log of the odds) function. The logit function of a probability is defined as\n\\[\n\\eta = \\mathrm{logit}(p) = \\log\\{p / (1 - p)\\}\n\\]\nwhere \\(p\\) is the probability and \\(O = p/(1-p)\\) is called the odds. Note, you can go backwards from odds to probability with the function \\(p = O / (1 + O)\\). Odds are exactly as used in gambling. If the odds of bet at 1 to 99, then you are saying the probability is \\(1 / (99 + 1) = 1\\%\\).\nWhy use odds? There’s a couple of reasons why odds are uniquely interprettable. First, there are specific study designs where odds make more sense than probabilities, particularly retrospective ones. Secondly, odds are unique in binomial models where they work out to be particularly tractible to work with. Finally, odds have a unique gambling interpretation. That is, it gives the ratio of a one dollar risk to the return in a fair bet. (A fair bet is where the expected return is 0.) So, when a horse track gives the odds on a horse to be 99 to 1, they are saying that you would get $99 dollars if you bet one dollar and the horse won. This is an implied probability of 99 / (99 + 1) = 99% that the horse loses and 1% probability that the horse wins. Note they don’t usually express it as a fraction, they usually espress it as value to 1 or 1 to value. So they would say 99 to 1 (odds against) or 1 to 99 (odds for) so you can easily see how much you’d win for a dollar bet.\nYou can go backwards from the logit function to the probability with the expit function. That is, if \\(\\eta\\) is defined as above, then\n\\[\np = \\frac{e^{\\eta}}{1 + e^\\eta} = \\frac{1}{1 + e^{-\\eta}}.\n\\]\nThis is sometimes called the expit function or sigmoid.\nWe model the log of the odds as linear. This is called logistic regression.\n\\[\n\\eta = \\mathrm{logit}\\left\\{ P(Y = 1 ~|~ X) \\right\\}\n= \\beta_0 + \\beta_1 X.\n\\]\nThe nice part about this model is that \\(e^\\beta_1\\) has the nice interpretation of the odds ratio associated with a one unit change in \\(X\\).\nThis is great, but we still need a function of the probabilities to optimize. We’ll use the cross entropy.\n\\[\n-\\sum_{i=1}^n \\left[Y_i \\log\\{P(Y_i = 1 ~|~ X_i)\\} + (1 - Y_i) \\log\\{1 - P(Y_i = 1 ~|~ X_i)\\}\\right].\n\\]\nThis function has the interpretation of being the negative of the log of the probabilities assuming the \\(Y_i\\) are independent. This model doesn’t have to hold for the minimization to be useful.\nPlugging our logit model in, the cross entropy now looks like\n\\[\n-\\sum_{i=1}^n \\left[\n  Y_i \\eta_i + \\log\\left\\{\\frac{1}{1 + e^\\eta_i} \\right\\} \\right].\n\\]\nThis is the function that we minimize to perform logistic regression. Later on, we’ll worry about how to minimize this function. However, today, let’s fit logistic regression to some data.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model as lm\n## this sets some style parameters\nsns.set()\n\n## Read in the data and display a few rows\ndat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv\")\n\n\n## Plot the data\nsns.scatterplot('FLAIR', 'GOLD_Lesions', data = dat)\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/seaborn/_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n<AxesSubplot:xlabel='FLAIR', ylabel='GOLD_Lesions'>\n\n\n\n\n\nLet’s now fit the model. Again we’re going to split into training and test data. But, now we’re not going to do it manually since we have to load a library that has a function to do this.\n\nx = dat[['FLAIR']]\ny = dat.GOLD_Lesions\ntrainFraction = .75\n\n## Once again hold out some data\nsample = np.random.uniform(size = 100) < trainFraction\nxtrain = x[ sample]\nytrain = y[ sample]\nxtest =  x[~sample]\nytest =  y[~sample]\n\n\nlr = lm.LogisticRegression(fit_intercept=True, penalty='none')\nfit = lr.fit(xtrain, ytrain)\n\nLet’s look at the parameters fit from the model\n\nbeta0, beta1 = [fit.intercept_[0], fit.coef_[0][0]]\n[beta0, beta1]\n\n[-3.384323747880977, 2.164635991519881]\n\n\n\nn = 1000\nxplot = np.linspace(-1, 5, n)\neta = beta0 + beta1 * xplot\np = 1 / (1 + np.exp(-eta))\n\nsns.scatterplot('FLAIR', 'GOLD_Lesions', data = dat, hue = 'GOLD_Lesions')\nsns.lineplot(xplot, p)\n\n## Of course, scikit has a predict\n## function so that you don't have to do this manually\n#yplot = fit.predict_proba(xplot.reshape((n, 1)))\n#sns.lineplot(xplot, yplot[:, 1])\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/seaborn/_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/seaborn/_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n<AxesSubplot:xlabel='FLAIR', ylabel='GOLD_Lesions'>\n\n\n\n\n\nNow let’s evaluate the test set.\n\n## This predicts the classes using a 50% probability cutoff\nyhat_test = fit.predict(xtest)\n\n## double checking that if you want\n#all(yhat_test == (fit.predict_proba(xtest)[:, 1] > .5))\n\naccuracy = np.mean(yhat_test == ytest)\nsensitivity = np.mean(yhat_test[ytest == 1] == ytest[ytest == 1])\nspecificity = np.mean(yhat_test[ytest == 0] == ytest[ytest == 0])\nnp.round([accuracy, sensitivity, specificity], 3)\n\narray([0.654, 0.727, 0.6  ])\n\n\n\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\n\nptest = fit.predict_proba(xtest)[:, 1]\nfpr, tpr, thresholds = roc_curve(ytest, ptest)\nroc_auc = auc(fpr, tpr)\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "statistics_logistic.html",
    "href": "statistics_logistic.html",
    "title": "22  Logistic regression",
    "section": "",
    "text": "Suppose now that we want to predict the gold standard from the FLAIR values. Fitting a line seems weird, since the outcome can only be 0 or 1. A line would allow for arbitrarily small or large predictions. Similiarly, forcing the prediction to be exactly 0 or 1 leads to difficult optimization problems. A clever solution is to instead model\n\\[\nP(Y_i = 1 ~|~ X_i)\n\\]\nwhere \\(Y_i\\) is the gold standard value (0 or 1 for no lesion or lesion at that voxel, respectively) and \\(X_i\\) is the FLAIR value for voxel \\(i\\). This solves the problem somewhat nicely, but it still leaves some issues unresolved. For example, what does probability even mean in this context? And also probabilities are between 0 and 1, that’s better than exactly 0 or 1, but still would create problems.\nFor the probability, it’s generally a good idea to think about what you’re modeling as random in the context. In this case, we’re thinking of our voxels as a random sample of FLAIR and gold standard voxel values from some population. This is a meaningful benchmark even if it’s not true. We’ll find that often in statistics we model data as if it comes from a probability distribution when we know it didn’t. We simply know that the probability distribution is a useful model for thinking about the problem.\nAs for getting the probabilities from \\([0,1]\\) to \\((-\\infty, \\infty)\\), we need a function, preferably a monotonic one. The generally agreed upon choice is the logit (natural log of the odds) function. The logit function of a probability is defined as\n\\[\n\\eta = \\mathrm{logit}(p) = \\log\\{p / (1 - p)\\}\n\\]\nwhere \\(p\\) is the probability and \\(O = p/(1-p)\\) is called the odds. Note, you can go backwards from odds to probability with the function \\(p = O / (1 + O)\\). Odds are exactly as used in gambling. If the odds of bet at 1 to 99, then you are saying the probability is \\(1 / (99 + 1) = 1\\%\\).\nWhy use odds? There’s a couple of reasons why odds are uniquely interprettable. First, there are specific study designs where odds make more sense than probabilities, particularly retrospective ones. Secondly, odds are unique in binomial models where they work out to be particularly tractible to work with. Finally, odds have a unique gambling interpretation. That is, it gives the ratio of a one dollar risk to the return in a fair bet. (A fair bet is where the expected return is 0.) So, when a horse track gives the odds on a horse to be 99 to 1, they are saying that you would get $99 dollars if you bet one dollar and the horse won. This is an implied probability of 99 / (99 + 1) = 99% that the horse loses and 1% probability that the horse wins. Note they don’t usually express it as a fraction, they usually espress it as value to 1 or 1 to value. So they would say 99 to 1 (odds against) or 1 to 99 (odds for) so you can easily see how much you’d win for a dollar bet.\nYou can go backwards from the logit function to the probability with the expit function. That is, if \\(\\eta\\) is defined as above, then\n\\[\np = \\frac{e^{\\eta}}{1 + e^\\eta} = \\frac{1}{1 + e^{-\\eta}}.\n\\]\nThis is sometimes called the expit function or sigmoid.\nWe model the log of the odds as linear. This is called logistic regression.\n\\[\n\\eta = \\mathrm{logit}\\left\\{ P(Y = 1 ~|~ X) \\right\\}\n= \\beta_0 + \\beta_1 X.\n\\]\nThe nice part about this model is that \\(e^\\beta_1\\) has the nice interpretation of the odds ratio associated with a one unit change in \\(X\\).\nThis is great, but we still need a function of the probabilities to optimize. We’ll use the cross entropy.\n\\[\n-\\sum_{i=1}^n \\left[Y_i \\log\\{P(Y_i = 1 ~|~ X_i)\\} + (1 - Y_i) \\log\\{1 - P(Y_i = 1 ~|~ X_i)\\}\\right].\n\\]\nThis function has the interpretation of being the negative of the log of the probabilities assuming the \\(Y_i\\) are independent. This model doesn’t have to hold for the minimization to be useful.\nPlugging our logit model in, the cross entropy now looks like\n\\[\n-\\sum_{i=1}^n \\left[\n  Y_i \\eta_i + \\log\\left\\{\\frac{1}{1 + e^\\eta_i} \\right\\} \\right].\n\\]\nThis is the function that we minimize to perform logistic regression. Later on, we’ll worry about how to minimize this function. However, today, let’s fit logistic regression to some data.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model as lm\n## this sets some style parameters\nsns.set()\n\n## Read in the data and display a few rows\ndat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv\")\n\n\n## Plot the data\nsns.scatterplot('FLAIR', 'GOLD_Lesions', data = dat)\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/seaborn/_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n<AxesSubplot:xlabel='FLAIR', ylabel='GOLD_Lesions'>\n\n\n\n\n\nLet’s now fit the model. Again we’re going to split into training and test data. But, now we’re not going to do it manually since we have to load a library that has a function to do this.\n\nx = dat[['FLAIR']]\ny = dat.GOLD_Lesions\ntrainFraction = .75\n\n## Once again hold out some data\nsample = np.random.uniform(size = 100) < trainFraction\nxtrain = x[ sample]\nytrain = y[ sample]\nxtest =  x[~sample]\nytest =  y[~sample]\n\n\nlr = lm.LogisticRegression(fit_intercept=True, penalty='none')\nfit = lr.fit(xtrain, ytrain)\n\nLet’s look at the parameters fit from the model\n\nbeta0, beta1 = [fit.intercept_[0], fit.coef_[0][0]]\n[beta0, beta1]\n\n[-3.7873195868013765, 2.3209126883453295]\n\n\n\nn = 1000\nxplot = np.linspace(-1, 5, n)\neta = beta0 + beta1 * xplot\np = 1 / (1 + np.exp(-eta))\n\nsns.scatterplot('FLAIR', 'GOLD_Lesions', data = dat, hue = 'GOLD_Lesions')\nsns.lineplot(xplot, p)\n\n## Of course, scikit has a predict\n## function so that you don't have to do this manually\n#yplot = fit.predict_proba(xplot.reshape((n, 1)))\n#sns.lineplot(xplot, yplot[:, 1])\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/seaborn/_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/seaborn/_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n<AxesSubplot:xlabel='FLAIR', ylabel='GOLD_Lesions'>\n\n\n\n\n\nNow let’s evaluate the test set.\n\n## This predicts the classes using a 50% probability cutoff\nyhat_test = fit.predict(xtest)\n\n## double checking that if you want\n#all(yhat_test == (fit.predict_proba(xtest)[:, 1] > .5))\n\naccuracy = np.mean(yhat_test == ytest)\nsensitivity = np.mean(yhat_test[ytest == 1] == ytest[ytest == 1])\nspecificity = np.mean(yhat_test[ytest == 0] == ytest[ytest == 0])\nnp.round([accuracy, sensitivity, specificity], 3)\n\narray([0.679, 0.6  , 0.769])\n\n\n\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\n\nptest = fit.predict_proba(xtest)[:, 1]\nfpr, tpr, thresholds = roc_curve(ytest, ptest)\nroc_auc = auc(fpr, tpr)\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "statistics_binary_classification.html",
    "href": "statistics_binary_classification.html",
    "title": "21  Introduction to binary classification",
    "section": "",
    "text": "The data we’re going to be working with for this example is the oasis data from Elizabeth Sweeney’s R package. The data contain MR (magnetic resonance) images for lesion segmentation in multiple sclerosis (MS). MS is a disorder primarily caused by whtie matter lesions. This dataset is a collection of voxels from an image with radiologists labeling of whether or not a white matter lesion exists at that location.\nNote that we loaded pandas first. The various columns are voxel values from different kinds of MR images of the same patient. FLAIR (fluid attenuated inversion recovery), PD (proton density), T1 and T2. The latter two aren’t acronyms, but instead named for the specific part of the relaxation time of the MR signal. Roughly, the relaxation time is related to the signal produced by protons snapping back into alignment in a strong magnetic field (recall magnetic resonance imaging). The _10 and _20 ending variables are local averages of the neighboring voxels. We’re trying to predict GOLD_Lesions, which is the radiologist standard of whether or not there is a lesion at this voxel. (A voxel is a three dimensional pixel.)\nNote here we are doing voxelwise segmentation that is trying to predict whether there is a lesion at each specific voxel. This can be viewed as an image processing problem. Other classification problem consider, for example, whether a patient has any lesions (and then where as a followup). Approaching the problem that way is a image level segmentation approach.\nLet’s plot it. I’m showing a couple of ways. I’ve been testing out plotting libraries in python, and I think that I like ‘seaborn’ (the second plot) the best. In the seaborn plots, I show both the marginal plot (without considering the gold standard) and then stratified by whether or not there was a lesion at that voxel."
  },
  {
    "objectID": "statistics_binary_classification.html#definitions",
    "href": "statistics_binary_classification.html#definitions",
    "title": "21  Introduction to binary classification",
    "section": "22.1 Definitions",
    "text": "22.1 Definitions\ntest set accuracy = proportion of correct classifications on the test data\ntest set sensitivity = proportion declared diseased among those that are actually diseased. (In this case lesion = disease)\ntest set specificity = proportion declared not diseased among those that are actually not diseased.\nTo interpret the sensitivity and specificity, imagine setting the threshold nearly to zero. Then we’ll declare almost every voxel a lesion and we’ll have nearly 100% sensitivity and nearly 0% specificity. If we declare a voxel as a lesion it’s not that interesting. If we declare a voxel as not lesions, then it’s probably not a lesion.\nIf we set the threshold really high, then we’ll have nearly 0% sensitivity and 100% specificity. If we say a voxel is not lesioned, it’s not that informative, since we declare nearly everything not a lesion. But if we declare a voxel a lesion, it usually is.\nSo, if you have a high sensitivity, it’s good for ruling diseases out. If you have a high specificity it’s good for ruling diseases in. If you have a high both? Then you have a very good test.\n\n## Let's test it out on the test set\ntestPredictions = (xtest > threshold)\n\n## The test set accuracy\ntestAccuracy = np.mean(testPredictions == ytest)\n\n## Let's see how it specifically does on the\n## set of instances where ytest == 0 and ytest == 1\n## The % it gets correct on ytest == 0 is called\n## the specificity and the percent correct when \n## ytest == 1 is called the sensitivity.\nsub0 = ytest == 0\nsub1 = ytest == 1\n\ntestSpec = np.mean(ytest[sub0] == testPredictions[sub0])\ntestSens = np.mean(ytest[sub1] == testPredictions[sub1])\n\npd.DataFrame({\n 'Threshold': threshold,\n 'Accuracy': testAccuracy, \n 'Specificity': testSpec, \n 'Sensitivity': testSens}, index = [0])\n\n\n\n\n\n  \n    \n      \n      Threshold\n      Accuracy\n      Specificity\n      Sensitivity\n    \n  \n  \n    \n      0\n      1.419322\n      0.730769\n      0.6\n      0.8125\n    \n  \n\n\n\n\n\nsns.kdeplot(x0, shade = True, label = 'Gold Std = 0')\nsns.kdeplot(x1, shade = True, label = 'Gold Std = 1')\nplt.axvline(x=threshold)\n            \nplt.show()\n\n\n\n\nOK, so out plot has better sensitivity than specificity and a test set accuracy of around 68%. The lower specificity is because there’s a lower percentage of blue below the line than orange above the line. Recall, we’re saying above the threshold is a lesion and orange is the distribution for voxels with lesions.\nSo, for this algorithm, the high sensitivity says that all else being equal, if you declare a voxel as not being a lesion, it probably isn’t. In other words, if you’re out in the lower part of the orange distribution, there’s a lot of blue there.\nHowever, all else isn’t equal. Most voxels aren’t lesions. This factors into our discussion in a way that we’ll discuss later.\n\nfpr, tpr, thresholds = roc_curve(ytest, xtest)\nroc_auc = auc(fpr, tpr)\n\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "tooling_numpy.html",
    "href": "tooling_numpy.html",
    "title": "20  Numpy",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndat = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n## Get Italy, drop everyrthing except dates, convert to long (unstack converts to tuple)\nItaly = dat[dat['Country/Region'] == 'Italy'].drop([\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"], axis=1).unstack()\n\nLet’s create a numpy array from the counts and work with it a bit. First we’ll take our data from Italy and convert it from the cumulative case counts to the daily case counts.\n\n## convert from tuple to array\nX = np.asarray(Italy)  \n## get case counts instead of cumulative counts\nX = X[1 : X.size] - X[0 : (X.size - 1)]\n\nLet’s get some basic statistical summaries. Note the default is that the standard deviations uses the formula\n\\[\n\\sqrt{\\frac{1}{N} \\sum_{i=1}^N (X_i - \\bar X)^2}\n\\]\nrather than\n\\[\n\\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (X_i - \\bar X)^2}.\n\\]\nTo get the latter (the unbiased version), set ddof=1. Personally, I prefer \\(N\\) as a divisor, though that’s a minority opinion. (Between bias or variance of the standard deviation estimate, I’d rather rather have lower variance.) To described the code below:\n\nX.mean() gives the mean; since X is a numpy object, it has statistical methods defined in the class\nX.std() gives the (biased version of the) standard deviation\nnp.round(A, 2) rounds the numpy object to two decimal places (useful for printing)\nstr is the python string converter (just to print)\n\n\nprint(\"Mean         : \" + str(np.round(X.mean(), 2))) \nprint(\"Std (biased) : \" + str(np.round(X.std() , 2)))\n\nMean         : 22614.37\nStd (biased) : 37008.8\n\n\nNumpy has a linear algebra library. Let’s calculate a distributed lag model using numpy (typically you would use this with regression software). A distributed lag model is of the form:\n\\[\nY_t = \\alpha + \\sum_{i=1}^p \\beta_i Y_{t-i} + \\epsilon_i\n\\]\nFirst, let’s create the lagged matrix considering 3 lags.\n\n## Create a matrix of three lagged versions\nX = np.array([ Italy.shift(1), Italy.shift(2), Italy.shift(3)]).transpose()\n## Add a vector of ones\nitc = np.ones( (X.shape[0], 1) )\nX = np.concatenate( (itc, X), axis = 1)\n\n## Visualize the results\nX[0 : 10,:]\n\narray([[ 1., nan, nan, nan],\n       [ 1.,  0., nan, nan],\n       [ 1.,  0.,  0., nan],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.]])\n\n\nLet’s get rid of the three NA rows.\n\nX = X[ 3 : X.shape[0], :]\nnp.any(np.isnan(X))\n\nFalse\n\n\n\n## Create the Y vector\nY = np.array(Italy[ 3 : Italy.shape[0]])\n[Y.shape, X.shape]\n\n[(1129,), (1129, 4)]\n\n\nThe matrix formula for minimizing the least squares regression model,\n\\[\n|| Y - X \\beta||^2\n\\]\nis given by\n\\[\n\\hat \\beta = (X' X)^{-1} X' Y\n\\]\nLet’s do this in numpy. Let’s find the estimated regression coefficients using the formula above. We’ll use the following functions\n\nmatmul(A,B) is the matrix multiplication of A and B\nA.T is the transpose of A, labeled above as \\(A'\\)\ninv(A) is the matrix inverse of A, labeled above as \\(A^{-1}\\)\n\n\nnp.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T), Y)\n\narray([ 2.80792789e+03,  1.43245988e+00, -6.94534587e-02, -3.62794472e-01])\n\n\nOf course, we don’t tend to do things this way. If needed, we’d use lstsq.\n\nnp.linalg.lstsq(X, Y, rcond = None)[0]\n\narray([ 2.80792691e+03,  1.43245988e+00, -6.94534582e-02, -3.62794472e-01])\n\n\nTypically, we wouldn’t do any of this for this problem, since high level regression models exist already. For example, sklearn’s linear_model module)\n\nfrom sklearn import linear_model\n\nmodel = linear_model.LinearRegression(fit_intercept = False)\nfit = model.fit(X, Y)\nfit.coef_\n\narray([ 2.80792691e+03,  1.43245988e+00, -6.94534582e-02, -3.62794472e-01])"
  },
  {
    "objectID": "statistics_causal.html",
    "href": "statistics_causal.html",
    "title": "24  Causal DAGs",
    "section": "",
    "text": "Causal models differ from associational models in that they codify causal directions not just associations. In our program, you might have learned of the use of propensity scores, counterfactuals or randomization to study causality. There, typically the goal is to make causal statements with as few assumptions as possible or at least understanding the assumptions. Typically, the object of study is the estimation of an effect avergated over covarites.\nCausal graphs take a different approach, even if they wind up at the same place. Here, the goal is to postulate hyptothetical causal relationships and use those hypothetical relationships to estimate causal effects.\n\n\nA graph, \\(G\\) is a collection of nodes, say \\(V=\\{1,\\ldots, p\\}\\) and a set edges between the nodes, i.e. a set of elements \\((i,j)\\). The graph is directed if \\((i,j)\\) is considered different then \\((j,i)\\).\nNode \\(i\\) is a parent of node \\(j\\) if \\((i,j) \\in E\\) and \\((j,i)\\notin E\\). Similarly, node \\(i\\) is a child of node \\(j\\) if \\((j,i) \\in E\\) and \\((i,j)\\notin E\\). A node is a descendant of another if it is a child, or a child of a child and so on.\n\n\n\nDAGs define a unique factorization (set of independence relationships) with compatible probability models. I find it useful to think of causal DAGs in the terms of structural causal models (SCMs). Such models demonstrate an example of a generative models that statisfy the DAG and the have clear connections with the probabability models. An SCM over a collection of variables, \\(X=(X_1, \\ldots, X_p)\\), postulates a set of functional relationships \\[\nX_j = f(P_j, \\epsilon_j)\n\\] where \\(P_j\\) are the antecedent causes of \\(X_j\\), called the parents of \\(X_j\\), and \\(\\epsilon_j\\) is an accumulation of variables treated as mutally independent. This defines a directed graph, \\(G\\) say, where a graph is collection of vertices corresponding to our variables, \\(V=\\{1,\\ldots, p\\}\\), corresponding to the \\(X_i\\), and edges, \\(E\\), which is a set of ordered pairs of nodes.\n\n\n\n\n\nIn this case, \\(P_1 = \\{\\}\\), \\(P_2 = \\{1\\}\\) and \\(P_3 = \\{1,2\\}\\). DAGs in general define the independence assumptions associated with compatible probability models. SCMs are such an example that clearly define compatible probability models. Of course, given a large enough cross-sectional sample, we can estimate the joint distribution of \\(P(X_1,\\ldots, X_p)\\) and all of its conditionals. Disregarding statistical variation, which can be accounted for using traditional inferential methods, these conditionals should agree with the independence relationships from the DAG, if the DAG is correct. This yields a fruitful way to consider probability models. For example one could use DAGs as a heuristic and see how the observed data agrees with the independence relationships in compatible probability models implied by the DAG.\nBy itself, this does not create any causal claims. However, the following strategy could. Postulate a causal model, like the SCM, consider the independence relationships implied by the SCM, compares those indepnence relationships with those seen in the observed data. This gives us a method to falsify causal models using the data.\nOne specific way in which we use the assumptions is to investigate how the graph changes when we fix a node at a specific value, like an intevention, thus breaking its association with its parents. This operation is conceptual, but at times we can relate probabilities associated with interventions that were not realized. Consider an instrance where where \\(X_1\\) is a collection of confounders, \\(X_2\\) is an exposure and \\(X_3\\) is an outcome. Ideally, we’d like to know \\[\nP(X_3 ~|~ do(X_2) = x_2)\n\\] That is, the impact on the response if we were to set the exposure to \\(e_0\\).\n\n\n\nBefore we talk about interventions, let’s consider discussing compatibility of the hypothetical directed graph and our observed data. Return to our previous diagram.\n\n\\(X1\\) is a confounder betweend \\(X2\\) and \\(X3\\)\n\\(X2\\) is a mediator between \\(X1\\) and \\(X3\\)\n\\(X3\\) is a collider between \\(X1\\) and \\(X2\\)\n\nConsider an example. \\(X1\\) is having a BMI > 35, \\(X2\\) is sleep disordered breathing and \\(X3\\) is hypertension.\n\n\n\n\n\nHere if we’re studying whether SDB causes HTN, BMI35 confounds the relationship as a possible common cause of both. We would need to adjust for BMI35 to make sure the association between SDB and HTN isn’t just due to this common cause.\nIf we were studying whether BMI35 causes HTN, we might be interested in how much of that effect is mediated (indirectly) through SDB and how much is directly from BMI35.\nIf we are studying the relationship between BMI35 and SDB directly, adjusting for HTN may cause an association. Consider the (fictitious) case where there is a large number of people who have SDB who are not obese, yet all have hypertension, for whatever the reason. Then, among the HTN, there could be a negative association between BMI35 and SDB, because of the large collection of patients would who have SDB and are not obese and the same for obese and not hyptertensive. That is, adjusting for HTN created an association. This is an example of Berkson’s paradox. This is a somewhat contrived example, but hopefully you get the point. The wikipedia article has a funny example where they consider \\(X_1\\) is whether or not the hamburger is good at a fast food restaurant, \\(X_2\\) is whether or not the fries are good and \\(X_3\\) is whether or not people eat there. Since few people would eat at a place where both the hamburger and fries are bad, conditioning on \\(X_3\\) can create a negative association.\nThe main point here is that adjusting for colliders may open up a pathway between the nodes.\nA path between two nodes \\(n_1\\) and \\(n_k\\) is a sequence of nodes, \\(v_1, v_2,\\ldots v_{k}\\), where \\(v_{i}\\) and \\(v_{i+1}\\) are connected. The path is directed if \\(v_{i}\\) points to \\(v_{i+1}\\) for \\(i=1,\\ldots,k\\). A graph is a Directed Acyclic Graph (DAG) if all edges are directed and there are no two nodes \\(v_i\\) and \\(v_j\\) with a directed path in both directions.\nA path between \\(v_1\\) and \\(v_k\\), \\(v_1,\\ldots, v_k\\), is blocked by a set of nodes, \\(S\\), if for some \\(v_j\\) in \\(S\\)\n\n\\(v_j\\in S\\) and \\(v_k\\) is a mediator or confounder between \\(v_{j-1}\\) and \\(v_{j+1}\\) in either direction or\n\\(v_j\\notin S\\) and all of the descendants of \\(v_j \\notin S\\) and \\(v_{j}\\) is a collider between \\(v_{j-1}\\) and \\(v_{j+1}\\).\n\nIn other words, a path is blocked if a mediator or confounder is included in \\(S\\) or a collider and all of its descendants is excluded from \\(S\\).\nFor 1. this is equivalent to saying one of\n\n\\(v_{j-1}\\rightarrow v_{j} \\rightarrow v_{j+1}\\)\n\\(v_{j-1}\\leftarrow v_{j} \\leftarrow v_{j+1}\\)\n\\(v_{j-1}\\leftarrow v_{j} \\rightarrow v_{j+1}\\)\n\nholds. For 2. recall a collider is \\(v_{j-1}\\rightarrow v_{j} \\leftarrow v_{j+1}\\).\nThis could be translated into the following statistical statement. Conditioning on a mediator or confounder or not conditioning on a collider blocks a path, conditioning on a collider opens a path.\nWe say that two nodes or groups of nodes are d-separated by a set of nodes, \\(S\\), if every path between nodes in the two groups is blocked by \\(S\\). d-separation is useful because it gives us conditional independence relationships in the sense that if \\(X_i\\) is d-separated with \\(X_j\\) given \\(S\\) then \\(X_i \\perp X_j ~|~ S\\) on all probability distribution compatible with the graph.\nConsider the following graph.\n\n\n\n\n\n\\(X_1\\) and \\(X_5\\) are conditionally indepndent given \\(X_2\\) and \\(X_3\\). Why? Conditioning on \\(X_2\\) blocks the paths \\(X_1 \\leftarrow X_2 \\rightarrow X_3 \\leftarrow X_5\\) even despite the part \\(X_2 \\rightarrow X_3 \\leftarrow X_5\\) is opened by conditioning on the collider, \\(X_3\\). Furthermore, conditioning on \\(X_2\\) or \\(X_3\\) blocks the path \\(X_1 \\leftarrow X_2 \\rightarrow X_3 \\rightarrow X_4 \\rightarrow X_6 \\leftarrow X_5\\). Finally, conditioning on \\(X_3\\) blocks the path \\(X_1 \\leftarrow X_3 \\leftarrow X_5\\).\nAnother interesting one is that \\(X_2\\) and \\(X_5\\) are marginally independent. This is because not conditioining on \\(X_3\\) blocks the path \\(X_2 \\rightarrow X_3 \\leftarrow X_5\\) and not conditioning on \\(X_6\\) blocks the path \\(X_2 \\rightarrow X_3 \\rightarrow X_4 \\rightarrow X_6 \\leftarrow X_5\\).\nHere’s the complete set of conditional independence relationships.\n\n\\(X_1\\) and \\(X_4\\) are d-separated by \\(\\{X_3\\}\\)\n\\(X_1\\) and \\(X_5\\) are d-separated by \\(\\{X_2, X_3\\}\\)\n\\(X_1\\) and \\(X_6\\) are d-separated by \\(\\{X_4, X_5\\}\\), \\(\\{X_3, X_5\\}\\), \\(\\{X_2, X_3\\}\\)\n\\(X_2\\) and \\(X_4\\) are d-separated by \\(\\{X_3\\}\\)\n\\(X_2\\) and \\(X_5\\) are d-separated by \\(\\{\\}\\) (the null set, i.e. they’re marginally independent).\n\\(X_2\\) and \\(X_6\\) are d-separated by \\(\\{X_4, X_5\\}\\), \\(\\{X_3, X_5\\}\\)\n\\(X_3\\) and \\(X_6\\) are d-separated by \\(\\{X_4, X_5\\}\\)\n\\(X_4\\) and \\(X_5\\) are d-separated by \\(X_3\\).\n\nThese all imply the independence relationships, such as \\(X_1 \\perp X_4 ~|~ X_3\\)."
  },
  {
    "objectID": "statistics_causal.html#do-calculus-and-backdoor-criterion",
    "href": "statistics_causal.html#do-calculus-and-backdoor-criterion",
    "title": "24  Causal DAGs",
    "section": "24.2 Do calculus and backdoor criterion",
    "text": "24.2 Do calculus and backdoor criterion\nRecall that specifying a causal graph implies the independence relationships of a probability distribution under assumptions such as the SCM. Sometimes, we’re interested in the causal relationship between an exposure, \\(X\\) and an outcome, \\(Y\\). Consider a theoretical intervention obtained by setting \\(X = a\\), which we write as \\(do(X) = a\\). We want to estimate \\(P(Y ~|~ do(X) = a)\\).\nA set \\(Z\\) satisfies the back door criterion with respect to nodes \\(X\\) and \\(Y\\) if\n\nNo descendant of \\(X\\) is in \\(Z\\).\n\\(Z\\) blocks every path \\(X\\) and \\(Y\\) that contains an arrow pointing to \\(X\\).\n\nThe back door criteria is similar to d-separation. However, we only focus on arrows pointing into \\(X\\) and don’t allow for descendants of \\(X\\).\nThe magic of the back door adjustment comes from the relationship, the adjustment formula:\n\\[\nP(Y ~|~ do(X) = x) = \\sum_{z\\in S} P(y ~ | x, z) p(z)\n\\]\nwhere \\(S\\) satisfies the back door criterion. If the \\(z\\) are all observed variables, then the RHS of this equation is estimable. Note the interesting statement that not all variables need to be observed, just \\(y\\), \\(x\\) and \\(z\\).\nSo, in our previous example, adjusting for \\(S = \\{X_2, X_3\\}\\) allows us to estimate the causal effect of \\(X\\) on \\(Y\\), even if \\(X_4\\) and \\(X_5\\) are not measured.\nIt’s important to emphasize, that every aspect of the adjustment formula is theoretically estimable if \\(Y\\), \\(X\\) and the nodes in \\(S\\) are observed.\nConsider the following graph.\n\n\n\n\n\nHere are the minimal backdoor adjustment variables between \\(X\\) and \\(Y\\):\n\n\\(S = \\{X_2, X_3\\}\\)\n\\(S = \\{X_3, X_5\\}\\)\n\\(S = \\{X_4, X_5\\}\\)\n\nHere are some invalid backdoor sets of variables.\n\n\\(S\\) equal to any single node.\n\n\\(S=\\{X_3\\}\\) does not block the path \\(X\\leftarrow X_2 \\rightarrow X_3 \\leftarrow X_5 \\rightarrow Y\\).\n\\(S=\\{X_4\\}\\) or \\(S=\\{X_2\\}\\) does not block the path \\(X \\leftarrow X_3 \\leftarrow X_5 \\rightarrow Y\\).\n\\(S=\\{X_5\\}\\) does not block the path \\(X \\leftarrow X_3 \\leftarrow X_4 \\rightarrow Y\\).\n\n\\(S=\\{X_3, X_4\\}\\) does not block the path \\(X \\leftarrow X_2 \\rightarrow X_3 \\leftarrow X_5 \\rightarrow Y\\).\n\\(S=\\{X_2, X_4\\}\\) does not block the path \\(X\\leftarrow X_3 \\leftarrow X_5 \\rightarrow Y\\).\n\n\n24.2.1 Example graphs\nIn all the following, we’re interested in the causal effect of \\(X\\) and \\(Y\\). \\(Z\\) variables are observed and \\(U\\) variables are unobserved. Every variable is binary to make the discussion easier.\n\n24.2.1.1 Randomization\nIf \\(X\\) is randomized and everyone takes the treatment assigned to them (left plot) then \\(X\\) has no parents other than the randomization mechanism,\\(R\\). We’re omitting any descendants of \\(X\\) since we don’t have to worray about them. Regardless of the complexity of the relationship between the collection of observed, unobserved, known and unknown variables, \\(Z, U\\), and \\(Y\\) we can estimate the causal effect simply without conditioning on anything.\nIn contrast, if some people ignore their randomized treatment status and elect to choose a different treatment one may have opened a backdoor path (right plot). For example, if the treatment can’t be blinded and those randomized to the control with the worst baseline symptoms elect to obtain the treatment elsewhere.\n\n\n\n\n\n\n\n24.2.1.2 Simple confounding\nThe diagram below shows classic confounding. Conditioning in \\(Z\\) allows for the estimation of the causal effect.\n\n\n\n\n\nNow the estimate of the adjusted effect (under our assumptions) is\n\\[\nP(Y ~|~ do(X) = x) = P(Y ~|~ X=x, z = 0)P(z = 0) + P(Y ~|~ X=x, Z=1)P(Z=1)\n\\]\nIn the following two examples, the unmeasured confounder \\(U\\) can be controlled for by conditioning on \\(Z\\) and exactly the same estimate can be used as in the simple confounding model.\n\n\n(-0.3, 1.3)\n\n\n\n\n\n\n\n24.2.1.3 Mediation\nIn mediation, all or part of the effect of \\(X\\) on \\(Y\\) flows through yet another variable \\(Z\\).\n\n\n\n\n\nThe backdoor criteria does not apply here, since \\(Z\\) is a descendant of \\(X\\). To answer the question “What is the causal effect of \\(X\\) on \\(Y\\)?” one need not adjust. However, mediation is typically studied in a different way. Instead, one asks questions such as “How much of the effect of \\(X\\) on \\(Y\\) flows or doesn’t flow through \\(Z\\)?”. To answer this question, one usually conditions on \\(Z\\) for a different goal than the backdoor adjustment is accomplishing.\n(cinelli2021crash?) shows an interesting example of mediation where one would want to adjust for \\(Z\\) (left plot below).\n\n\n(-0.3, 1.3)\n\n\n\n\n\nIn this case, \\(M\\) still mediates the relationship between \\(X\\) and \\(Y\\). However, \\(Z\\) is in a backdoor path to \\(M\\). So, some of the variation in \\(M\\) that impacts \\(Y\\) could be due to \\(Z\\) rather than \\(X\\). The right plot is similar and makes the point more explicit. \\(Z\\) confounds the relationship between \\(X\\) and \\(Y\\) through \\(M\\). Without adjusting for \\(Z\\), the path \\(X\\leftarrow Z \\rightarrow M \\rightarrow Y\\) remains unblocked.\n\n\n24.2.1.4 Bad controls\nThe following are all unhelpful for conditioning on \\(Z\\) using the backdoor criteria.\nUpper left. Adjusting for colliders is the standard bad control. Below adjusting for \\(Z\\) open ups a backdoor path that was closed. From a common sense perspective, why would you want to adjust for a consequence of \\(X\\) and \\(Y\\) when exploring their relationship?\nIn the upper right diagram below, \\(Z\\) is a so-called instrumental variable. A good example is \\(Z\\) being the randomization indicator and \\(X\\) being the treatment the person actually took. It is important in this example to emphasize that use of the instrumental variable is often a very fruitful method of analysis. However, it’s not a useful backdoor adjustment and conditioning on \\(Z\\) simply removes most of the relevant variation in \\(X\\). If one wants to use \\(Z\\) as an instrumental variable in this setting, then specific methods taylored to instrumental variable use need to be employed.\nIn the lower left plot, \\(Z\\) is a descendant of \\(X\\). Conditioning on \\(Z\\) removes relevant pathway information regarding the relationship between \\(X\\) and \\(Y\\)>\nThe lower right plot is similar. Conditioning on \\(Z\\) removes variation in \\(M\\) which hinders our ability to study the relationship between \\(X\\) and \\(Y\\) through \\(M\\).\n\n\n\n\n\n\n\n24.2.1.5 Conditioning may help\nIn the upper left plot, adjusting for \\(Z\\) may reduce variability in \\(Y\\) to help focus on the relationship between \\(X\\) and \\(Y\\).\nIn the upper left plot, adjusting for \\(Z\\) may reduce variation in the mediator unrelated to the relationship between \\(X\\) and \\(Y\\).\n\n\n(-0.3, 1.3)"
  },
  {
    "objectID": "statistics_causal.html#exercises",
    "href": "statistics_causal.html#exercises",
    "title": "24  Causal DAGs",
    "section": "24.3 Exercises",
    "text": "24.3 Exercises\n\n24.3.1 Graphs\nConsider the following graph where we want to answer the question: what is \\(P(Y ~|~ do(X) = x)\\) where every variable is binary.\n\n\n(-0.3, 1.3)\n\n\n\n\n\n\nWhat are the minimal set of adjustment variables for the backdoor criteria?\nIs \\(X \\perp Y ~|~ Z_1, Z_2\\)?\nIs \\(X \\perp Y ~|~ Z_2, Z_3\\)?\nGiven a cross sectional sample, if \\(Z_3\\) is unobserved, give a formula for the estimation of \\(P(Y ~|~ do(X) = x)\\) that only requires observed variables.\n\n\n\n\n\n\n\nWhat are the minimal set of adjustment variables for the backdoor criteria?\nGiven a cross sectional sample, give a formula for the estimation of \\(P(Y ~|~ do(X) = x)\\) that only requires observed variables.\n\n\n\n24.3.2 Data exercise\nThe wikipedia page on Simpson’s paradox gives this data concerning two treatments of kidney stones, the percentage of succcessful procedures and the size of the stone. Note, among both large stones and small stones A is better than B. However, among all B is preferable to A.\n\n\n\nSize\nTreatment\nSuccess\nN\nProp\n\n\n\n\nLarge\nA\n192\n263\n73%\n\n\n\nB\n55\n80\n69%\n\n\nSmall\nA\n81\n87\n93%\n\n\n\nB\n234\n270\n87%\n\n\nBoth\nA\n273\n350\n78%\n\n\n\nB\n289\n350\n83%\n\n\n\nEstimate the treatment effect difference: \\[\nP(Success ~|~ Do(Treatment) = B)\n- P(Success ~|~ Do(Treatment) = A)\n\\] under the following graphical models where \\(X\\) is treatment, \\(Y\\) is success and \\(Z\\) is stone size:\n\n\n\n\n\nComment on how reasonable each of these models are given the setting. Here’s a reference: (julious1994confounding?).\nGive any other DAGs, perhaps including unmeasured variables, that you think are relevant."
  },
  {
    "objectID": "statistics_causal.html#reading",
    "href": "statistics_causal.html#reading",
    "title": "24  Causal DAGs",
    "section": "24.4 Reading",
    "text": "24.4 Reading\n\nThe definitive causal reference is (pearl2009causality?).\nI got a lot of this stuff from (peters2017elements?), which you can read here\nAlso read (hardt2021patterns?), which you can read here\nA crash course in good and bad controls, or here\ndagitty"
  },
  {
    "objectID": "theory_measurement.html",
    "href": "theory_measurement.html",
    "title": "31  Measurement",
    "section": "",
    "text": "Repeatability or test/retest reliabity is the agreement of measurements across technical replications.\n\n\nThe Bland/Altman (bland1999measuring?) or Tukey mean/difference plot (tukey1977exploratory?) is a plot of agreement between two measured quantities. Here I use the mricloudpy package to read in data and convert it to a dataframe. Here, we have two measures of the same data. B/A plots typically add a 1.96 sd bar to detect outlying differences.\n\nimport statsmodels.api as sm\nimport numpy as np\n\nimport sys \nimport os\nsys.path.append(\"/home/bcaffo/sandboxes/MRICloudPy/mricloudpy/\")\n\nfrom mricloudpy.mricloudpy import Data\nexample = \"/home/bcaffo/sandboxes/MRICloudPy/mricloudpy/sample_data/\"\nd = Data(example)\n\nimport_data: Data files found \n['/home/bcaffo/sandboxes/MRICloudPy/mricloudpy/sample_data/kirby127a_3_1_ax_283Labels_M2_corrected_stats.txt', '/home/bcaffo/sandboxes/MRICloudPy/mricloudpy/sample_data/kirby142a_3_1_ax_283Labels_M2_corrected_stats.txt', '/home/bcaffo/sandboxes/MRICloudPy/mricloudpy/sample_data/kirby239a_3_1_ax_283Labels_M2_corrected_stats.txt', '/home/bcaffo/sandboxes/MRICloudPy/mricloudpy/sample_data/kirby346a_3_1_ax_283Labels_M2_corrected_stats.txt', '/home/bcaffo/sandboxes/MRICloudPy/mricloudpy/sample_data/kirby422a_3_1_ax_283Labels_M2_corrected_stats.txt', '/home/bcaffo/sandboxes/MRICloudPy/mricloudpy/sample_data/kirby492a_3_1_ax_283Labels_M2_corrected_stats.txt', '/home/bcaffo/sandboxes/MRICloudPy/mricloudpy/sample_data/kirby501a_3_1_ax_283Labels_M2_corrected_stats.txt', '/home/bcaffo/sandboxes/MRICloudPy/mricloudpy/sample_data/kirby505a_3_1_ax_283Labels_M2_corrected_stats.txt', '/home/bcaffo/sandboxes/MRICloudPy/mricloudpy/sample_data/kirby656a_3_1_ax_283Labels_M2_corrected_stats.txt', '/home/bcaffo/sandboxes/MRICloudPy/mricloudpy/sample_data/kirby679a_3_1_ax_283Labels_M2_corrected_stats.txt', '/home/bcaffo/sandboxes/MRICloudPy/mricloudpy/sample_data/kirby742a_3_1_ax_283Labels_M2_corrected_stats.txt', '/home/bcaffo/sandboxes/MRICloudPy/mricloudpy/sample_data/kirby800a_3_1_ax_283Labels_M2_corrected_stats.txt', '/home/bcaffo/sandboxes/MRICloudPy/mricloudpy/sample_data/kirby814a_3_1_ax_283Labels_M2_corrected_stats.txt']\nimport_data: Importing...\n\n\n\nvisit1 = Data(\"mricloud/visit1/\").df\nvisit2 = Data(\"mricloud/visit2/\").df\n\nimport_data: Data files found \n['mricloud/visit1/142_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit1/239_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit1/346_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit1/422_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit1/492_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit1/501_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit1/505_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit1/656_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit1/679_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit1/742_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit1/800_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit1/814_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit1/815_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit1/849_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit1/906_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit1/913_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit1/916_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit1/934_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit1/959_kirby_3_1_ax_283Labels_M2_corrected_stats.txt']\nimport_data: Importing...\n\n\nimport_data: Data files found \n['mricloud/visit2/142_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit2/239_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit2/346_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit2/422_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit2/492_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit2/501_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit2/505_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit2/656_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit2/679_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit2/742_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit2/800_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit2/814_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit2/815_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit2/849_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit2/906_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit2/913_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit2/916_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit2/934_kirby_3_1_ax_283Labels_M2_corrected_stats.txt', 'mricloud/visit2/959_kirby_3_1_ax_283Labels_M2_corrected_stats.txt']\nimport_data: Importing...\n\n\n\nimport statsmodels.api as sm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ncsf1 = visit1[(visit1['Type'] == 1) & (visit1['Level'] == 1) & (visit1['Object'] == 'CSF')]['Volume']\ncsf2 = visit2[(visit2['Type'] == 1) & (visit2['Level'] == 1) & (visit2['Object'] == 'CSF')]['Volume']\nplt.scatter(csf1, csf2)\n\n<matplotlib.collections.PathCollection at 0x7d3a7190dc90>\n\n\n\n\n\n\nsm.graphics.mean_diff_plot(csf1, csf2)\n\n\n\n\n\n\n\nOften the B/A plot is done on the log scale. This plots the log ratio on the y axis versus the geometric mean on the x axis.\n\nsm.graphics.mean_diff_plot(np.log(csf1), np.log(csf2))"
  },
  {
    "objectID": "theory_measurement.html#icc",
    "href": "theory_measurement.html#icc",
    "title": "31  Measurement",
    "section": "31.2 ICC",
    "text": "31.2 ICC\nThe intra-class correlation coefficient is a measure of agreement. It measures the ratio of the inter-subject variation to the total variation (intra and inter). I like to think of ICC as a random effect model. If \\(Y_{ij}\\) is measurement \\(j\\) on subject \\(i\\) then consider the random effect model\n\\[\nY_{ij} = \\mu + U_i + \\epsilon_{ij}\n~~~~ U_i \\sim \\mathrm{N}(0,\\sigma^2_u) ~~~~ \\epsilon_{ij} \\sim \\mathrm{N}(0, \\sigma^2)\n\\]\nThen, the ICC is defined as\n\\[\n\\frac{\\sigma^2_U}{\\sigma^2_U + \\sigma^2}.\n\\]\nNote, this ICC model applies even if there’s more than 2 measurements per subject.\nConsider two subjects, however. There’s an easy moment estimator in that \\[\nY_{i2} - Y_{i1} = \\epsilon_{i2} - \\epsilon_{i1}\n\\] Thus, the variance of the differences is an estimator of $2^2. Similarly, \\[\n(Y_{i1} + Y_{i2})/2 = U_i + (\\epsilon_{i2} + \\epsilon_{i1})/2.\n\\] Thus, the variance of the average is an estimate of \\(\\sigma_u^2 + \\sigma^2 / 4\\). Thus, we have two equations and two uknowns. This solution has the benefit that it doesn’t depend on the normality of the random effects. However, it can produce negative estimates. Another approach simply uses maximum likelihood.\n\nimport pingouin as pg\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\n\ncsfdf1 = visit1[(visit1['Type'] == 1) & (visit1['Level'] == 1) & (visit1['Object'] == 'CSF')]\ncsfdf2 = visit2[(visit2['Type'] == 1) & (visit2['Level'] == 1) & (visit2['Object'] == 'CSF')]\n\ncsfdf = pd.concat( [csfdf1, csfdf2] )\ncsfdf['logvolume'] = np.log(csfdf['Volume'])\n\nmd = smf.mixedlm(\"logvolume ~ 1\", csfdf, groups=csfdf[\"ID\"]).fit()\nmd.summary()\n\nsigmasq = md.scale"
  },
  {
    "objectID": "supervised_regression_origin.html",
    "href": "supervised_regression_origin.html",
    "title": "23  Regression through the origin",
    "section": "",
    "text": "In this notebook, we investigate a simple poblem where we’d like to use one scaled regressor to predict another. That is, let \\(Y_1, \\ldots Y_n\\) be a collection of variables we’d like to predict and \\(X_1, \\ldots, X_n\\) be predictors. Consider minimizing\n\\[\nl = \\sum_i ( Y_i - \\beta X_i)^2 = || Y - \\beta X||^2.\n\\]\nTaking a derivative of \\(l\\) with respect to \\(\\beta\\) yields\n\\[\nl' = - \\sum_i 2 (Y_i - \\beta X_i) X_i.\n\\]\nIf we set this equal to zero and solve for beta we obtain the classic solution:\n\\[\n\\hat \\beta = \\frac{\\sum_i Y_i X_i}{\\sum_i X_i^2} = \\frac{<Y, X>}{||X||^2}.\n\\]\nNote further, if we take a second derivative we get\n\\[\nl'' = \\sum_i 2 x_i^2  \n\\]\nwhich is strictly positive unless all of the \\(x_i\\) are zero (a case of zero variation in the predictor where regresssion is uninteresting). Regression through the origin is a very useful version of regression, but it’s quite limited in its application. Rarely do we want to fit a line that is forced to go through the origin, or stated equivalently, rarely do we want a prediction algorithm for \\(Y\\) that is simply a scale change of \\(X\\). Typically, we at least also want an intercept. In the example that follows, we’ll address this by centering the data so that the origin is the mean of the \\(Y\\) and the mean of the \\(X\\). As it turns out, this is the same as fitting the intercept, but we’ll do that more formally in the next section.\nFirst let’s load the necessary packages.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nNow let’s download and read in the data.\n\ndat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv\")\ndat.head()\n\n\n\n\n\n  \n    \n      \n      FLAIR\n      PD\n      T1\n      T2\n      FLAIR_10\n      PD_10\n      T1_10\n      T2_10\n      FLAIR_20\n      PD_20\n      T1_20\n      T2_20\n      GOLD_Lesions\n    \n  \n  \n    \n      0\n      1.143692\n      1.586219\n      -0.799859\n      1.634467\n      0.437568\n      0.823800\n      -0.002059\n      0.573663\n      0.279832\n      0.548341\n      0.219136\n      0.298662\n      0\n    \n    \n      1\n      1.652552\n      1.766672\n      -1.250992\n      0.921230\n      0.663037\n      0.880250\n      -0.422060\n      0.542597\n      0.422182\n      0.549711\n      0.061573\n      0.280972\n      0\n    \n    \n      2\n      1.036099\n      0.262042\n      -0.858565\n      -0.058211\n      -0.044280\n      -0.308569\n      0.014766\n      -0.256075\n      -0.136532\n      -0.350905\n      0.020673\n      -0.259914\n      0\n    \n    \n      3\n      1.037692\n      0.011104\n      -1.228796\n      -0.470222\n      -0.013971\n      -0.000498\n      -0.395575\n      -0.221900\n      0.000807\n      -0.003085\n      -0.193249\n      -0.139284\n      0\n    \n    \n      4\n      1.580589\n      1.730152\n      -0.860949\n      1.245609\n      0.617957\n      0.866352\n      -0.099919\n      0.384261\n      0.391133\n      0.608826\n      0.071648\n      0.340601\n      0\n    \n  \n\n\n\n\nIt’s almost always a good idea to plot the data before fitting the model.\n\nx = dat.T2\ny = dat.PD\nplt.plot(x, y, 'o')\n\n\n\n\nNow, let’s center the data as we mentioned so that it seems more reasonable to have the line go through the origin. Notice here, the middle of the data, both \\(Y\\) and \\(X\\), is right at (0, 0).\n\nx = x - np.mean(x)\ny = y - np.mean(y)\nplt.plot(x, y, 'o')\n\n\n\n\nHere’s our slope estimate according to our formula.\n\nb = sum(y * x) / sum(x ** 2 )\nb\n\n0.7831514763656\n\n\nLet’s plot it so to see how it did. It looks good. Now let’s see if we can do a line that doesn’t necessarily have to go through the origin.\n\nplt.plot(x, y, 'o')\nt = np.array([-1.5, 2.5])\nplt.plot(t, t * b)"
  },
  {
    "objectID": "supervised_regression.html",
    "href": "supervised_regression.html",
    "title": "27  Prediction with regression",
    "section": "",
    "text": "Recall, we discussed a strict threshold classifier with accuracy as the loss function. Now consider continuous prediction, we need a loss function. A reasonable strategy would be to minimize the squared distances between our predictions and the observed values. In other words, \\(\\sum_{i=1}^n (Y_i - \\hat \\mu_i)^2.\\)\nIf we were to dived this by \\(n\\), it would be the average of the squared errors, or the mean squared error (MSE). We can use minimizing the squared error both as a rule for finding a good prediction and as our evaluation strategy for held out data.\nWhat’s left is to figure out how to come up with \\(\\hat \\mu_i\\), our predictions for the observation \\(Y_i\\). We previously considered just a rescaled version of \\(X\\), our predictor, using regression through the origin. In this module, we’ll try a slightly more complex model that includes a location (intercept) shift and a scale factor (slope). The consequence will be to fit the best line, in a certain sense, through our \\(X\\), \\(Y\\) paired data.\nTo tie ourselves down with an example, consider the previous lecture’s example, consider trying to get the FLAIR value from the other, non-FLAIR, imaging values.\nLet’s look at the non-smoothed data (omitting the _10 and _20) using a pair plot. I’m color coding by whether or not the specific voxel is a lesion.\nT2 and PD (proton density) look pretty linearly related. Imagine a study where a researcher collected T2 but did not collect PD. Let’s try to predict their PD values from the T2 values using a line. We’ll use least squares as the loss function. Specifically\n\\[\n\\sum_{v=1}^V (PD_v - \\beta_0 - \\beta_1 T2_v)^2\n\\]\nwhere \\(v\\) stands for voxel and \\(PD_v\\) for the PD value at voxel \\(v\\), \\(T2_v\\) as the T2 value at voxel \\(v\\) and \\(\\beta_0\\) and \\(\\beta_1\\) are parameters that we have to learn.\nA general equation for fitting a line to data is\n\\[\n\\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2\n\\]\nwhere we want to use \\(X_i\\) to predict \\(Y_i\\).\nIt turns out that \\(\\beta_0\\) and \\(\\beta_1\\) have optimal solutions that we can write down. We get\n\\[\n\\hat \\beta_1 = Cor(X, Y) \\frac{SD_Y}{SD_X}\n\\]\nwhere \\(Cor(X, Y)\\) is the (Pearson) correlation between \\(X\\) and \\(Y\\) and \\(SD_X\\) is the standard deviation of \\(X\\) (and \\(SD_Y\\) is for \\(Y\\)). The intercept satisfies\n\\[\n\\hat \\beta_0 = \\bar Y - \\bar X \\hat \\beta_1\n\\]\nwhere \\(\\bar X\\) and \\(\\bar Y\\) are the means.\nNotice this latter equation reorganized is just\n\\[\n\\bar Y = \\hat \\beta_0 + \\bar X \\hat \\beta_1\n\\]\npointing out that the fitted line has to go through the point \\((\\bar X, \\bar Y)\\)."
  },
  {
    "objectID": "supervised_regression.html#some-definitions",
    "href": "supervised_regression.html#some-definitions",
    "title": "27  Prediction with regression",
    "section": "27.1 Some definitions",
    "text": "27.1 Some definitions\n\nThe covariance is defined as \\(Cov(X,Y) = \\sum_{i=1}^n (Y_i - \\bar Y) (X_i - \\bar X) / (N-1)\\)\nThe standard deviation of \\(X\\) is \\(SD_X\\), \\(\\sqrt{Cov(X, X)}\\)\nThe Pearson correlation is defined as \\(\\frac{Cov(X, Y)}{SD_X \\times SD_Y}\\)\n\nThe Pearson correlation measures the degree of linear association between two variables where neither is thought of as an outcome or predictor. It is a unit free quantity. If you just say “correlation” without further context, it’s understood to mean the Pearson correlation. The covariance measures the same thing, though it has the units of the units X times the units of Y. The sample standard deviation of X has the units of X and measures the spread, or variability, of X. The variance, \\(Cov(X, X)\\), is simply the square of the standard deviation and has units of X squared.\n\nx = dat['T2']\ny = dat['PD']\ntrainFraction = 0.75\n\n## Hold out data\nsample = np.random.uniform(size = 100) < trainFraction\nxtrain = x[ sample]\nytrain = y[ sample]\nxtest =  x[~sample]\nytest =  y[~sample]\n\n## get the slope on the training data\nbeta1 = st.pearsonr(xtrain, ytrain)[0] * np.std(ytrain) / np.std(xtrain)\nbeta0 = np.mean(ytrain) - np.mean(xtrain) * beta1\nprint([beta0, beta1])\n\nsns.scatterplot(xtrain, ytrain)\n## add a line\nsns.lineplot(xtrain, beta0 + beta1 * xtrain)\n\n[0.3238607069346463, 0.8059833725629524]\n\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/seaborn/_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/seaborn/_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n<AxesSubplot:xlabel='T2', ylabel='PD'>\n\n\n\n\n\n\nprint(st.linregress(x = xtrain, y = ytrain))\nsns.regplot(xtrain, ytrain)\n\nLinregressResult(slope=0.8059833725629525, intercept=0.32386070693464586, rvalue=0.7959549519668292, pvalue=6.709090489489516e-19, stderr=0.06896596174865925, intercept_stderr=0.061396000904746124)\n\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/seaborn/_decorators.py:36: FutureWarning:\n\nPass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n\n\n\n<AxesSubplot:xlabel='T2', ylabel='PD'>\n\n\n\n\n\nLet’s now calculate our predictions on the test set. Recall, the test set was not used to come up with estimates of \\(\\beta_0\\) and \\(\\beta_1\\). We’ll show the training MSE and the testing MSE as well as a plot of the test set actual Ys versus the predicted ones.\n\nyhat_test = beta0 + beta1 * xtest\nyhat_train = beta0 + beta1 * xtrain\n\n## claculate the MSE in the training and test sets\nprint([ np.mean( (ytrain - yhat_train) ** 2), \n        np.mean( (ytest -  yhat_test) ** 2 ) ])\n\n\nsns.scatterplot(yhat_test, ytest)\nplt.xlabel('Predicted value from xtest T2 values')\nplt.ylabel('Actual PD value from ytest')\n\n[0.19750891570592813, 0.132334362460783]\n\n\nText(0, 0.5, 'Actual PD value from ytest')"
  },
  {
    "objectID": "supervised_lm_interpretation.html",
    "href": "supervised_lm_interpretation.html",
    "title": "28  Regression interpretation",
    "section": "",
    "text": "\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 T_i + \\epsilon_i\n\\]\nTo tie ourselves down with a context, consider \\(Y_i\\) is blood pressure, \\(T_i\\) is a medication and \\(X_i\\) is BMI. Let’s look at different settings that could arise using plots.\nSince I’m going to be making the same plot over and over, I defined a function that\n\nfit the ANCOVA model using sklearn\nplotted the data as \\(X\\) versus \\(Y\\) with orange versus blue for treated versus not\nadded the fitted ANCOVA lines plus the marginal means (the means for each group disregarding \\(X\\)) as horizontal lines\n\nNote, the adjusted estimated treatment effect is the difference between the two parallel sloped lines. The unadjusted estimated treatment effect is the difference between the two horizontal lines. Let’s look at how adjustment changes things depending on the setting. First we’ll do our imports and then define a function that will make our plot for us and fit the ANCOVA model.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport copy\n\nsns.set()\n\n\ndef myplot(x, y, t):\n    x1 = x[t == 1]\n    x0 = x[t == 0]\n    y1 = y[t == 1]\n    y0 = y[t == 0]\n    xm1 = np.mean(x1)\n    xm0 = np.mean(x0)\n    ym1 = np.mean(y1)\n    ym0 = np.mean(y0)\n\n    X = np.array([x, t]).transpose()\n    out = LinearRegression().fit(X, y)\n    b0hat = out.intercept_\n    b1hat = out.coef_[0]\n    b2hat = out.coef_[1]\n    \n    plt.scatter(x0, y0)\n    plt.scatter(x1, y1)\n\n    col = sns.color_palette()\n\n    plt.axhline(y = ym0, c = col[0])\n    plt.axhline(y = ym1, c = col[1])\n\n    xlim = [np.min(x), np.max(x)]\n\n    ylim0 = [z * b1hat + b0hat + b2hat for z in xlim]\n    ylim1 = [z * b1hat + b0hat         for z in xlim]\n\n    plt.plot( xlim, ylim1)\n    plt.plot( xlim, ylim0) \n\n    plt.show()\n\nLet’s consider out model with \\(\\beta_0 = 0\\), \\(\\beta_1 = 1\\) and \\(\\beta_2 = 4\\). So the treated have an intercept 4 units higher. Let’s consider simulating from this model where the treatment is randomized.\n\nn = 100\nx = np.random.normal(size = n)\ne = np.random.normal(size = n)\nt = np.random.binomial(1, .5, n)\n\nbeta0 = 0\nbeta1 = 1\nbeta2 = 4\n\ny = beta0 + beta1 * x + beta2 * t + e\n\nmyplot(x, y, t)\n\n\n\n\nNotice that the marginal means (horizontal lines) are about 4 units appart, same as the lines. This is due to the randomization. A goal of randomization is to make our inference for the treatment unrelated to whether or not we adjust for the confounding variable (\\(X\\)). So, we get (up to random error) the ssame answer whether we adjust for \\(X\\) or not. Let’s consider a different setting.\n\nmyplot(x + t * 4, y, t)\n\n\n\n\nNow notice that there is a large unadjusted difference (difference between the horizontal lines) whereas there is not much of a difference between the lines. That is, when adjusting for \\(X\\), the relationship goes away. Of note, treatment assignment is highly related to the \\(X\\) variable. Orange dots tend to have a larger \\(X\\) value than the blue. Because of this, there’s pratically no area of overlap between the orange and the blue to directly compare them. The adjusted model is all model, extrapolating the blue line up to the orange and the orange down to the blue assuming that they’re parallel.\n\nmyplot(x + t * 4, y  - t * 4, t)\n\n\n\n\nAbove notice that the result is the reverse. There’s little association marginally, but a large one when conditioning. Let’s look at one final case.\n\nmyplot(x + t * 6, y  - t * 2, t)\n\n\n\n\nAbove things are even worse, the relationship has reversed itself. The marginal association is that the orange is above the blue whereas the conditional association is that the blue is above the orange. That is, if you fit the treatment model without \\(X\\) you get one answer, and with \\(X\\) you get the exact opposite answer! This is an example of so-called “Simpsons paradox”. The “paradox” isn’t that paradoxical. It simply says the relationship between two variables could reverse itself when factoring in another variable. Once again, note there’s no overlap in the distributions."
  },
  {
    "objectID": "supervised_example.html",
    "href": "supervised_example.html",
    "title": "29  Linear models: a classic example",
    "section": "",
    "text": "dat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/swiss.csv\")\ndat.head()\n\n\n\n\n\n  \n    \n      \n      Region\n      Fertility\n      Agriculture\n      Examination\n      Education\n      Catholic\n      Infant.Mortality\n    \n  \n  \n    \n      0\n      Courtelary\n      80.2\n      17.0\n      15\n      12\n      9.96\n      22.2\n    \n    \n      1\n      Delemont\n      83.1\n      45.1\n      6\n      9\n      84.84\n      22.2\n    \n    \n      2\n      Franches-Mnt\n      92.5\n      39.7\n      5\n      5\n      93.40\n      20.2\n    \n    \n      3\n      Moutier\n      85.8\n      36.5\n      12\n      7\n      33.77\n      20.3\n    \n    \n      4\n      Neuveville\n      76.9\n      43.5\n      17\n      15\n      5.16\n      20.6\n    \n  \n\n\n\n\n\ny = dat.Fertility\nx = dat.drop(['Region', 'Fertility'], axis=1)\nfit = LinearRegression().fit(x, y)\nyhat = fit.predict(x)\n[fit.intercept_, fit.coef_]\n\n[66.9151816789687,\n array([-0.17211397, -0.25800824, -0.87094006,  0.10411533,  1.07704814])]\n\n\n\nx2 = x\nx2['Test'] = x2.Agriculture + x2.Examination\nfit2 = LinearRegression().fit(x2, y)\nyhat2 = fit2.predict(x2)\n\n\nplt.plot(yhat, yhat2)\n\n\n\n\n\nx3 = x2.drop(['Agriculture'], axis = 1)\nfit3 = LinearRegression().fit(x3, y)\nyhat3 = fit3.predict(x3)\nplt.plot(yhat, yhat3)"
  },
  {
    "objectID": "unsupervised_pca_ica.html",
    "href": "unsupervised_pca_ica.html",
    "title": "34  PCA",
    "section": "",
    "text": "Let \\(\\{X_i\\}\\) for \\(i=1,\\ldots,n\\) be \\(p\\) random vectors with means \\((0,\\ldots,0)^t\\) and variance matrix \\(\\Sigma\\). Consider finding \\(v_1\\), a \\(p\\) dimensional vector with \\(||v_1|| = 1\\) so that \\(v_1^t \\Sigma v_1\\) is maximized. Notice this is equivalent to saying we want to maximize \\(\\mathrm{Var}( X_i^t V_1)\\). The well known solution to this equation is that \\(v_1\\) is the first eigenvector of \\(\\Sigma\\) and \\(\\lambda_1 = \\mathrm{Var}( X_i^t V_1)\\) is the associated eigenvalue. If \\(\\Sigma = V^t \\Lambda V\\) is the eigenvalue decomposition of where \\(V\\) are the eigenvectors and \\(\\Lambda\\) is a diagonal matrix of the eigenvalues ordered from greatest to least, then \\(v_1\\) corresponds to the first column of \\(V\\) and \\(\\lambda_1\\) corresponds to the first element of \\(\\Lambda\\). If one then finds \\(v_k\\) as the vector maximizing \\(v_k^t \\Sigma v_k\\) so that \\(v_k^t v_{k'} = I(k=k')\\), then the \\(v_k\\) are the columns of \\(V\\) and \\(v_k^t \\Sigma v_k = \\lambda_k\\) are the eigenvalues.\nNotice:\nNotation:\nStatistical properties under the assumption that the \\(x_i\\) are iid with mean 0 and variance \\(\\Sigma\\)"
  },
  {
    "objectID": "unsupervised_pca_ica.html#ica",
    "href": "unsupervised_pca_ica.html#ica",
    "title": "34  PCA",
    "section": "34.1 ICA",
    "text": "34.1 ICA\nICA, independent components analysis, tries to find linear transformations of the data that are statistically independent. Usually, independence is an assumption in ICA, not actually embedded in the loss function.\nLet \\(S_t\\) be an \\(\\mathbb{R}^p\\) collection of \\(p\\) source signals. Assume that the underlying signals are independent, so that \\(S_t \\sim F = F_1 \\times F_2 \\times \\ldots \\time f_p\\). Assume that the observed data is \\(X_t = M S_t\\) and \\(X_t \\sim G\\). It is typically assumed that \\(M\\) is invertible so that \\(S_t = M^{-1} X_t\\) and \\(M\\) and \\(M^{-1}\\) are called the mixing and unmixing matrices respectively. Note that, since we observe \\(X_t\\) over many repititions of \\(t\\), we can get an estimate of \\(G\\). Typically, it is also assumed that the \\(X_t\\) are iid over \\(t\\).\nOne way to characterize the estimation problem is to parameterize \\(F_1\\), \\(F_2\\) and \\(F_3\\) and use maximum likelihood, or equivalent [citations]. Another is to minimize some distance between \\(G\\) and \\(F_1\\), \\(F_2\\) and \\(F_3\\). Yet another is to actually maximize independence between the components of \\(S_t\\) using some estimate of independence [cite Matteson].\nThe most popular approaches try to find \\(M^{-1}\\) by maximizing non-Gaussianity. The logic goes that 1) interesting features tend to be non-Gaussian and 2) an appeal to the CLT over signals suggest that the mixed signals should be more Gaussian by being linear combinations of independent things. The latter claim is heuristic relative to the formal CLT. However, maximizing non-Gaussian components tends to work well in practice, thus validating the motivation empirically.\nOne form of ICA maximizes the kurtosis. If \\(Y\\) is a random variable, then \\(E[Y^4] - 3 E[Y^2]\\) is the kurtosis. One could then find \\(M^{-1}\\) that maximizes the empirical kurtosis of the unmixed signals. Another variation of non-Gaussianity maximizes neg-entropy. The neg-entropy of a density \\(h\\) is given by \\[\n- \\int h(y) \\log(h(y)) dy = - E_h[\\log h(Y)]\n\\] A well known theorem states that the Gaussian distribution has the largest entropy of all distributions with the same variance. Therefore, to maximize non-Gaussianity, we can minmize entropy, or equivalently maximize neg-entropy. We could subtract the entropy of the Gaussian distribution to consider this a cross entropy problem, but that only adds a constant to the loss function. The maximization of neg-entropy can be done many ways. We need the following. For a given \\(M^{-1}\\), estimate \\(G\\) from the collection \\(M^{-1} X_t\\), then calculate the neg-entropy of \\(f_j\\). Use that to then take an opimization step of \\(M\\) is the right direction. Some versions of estimation use a polynomial expansion of the \\(f_j\\), which then typically only requires higher order moments, like kurtosis. Fast ICA is a particular implmementation of maximizing neg-entropy.\nStatistical versions of ICA don’t require \\(M\\) to be invertible. Moreover, error terms can be added in which case you can see the connection between ICA and factor analytic models. However, factor analysis models tend to assume Gaussianity.\n\n34.1.1 Example\nConsider an example that PCA would have somewhat of a hard time with. In this case, our data is from a mixture of normals with half from a normal with a strong positive correlation and half with a strong negative correlation. Because the angle between the two is not 90 degrees PCA has no chance. No rotation of the axes satisfies the obvious structure in this data.\n\nn = 1000\n\nSigma = np.matrix([[4, 1.8], [1.8, 1]])\na = np.random.multivariate_normal( mean = [0, 0], cov = Sigma, size = int(n/2))\nSigma = np.matrix([[4, -1.8], [-1.8, 1]])\nb = np.random.multivariate_normal( mean = [0, 0], cov = Sigma, size = int(n/2))\nx = np.append( a, b, axis = 0)\nplt.scatter(x[:,0], x[:,1])\nplt.xlim([-6, 6])\nplt.ylim([-6, 6])\n\n(-6.0, 6.0)\n\n\n\n\n\nLet’s try fast ICA. Notice it comes much closer to discovering the structure we’d like to discover than PCA could. It pulls appart the two components to a fair degree. Also note, there’s a random starting point of ICA, so that I get fairly different fits over re-runs of the algorithm. I had to lower the tolerance to get a good fit.\nIndpendent components are order invariant and sign invariant.\n\ntransformer = FastICA(tol = 1e-7)\nicafit = transformer.fit(x)\ns = icafit.transform(x)\nplt.scatter(s[:,0], s[:,1])\nplt.xlim( [s.min(), s.max()])\nplt.ylim( [s.min(), s.max()])\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:116: ConvergenceWarning:\n\nFastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n\n\n\n(-0.13638425650171548, 0.12469398108999682)\n\n\n\n\n\n\n\n34.1.2 Cocktail party example\nThe classic ICA problem is the so called cocktail party problem. In this, you have \\(p\\) sources and \\(p\\) microphones. The microphones each pick up a mixture of signals from the different sources. The goal is to unmix the sources into the components. Independence makes sense in the cocktail party example, since logically conversations would have some independence.\n\nimport audio2numpy as a2n\ns1, i1 = a2n.audio_from_file(\"mp3s/4.mp3\")\ns2, i2 = a2n.audio_from_file(\"mp3s/2.mp3\")\ns3, i3 = a2n.audio_from_file(\"mp3s/6.mp3\")\n\n## Get everything to be the same shape and sum the two audio channels\nn = np.min((s1.shape[0], s2.shape[0], s3.shape[0]))\ns1 = s1[0:n,:].mean(axis = 1)\ns2 = s2[0:n,:].mean(axis = 1)\ns3 = s3[0:n,:].mean(axis = 1)\n\ns = np.matrix([s1, s2, s3])\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nMix the signals.\n\nw = np.matrix( [ [.7, .2, .1], [.1, .7, .2], [.2, .1, .7] ])\nx = np.transpose(np.matmul(w, s))\n\nHere’s an example mixed signal\n\nIPython.display.Audio(data = x[:,1].reshape(n), rate = i1)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nNow try to unmix using fastICA\n\ntransformer = FastICA(whiten=True, tol = 1e-7)\nicafit = transformer.fit(x)\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning:\n\nnp.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n\n\n\n\nicafit.mixing_\n\narray([[-12.70604783,  16.57484878, -51.16254562],\n       [-44.07047498,  31.47146517,  -7.49682766],\n       [ -5.51880659, 108.62753289, -14.1474812 ]])\n\n\nUnmixing matrix\n\nicafit.components_\n\narray([[ 0.00166492, -0.02401024,  0.00670218],\n       [-0.00262563, -0.00046277,  0.00974047],\n       [-0.02080964,  0.00581294,  0.0014911 ]])\n\n\nHere’s a scatterplot matrix where the real component is on the rows and the estimated component is on the columns.\n\nhat_s = np.transpose(icafit.transform(x))\n\nplt.figure(figsize=(10,10))\n\nfor i in range(3):\n  for j in range(3):\n    plt.subplot(3, 3, (3 * i + j) + 1)\n    plt.scatter(hat_s[i,:].squeeze(), np.asarray(s)[j,:])\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning:\n\nnp.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n\n\n\n\n\n\nWe can now play the estimated sources and see how they turned out.\n\nfrom scipy.io.wavfile import write\ni = 0\ndata = (hat_s[i,:].reshape(n) / np.max(np.abs(hat_s[i,:]))) * .5\nIPython.display.Audio(data = data, rate = i1)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ni = 1\ndata = (hat_s[i,:].reshape(n) / np.max(np.abs(hat_s[i,:]))) * .5\nIPython.display.Audio(data = data, rate = i1)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ni = 2\ndata = (hat_s[i,:].reshape(n) / np.max(np.abs(hat_s[i,:]))) * .5\nIPython.display.Audio(data = data, rate = i1)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n34.1.3 Imaging example using ICA\nLet’s see what we get for the images. Logically, one would consider voxels as mixed sources and images as the iid replications. But, then the sources would not be images. Let’s try the other dimension and see what we get where subject images are mixtures of source images. This is analogous to finding a soure basis of subject images.\nThis is often done in ICA where people transpose matrices to investigate different problems.\n\ntransformer = FastICA(n_components=10, random_state=0,whiten='unit-variance', tol = 1e-7)\nicafit = transformer.fit_transform(np.transpose(train_matrix))\nicafit.shape\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:116: ConvergenceWarning:\n\nFastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n\n\n\n(2352, 10)"
  },
  {
    "objectID": "tooling_streamlit.html",
    "href": "tooling_streamlit.html",
    "title": "21  Streamlit",
    "section": "",
    "text": "Streamlit, like voila, panel, shiny and flask, is an alternative to using dash. At this moment, dash seems to be the most popular solution for python dashboarding. However, streamlit seems to be gaining traction. Streamlit is very easy to learn.\nFor installation, simply do\npip install streamlit\nNow, create a python file. Now create a file, say app1.py, with the following code:\nimport streamlit as st\n\nst.write(\"\"\"\n# Streamlit can use markdown syntax\n## Level 2\n\n+ bullet 1\n+ bullet 2\n\"\"\"\n)\nNow open up a terminal in the directory of your app and type\nstreamlit run app1.py\nIt should pop up a browser with your app running!"
  },
  {
    "objectID": "tooling_streamlit.html#input-output",
    "href": "tooling_streamlit.html#input-output",
    "title": "21  Streamlit",
    "section": "21.2 Input / output",
    "text": "21.2 Input / output\nStreamlit is very easy for programming I/O. Here’s a simple example\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\n\ncb = st.checkbox(\"Did you check the box\")\nif cb:\n    st.write(\"Yes you did! :-)\")\nelse:\n    st.write(\"No you didn't :-(\")\n\n\nrb = st.radio(\n     \"Pick an option\",\n     ('Option 1', 'Option 2', 'Option 3'))\nst.write(\"you picked \" + rb)\n\n\n## Adding a chart\n\nnsims = st.number_input('Put in a number of sims', value = 10)\nif nsims < 10 : \n    st.write(\"Pick a bigger number\")\nelse :\n    chart_data = pd.DataFrame(\n         np.random.randn(np.round(nsims), 2),\n         columns=['a', 'b'])\n    st.line_chart(chart_data)\nsimply save this in a file, say app2.py and type streamlit run app2.py. Also, the app has a button that allows you to deploy it to the streamlit hosting service."
  },
  {
    "objectID": "tooling_streamlit.html#multipage-apps",
    "href": "tooling_streamlit.html#multipage-apps",
    "title": "21  Streamlit",
    "section": "21.3 Multipage apps",
    "text": "21.3 Multipage apps\nIt’s easy and possible to have multi-page apps. Simply\n\nCreate a directory, say multi-page\nCreate a root file in that directory, say home.py\nCreate a subdirectory called pages\nPut the subpages as files in the pages directory\nNavigate to multi-page then streamlit run home.py\n\nAn example is in book\\streamlit_apps. Here is the results\n\n\n\nPage picture"
  },
  {
    "objectID": "tooling_dash.html",
    "href": "tooling_dash.html",
    "title": "22  Dash",
    "section": "",
    "text": "Dash is a framework for developing dashboards and creating web based data apps in R, python or Julia. Dash is more popular among the python focused, while shiny, a related platform, is more popular among R focused, but also applies much more broadly than just for R apps.\nBecause we’re focusing on python as our base language, we’ll focus on dash. There’s a wonderful set of tutorials here. Follow the instructions there on installation. We’ll build a simple app here building on that tutorial. Let’s take the first plotly example and use one of our examples.\nPut this code below in a file, say dashApp.py (in the github repo it’s in assetts/dash), then run it with python dashApp.py. If all has gone well, your app should be running locally at http://127.0.0.1:8050/ (so visit that site in a browser).\nThe resulting website, running at http://127.0.0.1:8050/ looks like this for me:\nAgain, 127.0.0.1 is the localhost address and :8050 is the port. You can change the port in the .run_server method. But, we want fancier apps that call a server and return calculations back to us (so-called callbacks)."
  },
  {
    "objectID": "tooling_dash.html#layout",
    "href": "tooling_dash.html#layout",
    "title": "22  Dash",
    "section": "22.1 Layout",
    "text": "22.1 Layout\nDash allows you to add python elements such as headings and divs. However, it also allows you to add markdown, which it will convert for you. For example, try running the app assets/dash/dashApp4.py. Here the dcc element\napp.layout = dcc.Markdown('''\n# Section 1\n## Section 2\n### Section 3\n\n1. Numbered lists\n2. Second item\n\n* Bulleted list\n* Second item\n''')\nis filled with markdown syntax. That is, you can use these elements to put in markdown syntax without having to use HTML code."
  },
  {
    "objectID": "tooling_dash.html#dash-callbacks",
    "href": "tooling_dash.html#dash-callbacks",
    "title": "22  Dash",
    "section": "22.2 Dash callbacks",
    "text": "22.2 Dash callbacks\nOur previous dash application wasn’t that impressive. We’d like to take user input and modify our page. To do this, we need callbacks. In this example, we create a dropdown menu that selects which graph we’re going to show.\nfrom dash import Dash, dcc, html, Input, Output\nimport plotly.express as px\nimport pandas as pd\n\n\ndat = pd.read_csv('kirby21AllLevels.csv')\ndat = dat.loc[dat['type'] == 1].groupby([\"roi\", \"level\"])['volume'].mean().reset_index()\n\napp = Dash(__name__)\n\napp.layout = html.Div([\n    dcc.Dropdown(options = [\n            {'label' : '1', 'value' : 1},\n            {'label' : '2', 'value' : 2},\n            {'label' : '3', 'value' : 3},\n            {'label' : '4', 'value' : 4},\n            {'label' : '5', 'value' : 5}\n        ],\n        value = 1, id = 'input-level'\n                ),\n    dcc.Graph(id = 'output-graph')\n])\n\n\n@app.callback(\n    Output('output-graph', 'figure'),\n    Input('input-level', 'value'))\ndef update_figure(selected_level):\n    subdat = dat.loc[dat['level'] == int(selected_level)].sort_values(by = ['volume'])\n    fig = px.bar(subdat, x='roi', y='volume')\n    return fig\n\nif __name__ == '__main__':\n    app.run_server(debug=True, host = '127.0.0.1')\nThe result looks like the followingm, where you can see the dropdown selection.\n\n\n\nGrahpic"
  },
  {
    "objectID": "tooling_dash.html#diferent-kinds-of-inputs",
    "href": "tooling_dash.html#diferent-kinds-of-inputs",
    "title": "22  Dash",
    "section": "22.3 Diferent kinds of inputs",
    "text": "22.3 Diferent kinds of inputs\nDash has several kinds of inputs. A full list can be found here. We’ll go over a couple of types here. Try running the following code to see different kinds of inputs (also in assets/dash/dashApp3.py).\nfrom datetime import date\nfrom dash import Dash, html, dcc\nfrom dash.dependencies import Input, Output\n\napp = Dash(__name__)\n\napp.layout = html.Div([\n    html.H1(\"Dropdown\"),\n    dcc.Dropdown(\n       options=[\n           {'label': 'Type 1', 'value': 1},\n           {'label': 'Type 2', 'value': 2},\n           {'label': 'Type 3', 'value': 3},\n       ],\n       value = 2\n    ),\n    html.H1(\"Checklist\"),\n    dcc.Checklist(       \n        options=[\n           {'label': 'Type 1', 'value': 1},\n           {'label': 'Type 2', 'value': 2},\n           {'label': 'Type 3', 'value': 3},\n       ]\n    ), \n    html.H1(\"Slider\"),\n    dcc.Slider(min = 0, max = 20, step = 5, value = 10, id='slider'),\n    html.H1(\"Date picker\"),\n    dcc.DatePickerSingle(\n        id='my-date-picker-single',\n        min_date_allowed = date(1995, 8, 5),\n        max_date_allowed = date(2017, 9, 19),\n        initial_visible_month=date(2017, 8, 5),\n        date=date(2017, 8, 25)\n    )\n\n])\n\n\n\nif __name__ == '__main__':\n    app.run_server(host = '127.0.0.1')\nThis looks something like this\n\n\n\nGraphic"
  },
  {
    "objectID": "tooling_dash.html#multiple-callbacks",
    "href": "tooling_dash.html#multiple-callbacks",
    "title": "22  Dash",
    "section": "22.4 Multiple callbacks",
    "text": "22.4 Multiple callbacks\nIn this example, we show how to utilize multiple inputs in one app. We calculate the basal metabolic rate (BMR) from the formulas from this website. The app below needs statistical development in a few ways. For example, the formula utilizes gender, but some reading suggest sex might be more appropriate. However, the literature seems somewhat sparse on the relationship between of sex and interactions with hormone therapies in BMRs. Regardless, uncertainty is not quantified in the estimates. So, primarily, this app is useful for demonstrating dash development. In the later chapters, we’ll discuss building in specifics and evaluation into prediction apps.\nfrom dash import Dash, dcc, html, Input, Output\n\napp = Dash(__name__)\n\napp.layout = html.Div([\n    html.H1(\"Enter your data to see the results\"),\n    html.Div([\n        html.H2('Enter your weight in kg'),\n        dcc.Input(id = 'weight', value = 95, type = 'number'),\n        html.H2('Enter your height in cm'),\n        dcc.Input(id = 'height', value = 200, type = 'number'),\n        html.H2('Enter your age in years'),\n        dcc.Input(id = 'age', value = 50, type = 'number'),\n        html.H2('Enter your gender'),\n        dcc.RadioItems(options = [{'label': 'Male', 'value': 'm'},{'label': 'Female', 'value': 'f'}],\n                       value = 'm',\n                       id = 'gender')\n    ]),\n    html.Br(),\n    html.H1(\"Your estimated basal metabolic rate is: \"),\n    html.H1(id = 'bmr'),\n\n])\n\n\n@app.callback(\n    Output(component_id = 'bmr'   , component_property = 'children'),\n    Input(component_id  = 'weight', component_property = 'value'),\n    Input(component_id  = 'height', component_property = 'value'),\n    Input(component_id  = 'age'   , component_property = 'value'),\n    Input(component_id  = 'gender'   , component_property = 'value')\n)\ndef update_output_div(weight, height, age, gender):\n    if gender == 'm':\n        rval = 88.362 + (13.397 * weight) + (4.799 * height) - (5.677 * age)\n    if gender == 'f':\n        rval = 447.593 + (9.247 * weight) + (3.098 * height) - (4.330 * age)\n    return rval\n\nif __name__ == '__main__':\n    app.run_server(debug=True, host = '127.0.0.1')\n\nwhich then looks something like this\n\n\n\ngraphic"
  },
  {
    "objectID": "tooling_dash.html#adding-a-submit-button",
    "href": "tooling_dash.html#adding-a-submit-button",
    "title": "22  Dash",
    "section": "22.5 Adding a submit button",
    "text": "22.5 Adding a submit button\nLet’s talk about how to add a submit button. For this example, we’ll also use a custom library for returning and augmeting maps using dash called dash-leaflet. Notice a couple of things in this app. First, we use a submit button, and there’s no map plotted before we click submit the first time. Secondly, this shows an example of multiple callbacks in one app. Thirdly, we do an instance where we update a javascript graphic using server side calculations. On this final point, it’s probably preferable to actually do this on the client side. Dash does offer some facility for working with client side callbacks. However, we won’t discuss them as they tend to have a higher requirement for understanding the underlying javascript.\nimport dash_leaflet as dl\nfrom dash import Dash, html, dcc\nfrom dash.dependencies import Input, Output, State\nimport os\n\napp = Dash()  \napp.layout = html.Div([\n    html.H1(id = 'textout'),\n    dl.Map(id = \"output-state\"),\n    dcc.Input(id = 'lat', value = 39.298, type = 'number'),\n    dcc.Input(id = 'long', value = -76.590, type = 'number'),\n    dcc.Input(id = 'zoom', value = 11, type = 'number'),\n    html.Button('Submit', id='submit-button')\n])\n\n@app.callback(Output('output-state', 'children'),\n              Input('submit-button', 'n_clicks'),\n              State('lat', 'value'),\n              State('long', 'value'),\n              State('zoom', 'value'))\ndef update_output(n_clicks, lat, long, zoom):\n    if n_clicks is not None:\n        return dl.Map([dl.TileLayer()], \n                        center = (lat, long), \n                        zoom = zoom,\n                        style={'width': '100%', 'height': '75vh', 'margin': \"auto\", \"display\": \"block\"})\n\n@app.callback(Output('textout', 'children'),\n              Input('submit-button', 'n_clicks'),\n              State('lat', 'value'),\n              State('long', 'value'),\n              State('zoom', 'value'))\ndef update_text(n_clicks, lat, long, zoom):    \n        return  'Lat {}, Long {},  zoom {}, number of clicks {} times'.format(lat, long, zoom, n_clicks)\n\nif __name__ == '__main__':\n    app.run_server(debug = True, host = \"127.0.0.1\")"
  },
  {
    "objectID": "statistics_ml.html",
    "href": "statistics_ml.html",
    "title": "23  Maximum Likelihood",
    "section": "",
    "text": "How do we get the loss function that we use for logistic regression? It relies on a statistical argument called maximum likelihood (ML). Sadly, ML is used to be represent maximum likelihood and machine learning, both important topics in data science. So you’ll just kind of have to get used to which one is being used via the context.\nTo figure out maximum likelihood, let’s consider a bunch of coin flips, each with their own probability of a head. Say\n\\[\np_i = P(Y_i = 1 | x_i) ~~~ 1 - p_i = P(Y_i = 0 | x_i)\n\\]\nHere, we write \\(~| x_i\\) in the probability statement to denote that the probability may depend on the realized value of some variable that also depends on \\(i\\), \\(X_i\\), which is denoted as \\(x_i\\). So, for a context, think \\(Y_i\\) is event that person \\(i\\) has hypertension and \\(x_i\\) their smoking consumption in pack years. We’d like to estimate the probability that someone has hypertension given their pack years.\nWe could write this more compactly as:\n\\[\nP(Y_i = j | x_i) = p_i ^ j (1 - p_i)^{1-j} ~~~ j \\in \\{0, 1\\}.\n\\]\nConsider a dataset, \\(Y_1, \\ldots, Y_n\\) and \\(x_1, \\ldots, x_n\\). Consider a sequence of potential observed values of the \\(Y_i\\), say \\(y_1, \\ldots, y_n\\) where each \\(y_i\\) is 0 or 1. Then, using our formula:\n\\[\nP(Y_i = y_i | x_i) = p_i ^ {y_i} (1 - p_i)^{1-y_i}\n\\]\nThis is the (perhaps?) unfortunate notation that statisticians use, \\(Y_i\\) for the conceptual value of a variable and \\(y_i\\) for a realized value or number that we could plug in. This equation is just the probability of one \\(y_i\\). This is why I’m using a lowercase \\(x_i\\) for the variables we’re conditioning on. Perhaps if I was being more correct, I would write something like \\(P(Y_i = y_i ~|~ X_i = x_i)\\), but I find that adds too much notation.\nWhat about all of them jointly? If the coin flips are independent, a statistical way of saying unrelated, then the probabilities multiply. So the joint probability of our data in this case is\n\\[\nP(Y_1 = y_1, \\ldots, Y_n = y_n ~|~ x_1, \\ldots, x_n)\n= \\prod_{i=1}^n p_i ^ {y_i} (1 - p_i)^{1-y_i}\n\\]\nThis model doesn’t say much, there’s nothing to tie these probabilities together. In our example, all we could do is estimate the probability of hypertension for a bunch of people with exactly the same pack years. There’s no parsimony so to speak. It seems logical that groups with nearly the same pack years should have similar probabilities, or even better that they vary smoothly with pack years. Our logistic regression model does this.\nConsider again, our logistic regression model:\n\\[\n\\mathrm{logit}(p_i) = \\beta_0 + \\beta_1 x_i\n\\]\nNow we have a model that relates the probabilities to the \\(x_i\\) in a smooth way. This implies that if we plot \\(x_i\\) versus \\(\\mathrm{logit}(p_i)\\) we have a line and if we plot \\(x_i\\) versus \\(p_i\\) it looks like a sigmoid. So, under this model, what is our joint probability?\n\\[\nP(Y_1 = y_1, \\ldots, Y_n = y_n ~|~ x_1, \\ldots, x_n)\n= \\prod_{i=1}^n p_i ^ {y_i} (1 - p_i)^{1-y_i}\n= \\prod_{i=1}^n \\left(\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\right)^{y_i}\n\\left(\\frac{1}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\right)^{1-y_i}\n\\]\nWe can work around with this a bit to get\n\\[\n\\exp\\left\\{\\beta_0 \\sum_{i=1}^n y_i + \\beta_1 \\sum_{i=1}^n y_i x_i\\right\\}\\times \\prod_{i=1}^n \\left(\\frac{1}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\right)\n\\]\nNotice, interestingly, this only depends on \\(n\\), \\(\\sum_{i=1}^n y_i\\) and \\(\\sum_{i=1}^n y_i x_i\\). These are called the sufficient statistics, since we don’t actually need to know the individual data points, just these quantities. (Effectively these quantities can be turned into the proportion of Ys that are one and the correlation between the Ys and Xs.) Plug in these quantities and this equation spits out the joint probability of that particular sequence of 0s and 1s for the Ys given this particular collection of Xs. Of course, we can’t actually do this, because we don’t know \\(\\beta_0\\) or \\(\\beta_1\\).\nThe statistician Fisher got around this problem using maximum likelihood. The principle is something like this. Pick the values of \\(\\beta_0\\) and \\(\\beta_1\\) that make the data that we actually observed most probable. This seems like a good idea, since the data that we observed must be at least somewhat probable (since we observed it). When you take the joint probability and plug in the actual Ys and Xs that we observed and view it as a function of \\(\\beta_0\\) and \\(\\beta_1\\), it’s called a likelihood. So a likelihood is the joint probability with the observed data plugged in and maximum likelihood finds the values of the parameters that makes the data that we observed most likely.\nLet’s consider that for our problem. Generally, since sums are more convenient than producs, we take the natural logarithm. Then, this works out to be:\n\\[\n\\beta_0 \\sum_{i=1}^n y_i + \\beta_1 \\sum_{i=1}^n y_i x_i - \\sum_{i=1}^n \\log\\left(1 + e^{\\beta_0 + \\beta_1 x_i}\\right)\n\\]\nThis is the function that sklearn maximizes over \\(\\beta_1\\) and \\(\\beta_0\\) to obtain the estimates. There’s quite a few really good properties of maximum likelihood, which is why we use it."
  },
  {
    "objectID": "statistics_ml.html#linear-regression",
    "href": "statistics_ml.html#linear-regression",
    "title": "23  Maximum Likelihood",
    "section": "23.1 Linear regression",
    "text": "23.1 Linear regression\nYou might be surprised to find out that linear regression can also be cast as a likelihood problem. Consider an instance where we assume that the \\(Y_i\\) are Gaussian with a mean equal to \\(\\beta_0 + \\beta_1 x_i\\) and variance \\(\\sigma^2\\). What that means is that the probability that \\(Y_i\\) lies betweens the points \\(A\\) and \\(B\\) is governed by the equation\n\\[\nP(Y_i \\in [A, B) ~|~ x_i) = \\int_A^B \\exp\\left\\{ -(y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2 \\right\\} dy_i\n\\]\nLetting \\(A=-\\infty\\) and taking the derivative with respect to \\(B\\), we obtain the density function, sort of the probability on an infintessimally small interval:\n\\[\n\\exp\\left\\{ -(y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2 \\right\\}\n\\]\nUses the density evaluated at the observed data, the joint likelihood assuming independence is:\n\\[\n\\prod_{i=1}^n \\exp\\left\\{ -(y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2 \\right\\}\n= \\exp\\left\\{ -\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2 \\right\\}\n\\]\nSince it’s more convenient to deal with logs we get that the joint log likelihood is\n\\[\n- \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2\n\\]\nSince minimizing the negative is the same as maximizing this, and the constants of proportionality are irrelevant for maximizing for \\(\\beta_1\\) and \\(\\beta_0\\), we get that maximum likelihood for these parameters minimizes\n\\[\n\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2\n\\]\nwhich is the same thing we minimized to obtain our least squares regression estimates."
  },
  {
    "objectID": "tooling_streamlit.html#your-first-app",
    "href": "tooling_streamlit.html#your-first-app",
    "title": "21  Streamlit",
    "section": "21.1 your first app",
    "text": "21.1 your first app\nStreamlit, like voila, panel, shiny and flask, is an alternative to using dash. At this moment, dash seems to be the most popular solution for python dashboarding. However, streamlit seems to be gaining traction. Streamlit is very easy to learn.\nFor installation, simply do\npip install streamlit\nNow, create a python file. Now create a file, say app1.py, with the following code:\nimport streamlit as st\n\nst.write(\"\"\"\n# Streamlit can use markdown syntax\n## Level 2\n\n+ bullet 1\n+ bullet 2\n\"\"\"\n)\nNow open up a terminal in the directory of your app and type\nstreamlit run app1.py\nIt should pop up a browser with your app running!"
  }
]