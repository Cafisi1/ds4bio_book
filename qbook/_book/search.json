[
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This is a book for the Data Science for Bio/Biostat/Public Health/medical classes."
  },
  {
    "objectID": "intro_markdown.html",
    "href": "intro_markdown.html",
    "title": "1  Markdown",
    "section": "",
    "text": "As mentioned, markdown is markup language. So, you write in plain text and then it needs to be rendered into a pretty document or page. For example, all of these notes were written in markdown, but then converted to HTML. There are different flavors of markdown. So, syntax can change a bit. I’m using the one that works in quarto.\nHere’s how you section in markdown.\n# Top level heading\n## Second level heading\n### Third level heading\nand so on. It should look something like this, though the style can change depending on how it is being rendered.\n Top level heading \n Second level heading \n Third level heading \nYou can can bold text and italicize text like this:\n**bold** text and *italicize*\nIf you want an ordered list, like this:\n\nItem one.\nItem two.\nItem three.\n\nthen you can just do:\n1. Item one.\n2. Item two.\n3. Item three.\nNote, if you move around the order of the list in the markdown code, the markup engine will just reorder them starting at whatever the first number is. An unordered lists can be done with asterisks or plus signs or dashes. (I tend to use asterisks.) Also, putting brackets with an x makes for a check mark.\n* [ ] Pick up broccoli\n* [ ] Pick up oat milk\n* [x] Pick up golden berries\n* [x] Pick up tea\n\nPick up broccoli\nPick up oat milk\nPick up golden berries\nPick up tea\n\nInline code, like lambda x: x ** 2 can be written with backticks like this:\n`lambda x: x ** 2`\nBlock code is written in between three backticks.\n```\nlike this\n```\nLinks can be done like this:\n[Markdown cheat sheet](https://www.markdownguide.org/cheat-sheet/).\nwhich renders like this: Markdown cheat sheet. (Also, that’s a real link to a nice MD cheat sheet.) Images can be done like this\n![Image alt text](assets/images/book_graphic.png)\nIf your converter can use mathjax, or some other LaTeX math rendering library, you can insert LaTeX equations. For example,\n\\[\n\\int_{-\\infty}^\\infty (2\\pi\\sigma^2)^{-1/2} \\exp\\left\\{-\\frac{1}{2\\sigma^2} (x - \\mu)^2\\right\\} dx = 1\n\\]\ncan be written as\n$$\n\\int_{-\\infty}^\\infty (2\\pi\\sigma^2)^{-1/2} \\exp\\left\\{-\\frac{1}{2\\sigma^2} (x - \\mu)^2\\right\\} dx = 1\n$$\nI noticed the github markdown renderer doesn’t load mathjax, but most of the data science things do, like jupyter-lab, colab and quarto.\nThat’s plenty of markdown to start. Try it out. You’ll find that you pick it up really fast."
  },
  {
    "objectID": "intro_unix.html",
    "href": "intro_unix.html",
    "title": "2  Unix",
    "section": "",
    "text": "To get a unix terminal, you have several options. Since we’re promoting jupyter and jupyterlab, just open up the terminal on there. (Again, assuming you’re working on a unix/linux system.)\nThe first thing you should try is figuring out where you’re at. Do this by typing\nprompt> pwd\nThis will show you where you are in the directory structure. If you want to see the contents of the directory try these\nprompt> ls\nprompt> ls -al\nprompt> ls -alh\nAdding the flags -a lists everything, including directories with a weird character in front. The l gives the long format, which gives more information and the h changes the filesize lists to a more human readable format. I also like the option --color. What you get with-alh` is as follows.\ntotal 36K\ndrwxrwxrwx+ 7 codespace root      4.0K Feb 14 14:24 .\ndrwxr-xrwx+ 5 codespace root      4.0K Oct 19 15:21 ..\ndrwxrwxrwx+ 6 codespace codespace 4.0K Feb 14 14:31 book\ndrwxrwxrwx+ 8 codespace root      4.0K Feb 15 21:34 .git\n-rw-rw-rw-  1 codespace codespace  171 Feb 14 14:24 .gitignore\n-rw-rw-rw-  1 codespace codespace    0 Feb 14 14:23 .nojekyll\n-rw-rw-rw-  1 codespace codespace  444 Feb 14 14:24 README.md\ndrwxrwxrwx+ 3 codespace codespace 4.0K Feb 14 14:24 slides\ndrwxrwxrwx+ 7 codespace codespace 4.0K Oct 19 15:21 .venv\ndrwxrwxrwx+ 2 codespace codespace 4.0K Oct 19 15:23 .vscode\nThe drwxrwxrwx+1 looking columns give permissions d=directory, r=read, w=write and x=execute, the groups are owner (you), group, everyone. So a file that is -rw------- can be read and written to by the owner, but cannot be executed by anyone and no one else can read or write to it (except the superuser, who gets to do everything).\nTo change a directory, try the following\nprompt> cd DIRECTORY\nwhere DIRECTORY is the name of the directory that you want to change into. You can hit TAB to autocomplete names. The command\nprompt> mv PATH_TO_INPUT_FILE PATH_TO_OUTPUT_FILE\nmoves the file. This is also how you rename a file, since you could just do mv FILENAME1 FILENAME2 and change the name.\nThe unix command for removing things is rm. So\nrm FILENAME\ndeletes the file. Note linux really deletes things, so do this with some care. You can’t remove directories this way, instead you could do rmdir DIRECTORY, but the directory has to be empty. If you want to use rm to remove a directory and its contents, you can do rm -rf DIRECTORY. However, use this with care.\nFinally, I find it very useful to use wget to grab files from the internet. So, for example,\nwget https://URL.../FILENAME\nwill grab the file from that link. Super useful.\nThat’s enough unix to get you started. You’ll find as you use the terminal more and more, you’ll like it better and better. Eventually, you’ll find GUIs kind of frustrating."
  },
  {
    "objectID": "intro_html.html#html",
    "href": "intro_html.html#html",
    "title": "4  HTML, CSS and javascript",
    "section": "4.1 HTML",
    "text": "4.1 HTML\nHTML is a markup language used by web browsers. HTML stands for hypetext markup language. Like all markup languages, it gives a text set of instructions that get interpreted into a nicer looking document. Other markup languages include XML, LaTeX, Org and markdown. (Yes, mark”down” is named as such since it’s a ultra-simple mark”up” language.)\nWe’ll need a little html knowledge since so much data science output is web-page oriented. Also, we’ll need to know a little about html to scrape web content. A web page typically has three elements: the html which gives the page structure and markup, css (cascading style sheets) for style and javascript for interactivity. We’ll cover a little html and javascript so that we can better understand certain data science products. However, you should take a web development course if you want in depth treatments.\nWe won’t spend much time talking about CSS. CSS gives a set of standards for the style of a web page. With CSS one can take the skeleton (HTML/JS) and dramatically change the style in the same way you could choose to play some sheet music in different ways. A quick tutorial on CSS can be found here.\nBack to HTML. An HTML document looks something like this. Take a file, insert the following code and give it the extension .html. Then, open it up in a browser.\n&lt;!DOCTYPE html&gt;\n&lt;HTML&gt;\n    &lt;HEAD&gt;\n        &lt;TITLE&gt; This is the web page title&lt;/TITLE&gt;\n    &lt;/HEAD&gt;\n    &lt;BODY&gt;\n        &lt;H1&gt;Heading 1&lt;/H1&gt;\n        &lt;H2&gt;Heading 2&lt;/H2&gt;\n        &lt;P&gt; Paragraph &lt;/P&gt;\n        &lt;CODE&gt; CODE &lt;/CODE&gt;\n    &lt;/BODY&gt;\n&lt;/HTML&gt;\nThe resulting document will look like the following\n\n\n   \n      Heading 1\n      Heading 2\n       Paragraph \n      CODE \n   \n\nAs you probably noticed, a bit of markup is something like &lt;COMMAND&gt;CONTENT&lt;/COMMAND&gt;. The latter command has a forward slash. You should close your commands, even if your browser still renders the page like you like just because it makes for bad code not to. Also, someone else’s browser may not be as forgiving. Good code editors will help remind you to close your commands."
  },
  {
    "objectID": "intro_html.html#browser-stuff",
    "href": "intro_html.html#browser-stuff",
    "title": "4  HTML, CSS and javascript",
    "section": "4.2 Browser stuff",
    "text": "4.2 Browser stuff\nNote, since we’ll be working a lot with files, probably in one directory, you can use file:///PATH TO YOUR DIRECTORY to open up files (maybe even bookmark that directory). Also, CTRL-R is probably faster than clicking refresh and (in chrome at least) CTRL-I brings up developer tools (javascript console). When we have a web server running locallly, you usually go to localhost. For example, my jupyter lab server sends me to http://localhost:8888/lab/tree/. Here 8888 is a port, localhost refers to the server running on the lcoal computer and lab/tree is the relative path to the root of my jupyter lab server.\nBrowsers make choices in how they render HTML and CSS and implement javascript. So, unless you’re a web developer by trade, don’t get too exotic in your design choices. Also, a lot of HTML is auto generated. So, your mileage may vary by looking at page sources."
  },
  {
    "objectID": "intro_html.html#hosting",
    "href": "intro_html.html#hosting",
    "title": "4  HTML, CSS and javascript",
    "section": "4.3 Hosting",
    "text": "4.3 Hosting\nWhen you double click on your html file, it’s being hosted locally. So, no one else can see it. To have a web page on the internet it has to be hsoted on a server running web hosting software. Fortunately, github will actually allow us to host web pages. Basically, put an empty .nojekyll file in your repository (this tells it that it’s not a jekyll based web site and follow the instructions here. This will be really useful for us, since many of our datascience programs output web pages. For example, RMarkdown documents get translated into web documents. Similarly, jupyter-lab will output reveal.js (javascript/html) slide decks from our jupyter lab notebooks. Note that some of our programs will require servers that also run python or R in the back end, so github pages won’t suffice for that. There we need servers specifically set up to run those kinds of scripts."
  },
  {
    "objectID": "intro_html.html#javascript",
    "href": "intro_html.html#javascript",
    "title": "4  HTML, CSS and javascript",
    "section": "4.4 Javascript",
    "text": "4.4 Javascript\nJavascript is what makes webpages interactive. We’ll need a little javascript to understand how interactive web graphics work. Consider the following where we use javscript to change an HTML element in a web page\n<H2 id=\"textToChange\">Preference ?</H2>\n\n<button type=\"button\" onclick='document.getElementById(\"textToChange\").innerHTML = \"You prefer 1\"'>1</button>\n<button type=\"button\" onclick='document.getElementById(\"textToChange\").innerHTML = \"You prefer 2\"'>2</button>\nHere’s the result:\nPreference ?\nClick 1\nClick 2\n\n4.4.1 JSON\nJSON is a data format used in javascript, and adopted elsewhere. It’s a fairly straightforward data structure. In fact, you might have edited some JSON data by editing your Jupyter Lab ipython notebook properties. It goes \"name\":value where objects are enclosed in curly braces and arrays in brackets. You have to separate distinct object or values with quotes.\n{\n    \"name\" : \"Bowie\",\n    \"species\" : \"dog\",\n    \"age\" : 6,\n    \"likes\" : [\"biscuits\", \"naps\", \"walks\"],\n    \"dislikes\" : [\"the cat\", \"getting groomed\"]    \n}\nLet’s define a variable in our JS console. Open up your JS console and type:\npet = {\n    \"name\" : \"Bowie\",\n    \"species\" : \"dog\",\n    \"age\" : 6,\n    \"likes\" : [\"biscuits\", \"naps\", \"walks\"],\n    \"dislikes\" : [\"the cat\", \"getting groomed\"]\n};\nNow try doing things like pet.likes and hit enter."
  },
  {
    "objectID": "intro_html.html#example-of-using-a-javascript-library",
    "href": "intro_html.html#example-of-using-a-javascript-library",
    "title": "4  HTML, CSS and javascript",
    "section": "4.5 Example of using a javascript library",
    "text": "4.5 Example of using a javascript library\nLet’s use a javscript library to plot some data. Unless you need really fine control over the javascript elements, we usually do this by calling a python or R api to the JS library. But, it’s useful to do once to see how the library does things.\nHere is some data that we’ll use a lot. It is brain regions of interest (ROIs) by the percentage of the brain that they make up.\n               roi       comp\n0              CSF   7.370845\n1   Diencephalon_L   0.756288\n2   Diencephalon_R   0.763409\n3    Mesencephalon   0.864718\n4    Metencephalon  12.488275\n5   Myelencephalon   0.378464\n6  Telencephalon_L  42.030477\n7  Telencephalon_R  42.718368\nLet’s use Vegalite. This package creates the plot via a JSON object that contains all of the data and instructions.\n{\n    \"data\" : {\n        \"values\" : [\n{\"roi\" : \"CSF\"            , \"comp\" :  7.370845},\n{\"roi\" : \"Diencephalon_L\" , \"comp\" :  0.756288},\n{\"roi\" : \"Diencephalon_R\" , \"comp\" :  0.763409},\n{\"roi\" : \"Mesencephalon\"  , \"comp\" :  0.864718},\n{\"roi\" : \"Metencephalon\"  , \"comp\" : 12.488275},\n{\"roi\" : \"Myelencephalon\" , \"comp\" :  0.378464},\n{\"roi\" : \"Telencephalon_L\", \"comp\" : 42.030477},\n{\"roi\" : \"Telencephalon_R\", \"comp\" : 42.718368}\n        ]\n    },\n    },\n   \"mark\": \"bar\",\n   \"encoding\": {\n    \"y\": {\"field\": \"roi\" , \"type\": \"nominal\"},\n    \"x\": {\"field\": \"comp\", \"type\": \"quantitative\"}\n}\nThis needs to be embedded into html, plus the vega JS libraries loaded to execute. I have an example here. The output looks like this\n\n\n\nGraphic\n\n\nTypically, one creates these graphics in one’s home analysis language (like python or R). There are several libraries for doing as such. Some of the popular ones include: bookeh, vega, D3js, leaflet, but there are many more. There’s also connections to large private efforts including tableau, power bi, google charts and plotly."
  },
  {
    "objectID": "intro_git.html#the-least-you-need-to-know",
    "href": "intro_git.html#the-least-you-need-to-know",
    "title": "3  Git, github",
    "section": "3.1 The least you need to know",
    "text": "3.1 The least you need to know\nI’m not going to fully recreate git / github tutorials here; here’s one I recommend by Sean Kross. Instead, I’m going to go through a typical git / github workflow.\n\nInitialization I almost always initialize my git repository on github with a readme.md file.\nClone I typically clone the repository to my local computer using the command line or a local git gui that works with github, like this one. Note that you only have to clone the repo once. After it’s cloned you have a full local copy of the repository.\nadd new files to track and stage them after I’ve worked with them.\ncommit the changes to the local repository with a meaningful commit message.\npush the changes to the repository.\nIf there’s changes on the remote repository not represented in my local repository, I pull those changes to my local repo.\n\nFor larger projects, you’re likely working with multiple people, some of whom you’ve given access to your remote repository and some of whom you have not. The ones who can’t directly push to the remote repo might have their own version of the code and their own version on github. If they think you should incorporate those changes, they might issue a pull request to you. You can then opt to pull their changes into your repo on github, then pull them from github to your local repo. One of the reasons why services like github and bitbucket are so popular is that they make this coordination fairly easy, along with having nice project messaging and management tools.\nIn our class, we use github classroom. For github classroom, you’ll get a link to a repo to put your submission files into. When you push to the remote repository, you’ll have submitted. But, up to the due date you can make changes an push again."
  },
  {
    "objectID": "intro_git.html#a-little-more-detail",
    "href": "intro_git.html#a-little-more-detail",
    "title": "3  Git, github",
    "section": "3.2 A little more detail",
    "text": "3.2 A little more detail\n\n3.2.1 Getting started\nGit is a version control system invented by Linus Torvalds, the invetor of the linux operating system. A github repository is a directory that the software git is using to keep track of versions. Install git, it’s already installed on most cloud services and many operaiting systems. You can initialize a repository in a directory with the command git init in that directory. Honestly, I almost never start a git repo this way, because I’ve connected my repo to a cloud server. But, more on that later. After you’ve initialized it, your git repo is a locally version controlled system.\n\n\n3.2.2 Adding files\nOK, now you want to add some of your files to your repo. You have a directory You can add files with the command\ngit add FILENAME\nThe file FILENAME is now staged to be part of the repository, but isn’t added yet! Files that you haven’t added won’t be tracked. This is good, since there’s many files, llike log files, that we don’t want to track. You only need to add the files once, then they are being tracked. Try it out with a readme markdown file in your repository.\n\n\n3.2.3 Checking status\nOne of the most useful git commands is\ngit status\nThis gives the status of which files are being tracked, which are not and which have changes.\n\n\n3.2.4 Commiting changes\nYou have to actually commit your changes to the repository to have them represented. Committing changes is a local operation, subsequently, we’ll show you how to coordinate with a remote repository. Here’s the command I use to commit most often\ngit commit -a -m \"COMMIT MESSAGE\"\nHere the flag -a means commit all files staged for commits. You can also commit individual files or a collection of specific files. The -m \"COMMIT MESSAGE\" is the message associated with your commit. Make your commit messages more informative. If you don’t add -m github will bring up an editor for you to enter your commit message.\n\n\n3.2.5 Remote repositories\nGit becomes much more useful when it’s coordinated with a remote repository. Github is a server for hosting remote git repositories, there are several others, bitbucket is an example. Github also includes a great deal of functionality for working with the remote repository through the website. There’s many git and github guis that you could use if you want. I always liked to github one and magit in emacs.\n\n3.2.5.1 Connecting your remote and local repos\nYou need to connect your local repo to the remote repo. First create a version of the repo on github (actually then it gives you instructions). Then you can either\n\nClone the repo from the server git clone REMOTE SERVER\nConnect the server to your local repository\n\nFor 2. you use a command like:\ngit remote add origin git@github.com:USERNAME/REPONAME.git\nPersonally, I find it easier just to create the repo on github and clone it. You can clone over ssh or https, if you don’t know what ssh is use https and then switch over to ssh as you learn how to use it.\n\n\n3.2.5.2 Pushing changes\n\n\n\n\n\nYou push from the repo you’re working on to another (remote repo). So, for me, this typically means pushing from my local computer repo to github or to a scientific computing cluster.\ngit push origin main\nThe above command could be read as “push my new committed changes from my origin to the main branch on my remote repo”. Older repos had the default branch master instead of main. But, github switched in 2020 or so. So, some older documentation may not have this switch.\n\n\n3.2.5.3 Pulling changes\n\n\n\n\n\nThere might be changes that you want represented locally that were changed on the remote repository; for example, you might edit a readme file through github’s web interface. The command for this is really easy, it’s just pull\ngit pull\nThe pain occurs if you have made local changes that are in conflict with the remote changes. I’d write about merge conflicts, but I can’t, I just can’t (shudder). [Here’s some docs on managing merge conflicts] (https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/addressing-merge-conflicts/resolving-a-merge-conflict-using-the-command-line).\n\n\n3.2.5.4 Cloning\n\n\n\n\n\nTo clone a remote repository is to simply create a local copy where you can pull changes from the remote. If a repository is public and you own it, then you can clone it and push changes back to the remote. If you don’t, you can clone and pull changes, but then can’t push back to the remote.\nTry with the book repo, clone it so you have access to the files. In the figure below the owner can pull and push to the repo. In the picture below, consider that you clone GH1 off of github to you. You can pull updates from GH1 but can’t push changes to GH1. The owner of GH1 of course can pull and push.\nAs an example, close this book (via https) with:\ngit clone https://github.com/smart-stats/ds4bio_book.git\n\n\n3.2.5.5 Forking\n\n\n\n\n\nA common workflow for github is to “fork” the repository. This differs from cloning in the sense that your fork is an entirely separate new copy. In the picture above, GH2 is a fork of GH1. It can pull upstream changes from GH1, but can’t write to it, since you don’t own that repo. You can push and pull from Local2 which is connected to GH2.\nForking on github is easy. Just click the “fork” button in the upper right corner.\n\n\n3.2.5.6 Pull requests\n\n\n\n\n\nImagine a case where you forked the book repo and have some great additions (maybe a section on merge conflicts?). You think that these changes you think should be represented for other students. You can’t push changes to my repo; the very idea would be perposterous! How do you get your great changes represented in my repo?\nYou issue a pull request in the sense of requesting GH1 to pull your changes from GH2. Then, I would get notified of the pull request and would decide whether or not to merge them into my repo. This system has many benefits, and particularly, is great for operating on open source projects.\nI like to do pull requests directly on github through the web interface."
  },
  {
    "objectID": "intro_git.html#branching",
    "href": "intro_git.html#branching",
    "title": "3  Git, github",
    "section": "3.3 Branching",
    "text": "3.3 Branching\nYou often want multiple versions of a repository. For example, you might want a development version, a working version and a stable version. This is what branches are for in git. To create and switch to a branch called dev, use:\ngit checkout -b dev\nThis creates a new branch called dev and switches to that branch. If you already have dev, use checkout without -b. Suppose you want to merge your change from dev into your default branch (main)\ngit checkout main\ngit merge dev\nStart very basic with branches, then when you get the hang of it you can use it like a pro. Here’s more documentation."
  },
  {
    "objectID": "intro_git.html#clients",
    "href": "intro_git.html#clients",
    "title": "3  Git, github",
    "section": "3.4 Clients",
    "text": "3.4 Clients\nYou can avoid the command line stuff with a git client, of which there are many. It’s still good to know the command line options, for remote work if nothing else. I’ve tried several git clients and my favorite is the github client software and magit in emacs."
  },
  {
    "objectID": "intro_git.html#setting-up-ssh",
    "href": "intro_git.html#setting-up-ssh",
    "title": "3  Git, github",
    "section": "3.5 Setting up ssh",
    "text": "3.5 Setting up ssh\nIf you want to connect to a remote repository to github on a unix or linux system, it’s probably easiest to set up using so called secure shell. You can also set it up a variety of ways on Windows, but the rules are a little less universal. If you find this very hard to follow, just use a git and github gui (like github desktop, which is what I like to recommend).\nSecure shell works as follows. It creates a public key (think like a lock, everyone can see it) and a private key (like the key for the lock, only you have it). When you connect to github, it has your public key and you use the private key to “unlock” it. First, you need a .ssh directory from your personal directory. If you don’t have that, try ssh username@server to a server that you know and it will automatically create one.\nwe’ll go through this tutorial.\nThen, cd into your .ssh director and type the following and follow the instructions\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\nThis generates your public/prviate key pair. Then you need to take the public key file, and upload it to github under settings. This is described here.\nIn some cases I’ve also had to add this to a file config in my .ssh directory:\nHost github.com\n     ForwardX11 no\n     HostName github.com\n     User git\n     IdentityFile ~/.ssh/YOUR_PRIVATE_KEY_FILENAME\nOnce this is setup, then you should be able to push and pull from github from the command line without passwords."
  },
  {
    "objectID": "intro_git.html#github-pages",
    "href": "intro_git.html#github-pages",
    "title": "3  Git, github",
    "section": "3.6 Github pages",
    "text": "3.6 Github pages\nIt’s possible to host web pages on github. By hosting I mean serving the web page on the internet so that others can view it. There’s a couple of steps to doing this. First, you need an html file to serve. If you look in the chapter on html, there’s the code for a basic wepage. Save a web page as say “index.html”. You can double check that it works by simply double clicking on the file on your local computer. It should display as a web page in your browser, but it’s only viewable by you. Notice the addres begins with file:///.... To serve the file so that others can view it, we need it to exist on a server.\nGithub will serve the file for us, but first we need a repository. Create a public repository; I typically do this on github. Then you need to add an empty file called .nojekyll. Don’t forget the period at the beginning. This prevents github from thinking you’re using the jekyll framework, which is the default. Since we’re serving a basic webpage, we don’t need this framework. I create this file on github just by clicking Add File then Create new file.\nNext we need to tell github that this repo serves web pages. Click on settings then pages on the left. Then, under source choose the branch that you want to serve the pages from. Add your html file, commit and push to github. Give it a few minutes, then your file will be served from\nhttps:///GITHUBUSERNAME.github.io/REPONAME/FILENAME.html\nwhere here GITHUBUSERNAME is either your github username or the organization. REPONAME is the name of your repository and FILENAME.html is your html file, with the path if it’s in subdirectories. As an example, here’s a version I did https://bcaffo.github.io/testHtmlRepo/index.html."
  },
  {
    "objectID": "python.html#notebooks",
    "href": "python.html#notebooks",
    "title": "Python",
    "section": "Notebooks",
    "text": "Notebooks\nNotebooks are going to be especially useful for us, as they’re a great way to do data analyses. With notebooks, you can merge richer documentation together with analysis code. You can take this to the extreme, and have solutions that create reproducible final documents. This book is an example, where the entire thing is written in jupyter-book. We’ll discuss this idea a little more when we discuss reproducible research. Alternatively, you can use your notebook as a working document that\nMost notebook solutions have text blocks and code blocks. The text is marked up in a markup language called “Markdown”, which we discussed eariler.\nIf you’re very new to notebooks in python, I would suggest starting with colab. The colab documentation is useful."
  },
  {
    "objectID": "python.html#weaved-text-formats",
    "href": "python.html#weaved-text-formats",
    "title": "Python",
    "section": "Weaved text formats",
    "text": "Weaved text formats\nI wrote this book in a format called quarto (see https://quarto.org/ ). This is a slightly different approach than jupyter notebooks and are perhaps better at producing final document-style output. Other approaches similar to quarto documents include R markdown, sweave and org mode."
  },
  {
    "objectID": "python_basic.html#data-structures",
    "href": "python_basic.html#data-structures",
    "title": "5  Python basics",
    "section": "5.1 Data structures",
    "text": "5.1 Data structures\nPython has some more advanced data structures that build on its primitive types.\n\nLists: ordered collections of objects\nSets: like lists but only have unique elements\nTuples: like lists, but not mutable, i.e. need to create a new one to modify\nDictionaries: named elements that you can reference by their name rather than position\n\nFirst, let’s look at some list operations.\n\ndat = [1, 4, 8, 10] # define a list\nprint(dat[0])       # reference an element\nprint(dat[2 : 4])   # reference elements\nprint(dat[2 : ]) \nprint(dat[:2])\ndat2 = [dat, dat]        # creating a list of lists\nprint(dat2)\nprint(dat2[1][2])        # referencing an item in a nested list\ndat3 = [dat2, \"string1\"] # mixed types\nprint(dat3)\ndat4 = dat + dat         # list concatenation\nprint(dat4)\n\n1\n[8, 10]\n[8, 10]\n[1, 4]\n[[1, 4, 8, 10], [1, 4, 8, 10]]\n8\n[[[1, 4, 8, 10], [1, 4, 8, 10]], 'string1']\n[1, 4, 8, 10, 1, 4, 8, 10]\n\n\nNow, let’s look at dictionaries.\n\ndict = {\"a\" : 1, \"b\" : 2} # Create a dictionary of two elements named a and b taking values 1 and 2 respectively\nprint(dict)\nprint(dict['a'])          # reference the element named a\n\n{'a': 1, 'b': 2}\n1\n\n\nSets and tuples are similar to lists, however with some important distinctions. Sets, contain only unique elements and tuples are immutable lists.\n\nset1 = {\"a\", \"b\", \"c\"}\nset2 = {\"a\", 1, True}\nset3 = {\"a\", \"b\", \"c\", \"c\"}\nprint(set1)\nprint(set2)\nprint(set3)\n\n{'c', 'a', 'b'}\n{1, 'a'}\n{'c', 'a', 'b'}\n\n\nHere’s an example to illustrate a tuple.\n\nlist1 = [\"a\", \"b\", \"c\"]\ntuple1 = (\"a\", \"b\", \"c\")\nlist1[0] = \"aa\" #Works just fine\nlist1\n#tuple1[0] = \"aa\" #doesn't work\n\n['aa', 'b', 'c']\n\n\n\n5.1.1 Mutable in immutable entities\nWhen working with objects in python, mutable and immutable elements act differently. Lists are mutable. So, below, the element y gets appended along with x.\n\nx = [10]\ny = x\nx.append(20)\n## Notice y has the appended element\nprint(y)\n## let's try again, as of now x = [10, 20] \nx[0] = x[0] + 11\n## Now x = [21, 20], but did y change?\nprint(y)\n\n[10, 20]\n[21, 20]\n\n\nThings like numbers and strings are immutable. Notice that changing y does not change x.\n\nx = 10\ny = x\nx = x + 10\nprint((x, y))\n\n(20, 10)"
  },
  {
    "objectID": "python_programming.html",
    "href": "python_programming.html",
    "title": "6  Python programming",
    "section": "",
    "text": "# do this if you'd like to prompt for an input\n# x = input(\"are you mean (y/n)? > \")\n# Let's just assume the user input 'n'\nx = 'n'\nif x == 'y': \n print(\"Slytherine!\")\nelse:\n print(\"Gryffindor\")\n\nGryffindor\n\n\nJust to further describe white space useage in python, consider testing whether statementA is True. Below, statementB is executed as part of the if statement whereas statementC is outside of it because it’s not indented. This is often considered an eye rolling aspect of the language, but I think it’s nice in the sense that it bakes good code identation practices into the language.\n## Some more about white space\nif statementA:\n  statementB   # Executed if statementA is True\nstatementC     # Executed regardless since it's not indented\nThe generic structure of if statements in python are\nif statement1 :\n ...\nelif statement2 :\n ...\nelse \n ...\nHere’s an example (note this is just equal to the statement (a < 0) - (a > 0)\n\na = 5\n\nif a < 0 :\n  a = -1\nelif a > 0 :\n  a = 1\nelse :\n  a = 0\n\nprint(a)\n\n1\n\n\nfor and while loops can be used for iteration. Here’s some examples\n\nfor i in range(4) :\n print(i)\n\n0\n1\n2\n3\n\n\n\nx = 4\nwhile x > 0 :\n x = x - 1\n print(x)\n\n3\n2\n1\n0\n\n\nNote for loops can iterate over list-like structures.\n\nfor w in 'word':\n print(w)\n\nw\no\nr\nd\n\n\nThe range function is useful for creating a structure to loop over. It creates a data type that can be iterated over, but isn’t itself a list. So if you want a list out of it, you have to convert it.\n\na = range(3)\nprint(a)\nprint(list(a))\n\nrange(0, 3)\n[0, 1, 2]"
  },
  {
    "objectID": "python_functions.html",
    "href": "python_functions.html",
    "title": "7  Functions",
    "section": "",
    "text": "def pow(x, n = 2):\n  return x ** n\n\nprint(pow(5, 3))\n\n125\n\n\nNote our function has a mandatory arugment, x, and an optional arugment, n, that takes the default value 2. Consider this example to think about how python evaluates function arguments. These are all the same.\n\nprint(pow(3, 2))\nprint(pow(x = 3, n = 2))\nprint(pow(n = 2, x = 3))\n#pow(n = 2, 3) this returns an error, the second position is n, but it's a named argument too\n\n9\n9\n9\n\n\nYou can look here, https://docs.python.org/3/tutorial/controlflow.html, to study the rules. It doesn’t make a lot of sense to get to cute with your function calling arguments. I try to obey both the order and the naming. I argue that this is the way to go since usually functions are written with some sensible ordering of arguments and naming removes all doubt. Python has a special variable for variable length arguments. Here’s an example.\n\ndef concat(*args, sep=\"/\"):\n return sep.join(args)  \n\nprint(concat(\"a\", \"b\", \"c\"))\nprint(concat(\"a\", \"b\", \"c\", sep = \":\"))\n\na/b/c\na:b:c\n\n\nLambda can be used to create short, unnamed functions. This has a lot of uses that we’ll see later.\n\nf = lambda x: x ** 2\nprint(f(5))\n\n25\n\n\nHere’s an example useage where we use lambda to make specific “raise to the power” functions.\n\ndef makepow(n):\n return lambda x: x ** n\n\nsquare = makepow(2)\nprint(square(3))\ncube = makepow(3)\nprint(cube(2))\n\n9\n8"
  },
  {
    "objectID": "data_cleaning_example.html",
    "href": "data_cleaning_example.html",
    "title": "10  Data cleaning, an example",
    "section": "",
    "text": "We’re going to cover data cleaning by an example. Primarily, you’re going to work in pandas, a library for manipulating tabular data."
  },
  {
    "objectID": "data_cleaning_example.html#imports-and-files",
    "href": "data_cleaning_example.html#imports-and-files",
    "title": "10  Data cleaning, an example",
    "section": "10.1 Imports and files",
    "text": "10.1 Imports and files\nThe first thing we’ll try is loading some data and plotting it. To do this, we’ll need some packages. Let’s load up pandas, a package for data management, matplotlib for plotting and numpy for numerical manipulations. The python command for this is import.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\n\nwe say as in order to not have to type out the entire module name to access its methods."
  },
  {
    "objectID": "data_cleaning_example.html#reading-data-in-with-pandas",
    "href": "data_cleaning_example.html#reading-data-in-with-pandas",
    "title": "10  Data cleaning, an example",
    "section": "10.2 Reading data in with pandas",
    "text": "10.2 Reading data in with pandas\nLet’s now read in an MRICloud dataset using pandas. We want to use the function read_csv within pandas. Notice we imported pandas as pd so the command is pd.read_csv. Also, pandas can accept URLs, so we just put the link to the file in the argument. The data we want to read in is in a github repo I created.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/kirby127a_3_1_ax_283Labels_M2_corrected_stats.csv\")\n\nYou can see the variables created with locals. However, this shows you everything and you usually have to text process it a little.\nLet’s look at the first 4 rows of our dataframe. The object dataset is a pandas object with associated methods. One is head which allows one to see the first few rows of data.\n\ndf.head(4)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      rawid\n      roi\n      volume\n      min\n      max\n      mean\n      std\n      type\n      level\n    \n  \n  \n    \n      0\n      1\n      kirby127a_3_1_ax.img\n      Telencephalon_L\n      531111\n      0\n      374\n      128.3013\n      51.8593\n      1\n      1\n    \n    \n      1\n      2\n      kirby127a_3_1_ax.img\n      Telencephalon_R\n      543404\n      0\n      300\n      135.0683\n      53.6471\n      1\n      1\n    \n    \n      2\n      3\n      kirby127a_3_1_ax.img\n      Diencephalon_L\n      9683\n      15\n      295\n      193.5488\n      32.2733\n      1\n      1\n    \n    \n      3\n      4\n      kirby127a_3_1_ax.img\n      Diencephalon_R\n      9678\n      10\n      335\n      193.7051\n      32.7869\n      1\n      1"
  },
  {
    "objectID": "data_sqlite.html#a-more-reaslistic-example",
    "href": "data_sqlite.html#a-more-reaslistic-example",
    "title": "11  SQL via sqlite",
    "section": "11.1 A more reaslistic example",
    "text": "11.1 A more reaslistic example\nLet’s create and work with a more realistic example. Consider the data Opiods in the US at Open Case Studies https://github.com/opencasestudies/ocs-bp-opioid-rural-urban as described here. Read over their writeup, as we’re mostly going to be showing how to duplicate a lot of their steps in sqlite.\nFirst, you need to download the data, which you could do by right clicking and saving the file or with a command:\nwget https://raw.githubusercontent.com/opencasestudies/ocs-bp-opioid-rural-urban/master/data/simpler_import/county_pop_arcos.csv\nwget https://raw.githubusercontent.com/opencasestudies/ocs-bp-opioid-rural-urban/master/data/simpler_import/land_area.csv\nwget https://raw.githubusercontent.com/opencasestudies/ocs-bp-opioid-rural-urban/master/data/simpler_import/county_annual.csv\nNext, let’s import them into sqlite\ncommand prompt> sqlite3 opioid.db\nsqlite> .mode csv\nsqlite> .import county_pop_arcos.csv population\nsqlite> .import county_annual.csv annual\nsqlite> .import land_area.csv land\nsqlite> .tables\nannual      land        population\nWhat variables do the tables include? The pragma command is unique to sqlite and contains a bunch of helper functions.\nsqlite> pragma table_info(population);\ncid  name          type  notnull  dflt_value  pk\n---  ------------  ----  -------  ----------  --\n0                  TEXT  0                    0 \n1    BUYER_COUNTY  TEXT  0                    0 \n2    BUYER_STATE   TEXT  0                    0 \n3    countyfips    TEXT  0                    0 \n4    STATE         TEXT  0                    0 \n5    COUNTY        TEXT  0                    0 \n6    county_name   TEXT  0                    0 \n7    NAME          TEXT  0                    0 \n8    variable      TEXT  0                    0 \n9    year          TEXT  0                    0 \n10   population    TEXT  0                    0 \nsqlite> pragma table_info(annual);\ncid  name          type  notnull  dflt_value  pk\n---  ------------  ----  -------  ----------  --\n0                  TEXT  0                    0 \n1    BUYER_COUNTY  TEXT  0                    0 \n2    BUYER_STATE   TEXT  0                    0 \n3    year          TEXT  0                    0 \n4    count         TEXT  0                    0 \n5    DOSAGE_UNIT   TEXT  0                    0 \n6    countyfips    TEXT  0                    0\nsqlite> pragma table_info(land)\ncid  name         type  notnull  dflt_value  pk\n---  -----------  ----  -------  ----------  --\n0                 TEXT  0                    0 \n1    Areaname     TEXT  0                    0 \n2    STCOU        TEXT  0                    0 \n3    LND010190F   TEXT  0                    0 \n4    LND010190D   TEXT  0                    0 \n5    LND010190N1  TEXT  0                    0\n(I truncated this latter output at 5.)"
  },
  {
    "objectID": "data_sqlite.html#working-with-data",
    "href": "data_sqlite.html#working-with-data",
    "title": "11  SQL via sqlite",
    "section": "11.2 Working with data",
    "text": "11.2 Working with data\nLet’s print out a few columns of the population data.\nsqlite> select BUYER_COUNTY, BUYER_STATE, STATE, COUNTY, year, population from population limit 5;\nBUYER_COUNTY  BUYER_STATE  STATE  COUNTY  year  population\n------------  -----------  -----  ------  ----  ----------\nAUTAUGA       AL           1      1       2006  51328     \nBALDWIN       AL           1      3       2006  168121    \nBARBOUR       AL           1      5       2006  27861     \nBIBB          AL           1      7       2006  22099     \nBLOUNT        AL           1      9       2006  55485   \nThe limit 5 prints out five rows. Let’s perform some of the tasks in the write up. For example, they want to print out some of the missing data in the annual dataset.\nsqlite> select * from annual where countyfips = \"NA\" limit 10;\n     BUYER_COUNTY  BUYER_STATE  year  count  DOSAGE_UNIT  countyfips\n---  ------------  -----------  ----  -----  -----------  ----------\n188  ADJUNTAS      PR           2006  147    102800       NA        \n189  ADJUNTAS      PR           2007  153    104800       NA        \n190  ADJUNTAS      PR           2008  153    45400        NA        \n191  ADJUNTAS      PR           2009  184    54200        NA        \n192  ADJUNTAS      PR           2010  190    56200        NA        \n193  ADJUNTAS      PR           2011  186    65530        NA        \n194  ADJUNTAS      PR           2012  138    57330        NA        \n195  ADJUNTAS      PR           2013  138    65820        NA        \n196  ADJUNTAS      PR           2014  90     59490        NA        \n197  AGUADA        PR           2006  160    49200        NA   \nHere, we used the condition “NA” to test for missingness, since the CSV files have the string NA values for missing data. Places other than Puerto Rico (PR)? Lets check some\nsqlite> select * from annual where countyfips = \"NA\" and BUYER_STATE != \"PR\" limit 10;\n       BUYER_COUNTY  BUYER_STATE  year  count  DOSAGE_UNIT  countyfips\n-----  ------------  -----------  ----  -----  -----------  ----------\n10072  GUAM          GU           2006  319    265348       NA        \n10073  GUAM          GU           2007  330    275600       NA        \n10074  GUAM          GU           2008  313    286900       NA        \n10075  GUAM          GU           2009  390    355300       NA        \n10076  GUAM          GU           2010  510    413800       NA        \n10077  GUAM          GU           2011  559    475600       NA        \n10078  GUAM          GU           2012  616    564800       NA        \n10079  GUAM          GU           2013  728    623200       NA        \n10080  GUAM          GU           2014  712    558960       NA        \n17430  MONTGOMERY    AR           2006  469    175390       NA     \nInspect the missing data further on your own. It looks like its the unincorporated territories and a handful of Arkansas values missing countyfips (Federal Information Processing Standard). Specifically, Montgomery county AR is missing FIPs codes. Since we want to look US states in specific, excluding territories, we will just set the Montgomery county ones to the correct value 05097 and ignore the other missing values.\nsqlite> update annual set countyfips = 05097 where BUYER_STATE = \"AR\" and BUYER_COUNTY = \"MONTGOMERY\"\nsqlite> select * from annual where BUYER_STATE = \"AR\" and BUYER_COUNTY = \"MONTGOMERY\"\n\n       BUYER_COUNTY  BUYER_STATE  year  count  DOSAGE_UNIT  countyfips\n-----  ------------  -----------  ----  -----  -----------  ----------\n17430  MONTGOMERY    AR           2006  469    175390       5097      \n17431  MONTGOMERY    AR           2007  597    241270       5097      \n17432  MONTGOMERY    AR           2008  561    251760       5097      \n17433  MONTGOMERY    AR           2009  554    244160       5097      \nNow lets delete rows from the annual table that have missing county data. Check on these counties before and verify that the’ve been deleted afterwards. Also, we want to grab just three columns from the land table, so let’s create a new one called land_area. Also, the column there is called STCOU, which we want to rename to coutyfips. (I’m going to stop printing out the results of every step, so make sure you’re checking your work as you go.)\nsqlite> delete from annual where BUYER_COUNTY = \"NA\"\nsqlite> create table land_area as select Areaname, STCOU, LND110210D from land;\nsqlite> alter table land_area rename column STCOU to countyfips;\nNext we want to start joining the tables, so let’s left join our table and print out the counts to make sure we accounted correctly.\nsqlite> create table county_info as select * from population left join land_area using(countyfips);\nsqlite> select count(*) from land;\n3198\nsqlite> select count(*) from land_area;\n3198\nsqlite> select count(*) from county_info;\n28265\nsqlite> select count(*) from population;"
  },
  {
    "objectID": "data_sqlite.html#notes",
    "href": "data_sqlite.html#notes",
    "title": "11  SQL via sqlite",
    "section": "11.3 Notes",
    "text": "11.3 Notes\nAt this point, hopefully you have enough of a background to finish doing the example from Open Case Studies. I have to say, that working with SQL is pleasant, but I prefer python as a home base. In addition, after working with the data, I want to use plotting and analysis tools. In the next chapter, we’ll look at using python as a base language to interact with an sqlite database."
  },
  {
    "objectID": "data_sqlite.html#sqlite-in-python",
    "href": "data_sqlite.html#sqlite-in-python",
    "title": "11  SQL via sqlite",
    "section": "11.4 sqlite in python",
    "text": "11.4 sqlite in python\nAn sqlite3 library ships with python. In this tutorial, we’ll discuss how to utilize this library and read sqlite tables into pandas. With this, you can generalize to other python APIs to other databases.\nFirst, let’s continue on with our work from the previous notebook. A nice little tutorial can be found here.\n\nimport sqlite3 as sq3\nimport pandas as pd\n\ncon = sq3.connect(\"sql/opioid.db\")\n# cursor() creates an object that can execute functions in the sqlite cursor\n\nsql = con.cursor()\n\nfor row in sql.execute(\"select * from county_info limit 5;\"):\n    print(row)\n\n    \n# you have to close the connection\ncon.close\n\n('1', 'AUTAUGA', 'AL', '01001', '1', '1', 'Autauga', 'Autauga County, Alabama', 'B01003_001', '2006', '51328', 'Autauga, AL', '594.44')\n('2', 'BALDWIN', 'AL', '01003', '1', '3', 'Baldwin', 'Baldwin County, Alabama', 'B01003_001', '2006', '168121', 'Baldwin, AL', '1589.78')\n('3', 'BARBOUR', 'AL', '01005', '1', '5', 'Barbour', 'Barbour County, Alabama', 'B01003_001', '2006', '27861', 'Barbour, AL', '884.88')\n('4', 'BIBB', 'AL', '01007', '1', '7', 'Bibb', 'Bibb County, Alabama', 'B01003_001', '2006', '22099', 'Bibb, AL', '622.58')\n('5', 'BLOUNT', 'AL', '01009', '1', '9', 'Blount', 'Blount County, Alabama', 'B01003_001', '2006', '55485', 'Blount, AL', '644.78')\n\n\n<function Connection.close()>"
  },
  {
    "objectID": "data_sqlite.html#reading-into-pandas",
    "href": "data_sqlite.html#reading-into-pandas",
    "title": "11  SQL via sqlite",
    "section": "11.5 Reading into pandas",
    "text": "11.5 Reading into pandas\nLet’s read our sqlite database into pandas. At this point, we can then work on the dataset entirely in pandas. This is closest to how I work. I’m typically more comfortable working in R or python and so get my data out of database formats and into tidyverse or pandas formats as soon as I can.\n\ncon = sq3.connect(\"sql/opioid.db\")\n\ncounty_info = pd.read_sql_query(\"SELECT * from county_info\", con)\n\n# you have to close the connection\ncon.close\n\ncounty_info.head\n\n<bound method NDFrame.head of                BUYER_COUNTY BUYER_STATE countyfips STATE COUNTY  \\\n0          1        AUTAUGA          AL      01001     1      1   \n1          2        BALDWIN          AL      01003     1      3   \n2          3        BARBOUR          AL      01005     1      5   \n3          4           BIBB          AL      01007     1      7   \n4          5         BLOUNT          AL      01009     1      9   \n...      ...            ...         ...        ...   ...    ...   \n28260  28261       WASHAKIE          WY      56043    56     43   \n28261  28262         WESTON          WY      56045    56     45   \n28262  28263        SKAGWAY          AK      02230     2    230   \n28263  28264  HOONAH ANGOON          AK      02105     2    105   \n28264  28265     PETERSBURG          AK      02195     2    195   \n\n         county_name                               NAME    variable  year  \\\n0            Autauga            Autauga County, Alabama  B01003_001  2006   \n1            Baldwin            Baldwin County, Alabama  B01003_001  2006   \n2            Barbour            Barbour County, Alabama  B01003_001  2006   \n3               Bibb               Bibb County, Alabama  B01003_001  2006   \n4             Blount             Blount County, Alabama  B01003_001  2006   \n...              ...                                ...         ...   ...   \n28260       Washakie           Washakie County, Wyoming  B01003_001  2014   \n28261         Weston             Weston County, Wyoming  B01003_001  2014   \n28262        Skagway       Skagway Municipality, Alaska  B01003_001  2014   \n28263  Hoonah Angoon  Hoonah-Angoon Census Area, Alaska  B01003_001  2014   \n28264     Petersburg         Petersburg Borough, Alaska  B01003_001  2014   \n\n      population           Areaname LND110210D  \n0          51328        Autauga, AL     594.44  \n1         168121        Baldwin, AL    1589.78  \n2          27861        Barbour, AL     884.88  \n3          22099           Bibb, AL     622.58  \n4          55485         Blount, AL     644.78  \n...          ...                ...        ...  \n28260       8444       Washakie, WY    2238.55  \n28261       7135         Weston, WY    2398.09  \n28262        996        Skagway, AK     452.33  \n28263       2126  Hoonah-Angoon, AK    7524.92  \n28264       3212     Petersburg, AK    3281.98  \n\n[28265 rows x 13 columns]>"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data science and AI for the biological and medical sciences using python",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "data_advanced_databases.html#blockwise-basic-statistical-calculations",
    "href": "data_advanced_databases.html#blockwise-basic-statistical-calculations",
    "title": "12  Big data storage",
    "section": "12.1 Blockwise basic statistical calculations",
    "text": "12.1 Blockwise basic statistical calculations\nNow, consider taking the mean of both variables. Imagine that the time series is so long it’s not feasible to load into memory. So, we want to read it in blocks. You want your blocks to be as big as possible, since that’s fastest. In our case, of course, none of this is necessary.\nOur goal in this section is to do the following: calculate the empirical mean and variance for each sensor, center and scale each sensor, and write those changes to those variables, calculate the sample correlation then calculate the residual for sensor1 given sensor2. (I think typically you wouldn’t want to overwrite the original data; but, this is for pedagogical purposes.) We want our data organized so sensors are stored in a hierarchical “folder” called sensors and processed data is in a different folder.\nWe’re just simulating iid standard normals. So, we have a rough idea of the answers we should get, since the the data are theoretically mean 0, variance 1 and uncorrelated. After our calculations, they will have empirical mean 0 and variance 1 and the empirical correlation between the residual and sensor 2 will be 0.\nLet’s consider a block variation of the inner product. \\[\n&lt;a, b&gt; = \\sum_{i=0}^{n-1} a_i b_i = \\sum_{i=0}^{n/B} \\sum_{j=0}^{B-1} a_{j + i B} b_{j + i B}\n\\] (if \\(n\\) is divisible by \\(B\\). Otherwise you have to figure out what to do with the final block, which isn’t hard but makes the notation messier.) So, for example, the (sample) mean is then \\(&lt;x, J&gt;/n\\) where \\(J\\) is a vector of ones.\nLet’s calculate the mean using blockwise calculations.\n\nn = s1.shape[0]\nB = 32\n## mean center the blocks\nmean1 = 0\nmean2 = 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    mean1 += s1[block_indices].sum() / n \n    mean2 += s2[block_indices].sum() / n\n\n[mean1, mean2]\n\n[-0.022212967172872494, -0.00825148352928065]\n\n\nLet’s now center our time series.\n\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    s1[block_indices] -= mean1  \n    s2[block_indices] -= mean2\n\nNow the (unbiased, sample) variance of centered vector \\(a\\) is simply \\(&lt;a, a&gt;/(n-1)\\).\n\nv1, v2 = 0, 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    v1 += np.sum(s1[block_indices] ** 2) / (n - 1)\n    v2 += np.sum(s2[block_indices] ** 2) / (n - 1)\n[v1, v2]\n\n[0.958662426534898, 1.0333468113266224]\n\n\nNow let’s scale our vectors as\n\nsd1 = np.sqrt(v1)\nsd2 = np.sqrt(v2)\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    s1[block_indices] /= v1  \n    s2[block_indices] /= v2\n\nNow that our vectors are centered and scaled, the empirical correlation is simply \\(&lt;a, b&gt;/(n-1)\\). Let’s do that\n\ncor = 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    cor += np.sum(s1[block_indices] * s2[block_indices]) / (n-1) \ncor\n\n0.014622420886487464\n\n\nFinally, we want to “regress out” s2 from s1. Since we normalized our series, the correlation is slope coefficient from linear regression (regardless of the outcome and dependent variable) and the intercept is zero (since we centered). Thus, the residual we want is \\(e_{12} = s_1 - \\rho s_2\\) where \\(\\rho\\) is the correlation.\n\nf['processed/resid_s1_s2'] = np.empty(n)\ne12 = f['processed/resid_s1_s2']\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    e12[block_indices] += s1[block_indices] - cor * s2[block_indices] \n\nNow we have our new processed data stored in a vector. To close our database simply do:\n\nf.close()\n\nNow our processed data is stored on disk.\n\nf = h5py.File('sensor.hdf5', 'r')\nf['processed/resid_s1_s2']\n\n&lt;HDF5 dataset \"resid_s1_s2\": shape (1024,), type \"&lt;f8\"&gt;\n\n\n\nf.close()"
  },
  {
    "objectID": "data_advanced_databases.html#homework",
    "href": "data_advanced_databases.html#homework",
    "title": "12  Big data storage",
    "section": "12.2 Homework",
    "text": "12.2 Homework\n\nPerform lots of regressions. Suppose that you have a setting where you would like to perform the operation \\[\n(X'X)^{-1} X' Y\n\\] where \\(X\\) is \\(n\\times p\\) and \\(Y\\) is \\(n\\times v\\). Consider the case where \\(Y\\) is very large (so \\(V\\) is large). Simulate some data where you perform this linear model in block calculations.\nWrite a block matrix multiplication program that takes in two matrices with agreeable dimensions stored as HDF5 and multiplies them in block sizes specified by the user."
  },
  {
    "objectID": "statistics_ml.html#linear-regression",
    "href": "statistics_ml.html#linear-regression",
    "title": "23  Maximum Likelihood",
    "section": "23.1 Linear regression",
    "text": "23.1 Linear regression\nYou might be surprised to find out that linear regression can also be cast as a likelihood problem. Consider an instance where we assume that the \\(Y_i\\) are Gaussian with a mean equal to \\(\\beta_0 + \\beta_1 x_i\\) and variance \\(\\sigma^2\\). What that means is that the probability that \\(Y_i\\) lies betweens the points \\(A\\) and \\(B\\) is governed by the equation\n\\[\nP(Y_i \\in [A, B) ~|~ x_i) = \\int_A^B \\exp\\left\\{ -(y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2 \\right\\} dy_i\n\\]\nLetting \\(A=-\\infty\\) and taking the derivative with respect to \\(B\\), we obtain the density function, sort of the probability on an infintessimally small interval:\n\\[\n\\exp\\left\\{ -(y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2 \\right\\}\n\\]\nUses the density evaluated at the observed data, the joint likelihood assuming independence is:\n\\[\n\\prod_{i=1}^n \\exp\\left\\{ -(y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2 \\right\\}\n= \\exp\\left\\{ -\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2 \\right\\}\n\\]\nSince it’s more convenient to deal with logs we get that the joint log likelihood is\n\\[\n- \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2\n\\]\nSince minimizing the negative is the same as maximizing this, and the constants of proportionality are irrelevant for maximizing for \\(\\beta_1\\) and \\(\\beta_0\\), we get that maximum likelihood for these parameters minimizes\n\\[\n\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2\n\\]\nwhich is the same thing we minimized to obtain our least squares regression estimates."
  },
  {
    "objectID": "statistics_causal.html#associational-models-versus-causal-models",
    "href": "statistics_causal.html#associational-models-versus-causal-models",
    "title": "24  Causal DAGs",
    "section": "24.1 Associational models versus causal models",
    "text": "24.1 Associational models versus causal models\nCausal models differ from associational models in that they codify causal directions not just associations. In our program, you might have learned of the use of propensity scores, counterfactuals or randomization to study causality. There, typically the goal is to make causal statements with as few assumptions as possible or at least understanding the assumptions. Typically, the object of study is the estimation of an effect avergated over covarites.\nCausal graphs take a different approach, even if they wind up at the same place. Here, the goal is to postulate hyptothetical causal relationships and use those hypothetical relationships to estimate causal effects.\n\n24.1.1 Graphs and graphical models\nA graph, \\(G\\) is a collection of nodes, say \\(V=\\{1,\\ldots, p\\}\\) and a set edges between the nodes, i.e. a set of elements \\((i,j)\\). The graph is directed if \\((i,j)\\) is considered different then \\((j,i)\\).\nNode \\(i\\) is a parent of node \\(j\\) if \\((i,j) \\in E\\) and \\((j,i)\\notin E\\). Similarly, node \\(i\\) is a child of node \\(j\\) if \\((j,i) \\in E\\) and \\((i,j)\\notin E\\). A node is a descendant of another if it is a child, or a child of a child and so on.\n\n\n24.1.2 DAG and SCMs\nDAGs define a unique factorization (set of independence relationships) with compatible probability models. I find it useful to think of causal DAGs in the terms of structural causal models (SCMs). Such models demonstrate an example of a generative models that statisfy the DAG and the have clear connections with the probabability models. An SCM over a collection of variables, \\(X=(X_1, \\ldots, X_p)\\), postulates a set of functional relationships \\[\nX_j = f(P_j, \\epsilon_j)\n\\] where \\(P_j\\) are the antecedent causes of \\(X_j\\), called the parents of \\(X_j\\), and \\(\\epsilon_j\\) is an accumulation of variables treated as mutally independent. This defines a directed graph, \\(G\\) say, where a graph is collection of vertices corresponding to our variables, \\(V=\\{1,\\ldots, p\\}\\), corresponding to the \\(X_i\\), and edges, \\(E\\), which is a set of ordered pairs of nodes.\n\n\n\n\n\nIn this case, \\(P_1 = \\{\\}\\), \\(P_2 = \\{1\\}\\) and \\(P_3 = \\{1,2\\}\\). DAGs in general define the independence assumptions associated with compatible probability models. SCMs are such an example that clearly define compatible probability models. Of course, given a large enough cross-sectional sample, we can estimate the joint distribution of \\(P(X_1,\\ldots, X_p)\\) and all of its conditionals. Disregarding statistical variation, which can be accounted for using traditional inferential methods, these conditionals should agree with the independence relationships from the DAG, if the DAG is correct. This yields a fruitful way to consider probability models. For example one could use DAGs as a heuristic and see how the observed data agrees with the independence relationships in compatible probability models implied by the DAG.\nBy itself, this does not create any causal claims. However, the following strategy could. Postulate a causal model, like the SCM, consider the independence relationships implied by the SCM, compares those indepnence relationships with those seen in the observed data. This gives us a method to falsify causal models using the data.\nOne specific way in which we use the assumptions is to investigate how the graph changes when we fix a node at a specific value, like an intevention, thus breaking its association with its parents. This operation is conceptual, but at times we can relate probabilities associated with interventions that were not realized. Consider an instrance where where \\(X_1\\) is a collection of confounders, \\(X_2\\) is an exposure and \\(X_3\\) is an outcome. Ideally, we’d like to know \\[\nP(X_3 ~|~ do(X_2) = x_2)\n\\] That is, the impact on the response if we were to set the exposure to \\(e_0\\).\n\n\n24.1.3 Blocking and d-separation\nBefore we talk about interventions, let’s consider discussing compatibility of the hypothetical directed graph and our observed data. Return to our previous diagram.\n\n\\(X1\\) is a confounder betweend \\(X2\\) and \\(X3\\)\n\\(X2\\) is a mediator between \\(X1\\) and \\(X3\\)\n\\(X3\\) is a collider between \\(X1\\) and \\(X2\\)\n\nConsider an example. \\(X1\\) is having a BMI &gt; 35, \\(X2\\) is sleep disordered breathing and \\(X3\\) is hypertension.\n\n\n\n\n\nHere if we’re studying whether SDB causes HTN, BMI35 confounds the relationship as a possible common cause of both. We would need to adjust for BMI35 to make sure the association between SDB and HTN isn’t just due to this common cause.\nIf we were studying whether BMI35 causes HTN, we might be interested in how much of that effect is mediated (indirectly) through SDB and how much is directly from BMI35.\nIf we are studying the relationship between BMI35 and SDB directly, adjusting for HTN may cause an association. Consider the (fictitious) case where there is a large number of people who have SDB who are not obese, yet all have hypertension, for whatever the reason. Then, among the HTN, there could be a negative association between BMI35 and SDB, because of the large collection of patients would who have SDB and are not obese and the same for obese and not hyptertensive. That is, adjusting for HTN created an association. This is an example of Berkson’s paradox. This is a somewhat contrived example, but hopefully you get the point. The wikipedia article has a funny example where they consider \\(X_1\\) is whether or not the hamburger is good at a fast food restaurant, \\(X_2\\) is whether or not the fries are good and \\(X_3\\) is whether or not people eat there. Since few people would eat at a place where both the hamburger and fries are bad, conditioning on \\(X_3\\) can create a negative association.\nThe main point here is that adjusting for colliders may open up a pathway between the nodes.\nA path between two nodes \\(n_1\\) and \\(n_k\\) is a sequence of nodes, \\(v_1, v_2,\\ldots v_{k}\\), where \\(v_{i}\\) and \\(v_{i+1}\\) are connected. The path is directed if \\(v_{i}\\) points to \\(v_{i+1}\\) for \\(i=1,\\ldots,k\\). A graph is a Directed Acyclic Graph (DAG) if all edges are directed and there are no two nodes \\(v_i\\) and \\(v_j\\) with a directed path in both directions.\nA path between \\(v_1\\) and \\(v_k\\), \\(v_1,\\ldots, v_k\\), is blocked by a set of nodes, \\(S\\), if for some \\(v_j\\) in \\(S\\)\n\n\\(v_j\\in S\\) and \\(v_k\\) is a mediator or confounder between \\(v_{j-1}\\) and \\(v_{j+1}\\) in either direction or\n\\(v_j\\notin S\\) and all of the descendants of \\(v_j \\notin S\\) and \\(v_{j}\\) is a collider between \\(v_{j-1}\\) and \\(v_{j+1}\\).\n\nIn other words, a path is blocked if a mediator or confounder is included in \\(S\\) or a collider and all of its descendants is excluded from \\(S\\).\nFor 1. this is equivalent to saying one of\n\n\\(v_{j-1}\\rightarrow v_{j} \\rightarrow v_{j+1}\\)\n\\(v_{j-1}\\leftarrow v_{j} \\leftarrow v_{j+1}\\)\n\\(v_{j-1}\\leftarrow v_{j} \\rightarrow v_{j+1}\\)\n\nholds. For 2. recall a collider is \\(v_{j-1}\\rightarrow v_{j} \\leftarrow v_{j+1}\\).\nThis could be translated into the following statistical statement. Conditioning on a mediator or confounder or not conditioning on a collider blocks a path, conditioning on a collider opens a path.\nWe say that two nodes or groups of nodes are d-separated by a set of nodes, \\(S\\), if every path between nodes in the two groups is blocked by \\(S\\). d-separation is useful because it gives us conditional independence relationships in the sense that if \\(X_i\\) is d-separated with \\(X_j\\) given \\(S\\) then \\(X_i \\perp X_j ~|~ S\\) on all probability distribution compatible with the graph.\nConsider the following graph.\n\n\n\n\n\n\\(X_1\\) and \\(X_5\\) are conditionally indepndent given \\(X_2\\) and \\(X_3\\). Why? Conditioning on \\(X_2\\) blocks the paths \\(X_1 \\leftarrow X_2 \\rightarrow X_3 \\leftarrow X_5\\) even despite the part \\(X_2 \\rightarrow X_3 \\leftarrow X_5\\) is opened by conditioning on the collider, \\(X_3\\). Furthermore, conditioning on \\(X_2\\) or \\(X_3\\) blocks the path \\(X_1 \\leftarrow X_2 \\rightarrow X_3 \\rightarrow X_4 \\rightarrow X_6 \\leftarrow X_5\\). Finally, conditioning on \\(X_3\\) blocks the path \\(X_1 \\leftarrow X_3 \\leftarrow X_5\\).\nAnother interesting one is that \\(X_2\\) and \\(X_5\\) are marginally independent. This is because not conditioining on \\(X_3\\) blocks the path \\(X_2 \\rightarrow X_3 \\leftarrow X_5\\) and not conditioning on \\(X_6\\) blocks the path \\(X_2 \\rightarrow X_3 \\rightarrow X_4 \\rightarrow X_6 \\leftarrow X_5\\).\nHere’s the complete set of conditional independence relationships.\n\n\\(X_1\\) and \\(X_4\\) are d-separated by \\(\\{X_3\\}\\)\n\\(X_1\\) and \\(X_5\\) are d-separated by \\(\\{X_2, X_3\\}\\)\n\\(X_1\\) and \\(X_6\\) are d-separated by \\(\\{X_4, X_5\\}\\), \\(\\{X_3, X_5\\}\\), \\(\\{X_2, X_3\\}\\)\n\\(X_2\\) and \\(X_4\\) are d-separated by \\(\\{X_3\\}\\)\n\\(X_2\\) and \\(X_5\\) are d-separated by \\(\\{\\}\\) (the null set, i.e. they’re marginally independent).\n\\(X_2\\) and \\(X_6\\) are d-separated by \\(\\{X_4, X_5\\}\\), \\(\\{X_3, X_5\\}\\)\n\\(X_3\\) and \\(X_6\\) are d-separated by \\(\\{X_4, X_5\\}\\)\n\\(X_4\\) and \\(X_5\\) are d-separated by \\(X_3\\).\n\nThese all imply the independence relationships, such as \\(X_1 \\perp X_4 ~|~ X_3\\)."
  },
  {
    "objectID": "statistics_causal.html#do-calculus-and-backdoor-criterion",
    "href": "statistics_causal.html#do-calculus-and-backdoor-criterion",
    "title": "26  Causal DAGs",
    "section": "26.2 Do calculus and backdoor criterion",
    "text": "26.2 Do calculus and backdoor criterion\nRecall that specifying a causal graph implies the independence relationships of a probability distribution under assumptions such as the SCM. Sometimes, we’re interested in the causal relationship between an exposure, \\(X\\) and an outcome, \\(Y\\). Consider a theoretical intervention obtained by setting \\(X = a\\), which we write as \\(do(X) = a\\). We want to estimate \\(P(Y ~|~ do(X) = a)\\).\nA set \\(Z\\) satisfies the back door criterion with respect to nodes \\(X\\) and \\(Y\\) if\n\nNo descendant of \\(X\\) is in \\(Z\\).\n\\(Z\\) blocks every path \\(X\\) and \\(Y\\) that contains an arrow pointing to \\(X\\).\n\nThe back door criteria is similar to d-separation. However, we only focus on arrows pointing into \\(X\\) and don’t allow for descendants of \\(X\\).\nThe magic of the back door adjustment comes from the relationship, the adjustment formula:\n\\[\nP(Y ~|~ do(X) = x) = \\sum_{z\\in S} P(y ~ | x, z) p(z)\n\\]\nwhere \\(S\\) satisfies the back door criterion. If the \\(z\\) are all observed variables, then the RHS of this equation is estimable. Note the interesting statement that not all variables need to be observed, just \\(y\\), \\(x\\) and \\(z\\).\nSo, in our previous example, adjusting for \\(S = \\{X_2, X_3\\}\\) allows us to estimate the causal effect of \\(X\\) on \\(Y\\), even if \\(X_4\\) and \\(X_5\\) are not measured.\nIt’s important to emphasize, that every aspect of the adjustment formula is theoretically estimable if \\(Y\\), \\(X\\) and the nodes in \\(S\\) are observed.\nConsider the following graph.\n\n\n\n\n\nHere are the minimal backdoor adjustment variables between \\(X\\) and \\(Y\\):\n\n\\(S = \\{X_2, X_3\\}\\)\n\\(S = \\{X_3, X_5\\}\\)\n\\(S = \\{X_4, X_5\\}\\)\n\nHere are some invalid backdoor sets of variables.\n\n\\(S\\) equal to any single node.\n\n\\(S=\\{X_3\\}\\) does not block the path \\(X\\leftarrow X_2 \\rightarrow X_3 \\leftarrow X_5 \\rightarrow Y\\).\n\\(S=\\{X_4\\}\\) or \\(S=\\{X_2\\}\\) does not block the path \\(X \\leftarrow X_3 \\leftarrow X_5 \\rightarrow Y\\).\n\\(S=\\{X_5\\}\\) does not block the path \\(X \\leftarrow X_3 \\leftarrow X_4 \\rightarrow Y\\).\n\n\\(S=\\{X_3, X_4\\}\\) does not block the path \\(X \\leftarrow X_2 \\rightarrow X_3 \\leftarrow X_5 \\rightarrow Y\\).\n\\(S=\\{X_2, X_4\\}\\) does not block the path \\(X\\leftarrow X_3 \\leftarrow X_5 \\rightarrow Y\\).\n\n\n26.2.1 Example graphs\nIn all the following, we’re interested in the causal effect of \\(X\\) and \\(Y\\). \\(Z\\) variables are observed and \\(U\\) variables are unobserved. Every variable is binary to make the discussion easier.\n\n26.2.1.1 Randomization\nIf \\(X\\) is randomized and everyone takes the treatment assigned to them (left plot) then \\(X\\) has no parents other than the randomization mechanism,\\(R\\). We’re omitting any descendants of \\(X\\) since we don’t have to worray about them. Regardless of the complexity of the relationship between the collection of observed, unobserved, known and unknown variables, \\(Z, U\\), and \\(Y\\) we can estimate the causal effect simply without conditioning on anything.\nIn contrast, if some people ignore their randomized treatment status and elect to choose a different treatment one may have opened a backdoor path (right plot). For example, if the treatment can’t be blinded and those randomized to the control with the worst baseline symptoms elect to obtain the treatment elsewhere.\n\n\n\n\n\n\n\n26.2.1.2 Simple confounding\nThe diagram below shows classic confounding. Conditioning in \\(Z\\) allows for the estimation of the causal effect.\n\n\n\n\n\nNow the estimate of the adjusted effect (under our assumptions) is\n\\[\nP(Y ~|~ do(X) = x) = P(Y ~|~ X=x, z = 0)P(z = 0) + P(Y ~|~ X=x, Z=1)P(Z=1)\n\\]\nIn the following two examples, the unmeasured confounder \\(U\\) can be controlled for by conditioning on \\(Z\\) and exactly the same estimate can be used as in the simple confounding model.\n\n\n(-0.3, 1.3)\n\n\n\n\n\n\n\n26.2.1.3 Mediation\nIn mediation, all or part of the effect of \\(X\\) on \\(Y\\) flows through yet another variable \\(Z\\).\n\n\n\n\n\nThe backdoor criteria does not apply here, since \\(Z\\) is a descendant of \\(X\\). To answer the question “What is the causal effect of \\(X\\) on \\(Y\\)?” one need not adjust. However, mediation is typically studied in a different way. Instead, one asks questions such as “How much of the effect of \\(X\\) on \\(Y\\) flows or doesn’t flow through \\(Z\\)?”. To answer this question, one usually conditions on \\(Z\\) for a different goal than the backdoor adjustment is accomplishing.\nCinelli, Forney, and Pearl (2021) shows an interesting example of mediation where one would want to adjust for \\(Z\\) (left plot below).\n\n\n(-0.3, 1.3)\n\n\n\n\n\nIn this case, \\(M\\) still mediates the relationship between \\(X\\) and \\(Y\\). However, \\(Z\\) is in a backdoor path to \\(M\\). So, some of the variation in \\(M\\) that impacts \\(Y\\) could be due to \\(Z\\) rather than \\(X\\). The right plot is similar and makes the point more explicit. \\(Z\\) confounds the relationship between \\(X\\) and \\(Y\\) through \\(M\\). Without adjusting for \\(Z\\), the path \\(X\\leftarrow Z \\rightarrow M \\rightarrow Y\\) remains unblocked.\n\n\n26.2.1.4 Bad controls\nThe following are all unhelpful for conditioning on \\(Z\\) using the backdoor criteria.\nUpper left. Adjusting for colliders is the standard bad control. Below adjusting for \\(Z\\) open ups a backdoor path that was closed. From a common sense perspective, why would you want to adjust for a consequence of \\(X\\) and \\(Y\\) when exploring their relationship?\nIn the upper right diagram below, \\(Z\\) is a so-called instrumental variable. A good example is \\(Z\\) being the randomization indicator and \\(X\\) being the treatment the person actually took. It is important in this example to emphasize that use of the instrumental variable is often a very fruitful method of analysis. However, it’s not a useful backdoor adjustment and conditioning on \\(Z\\) simply removes most of the relevant variation in \\(X\\). If one wants to use \\(Z\\) as an instrumental variable in this setting, then specific methods taylored to instrumental variable use need to be employed.\nIn the lower left plot, \\(Z\\) is a descendant of \\(X\\). Conditioning on \\(Z\\) removes relevant pathway information regarding the relationship between \\(X\\) and \\(Y\\)>\nThe lower right plot is similar. Conditioning on \\(Z\\) removes variation in \\(M\\) which hinders our ability to study the relationship between \\(X\\) and \\(Y\\) through \\(M\\).\n\n\n\n\n\n\n\n26.2.1.5 Conditioning may help\nIn the upper left plot, adjusting for \\(Z\\) may reduce variability in \\(Y\\) to help focus on the relationship between \\(X\\) and \\(Y\\).\nIn the upper left plot, adjusting for \\(Z\\) may reduce variation in the mediator unrelated to the relationship between \\(X\\) and \\(Y\\).\n\n\n(-0.3, 1.3)"
  },
  {
    "objectID": "statistics_causal.html#exercises",
    "href": "statistics_causal.html#exercises",
    "title": "26  Causal DAGs",
    "section": "26.3 Exercises",
    "text": "26.3 Exercises\n\n26.3.1 Graphs\nConsider the following graph where we want to answer the question: what is \\(P(Y ~|~ do(X) = x)\\) where every variable is binary.\n\n\n(-0.3, 1.3)\n\n\n\n\n\n\nWhat are the minimal set of adjustment variables for the backdoor criteria?\nIs \\(X \\perp Y ~|~ Z_1, Z_2\\)?\nIs \\(X \\perp Y ~|~ Z_2, Z_3\\)?\nGiven a cross sectional sample, if \\(Z_3\\) is unobserved, give a formula for the estimation of \\(P(Y ~|~ do(X) = x)\\) that only requires observed variables.\n\n\n\n\n\n\n\nWhat are the minimal set of adjustment variables for the backdoor criteria?\nGiven a cross sectional sample, give a formula for the estimation of \\(P(Y ~|~ do(X) = x)\\) that only requires observed variables.\n\n\n\n26.3.2 Data exercise\nThe wikipedia page on Simpson’s paradox gives this data concerning two treatments of kidney stones, the percentage of succcessful procedures and the size of the stone. Note, among both large stones and small stones A is better than B. However, among all B is preferable to A.\n\n\n\nSize\nTreatment\nSuccess\nN\nProp\n\n\n\n\nLarge\nA\n192\n263\n73%\n\n\n\nB\n55\n80\n69%\n\n\nSmall\nA\n81\n87\n93%\n\n\n\nB\n234\n270\n87%\n\n\nBoth\nA\n273\n350\n78%\n\n\n\nB\n289\n350\n83%\n\n\n\nEstimate the treatment effect difference: \\[\nP(Success ~|~ Do(Treatment) = B)\n- P(Success ~|~ Do(Treatment) = A)\n\\] under the following graphical models where \\(X\\) is treatment, \\(Y\\) is success and \\(Z\\) is stone size:\n\n\n\n\n\nComment on how reasonable each of these models are given the setting. Here’s a reference: Julious and Mullee (1994).\nGive any other DAGs, perhaps including unmeasured variables, that you think are relevant."
  },
  {
    "objectID": "statistics_causal.html#reading",
    "href": "statistics_causal.html#reading",
    "title": "26  Causal DAGs",
    "section": "26.4 Reading",
    "text": "26.4 Reading\n\nThe definitive causal reference is Pearl (2009).\nI got a lot of this stuff from Peters, Janzing, and Schölkopf (2017), which you can read here\nAlso read Hardt and Recht (2021), which you can read here\nA crash course in good and bad controls, or here\ndagitty\n\n\n\n\n\nCinelli, Carlos, Andrew Forney, and Judea Pearl. 2021. “A Crash Course in Good and Bad Controls.” Sociological Methods & Research, 00491241221099552.\n\n\nHardt, Moritz, and Benjamin Recht. 2021. “Patterns, Predictions, and Actions: A Story about Machine Learning.” arXiv Preprint arXiv:2102.05242.\n\n\nJulious, Steven A, and Mark A Mullee. 1994. “Confounding and Simpson’s Paradox.” Bmj 309 (6967): 1480–81.\n\n\nPearl, Judea. 2009. Causality. Cambridge university press.\n\n\nPeters, Jonas, Dominik Janzing, and Bernhard Schölkopf. 2017. Elements of Causal Inference: Foundations and Learning Algorithms. The MIT Press."
  },
  {
    "objectID": "supervised_binary_classification.html",
    "href": "supervised_binary_classification.html",
    "title": "25  Introduction to binary classification",
    "section": "",
    "text": "26 Classification\nLet’s try creating the simplest possible classifier, a threshold. So here we want to pick the value of the threshold so that lower values are classified GOLD_Lesion == 0 (i.e. no lesion) and higher values are GOLD_Lesion == 1 (lesion at this voxel). We want to do this on labeled voxels so that we can pick a meaningful threshold on voxels without a gold standard labeling. That is, for new patients we want to automatically label their images one voxel at a time with a simple thresholding rule. We’re going to use our training data where we know the truth to develop the threshold.\nNote the idea behind doing this is only useful if the new images without the gold standard are in the same units as the old one, which is not usually the case for MRIs. The technique for trying to make images comparable is called normalization.\nLet’s first just try each of the datapoints itself as a threshold and pick which one does best. However, I’m going to break the data into a training and testing set. The reason for this is that I want to make sure that I don’t overfit. That is, we’re going to test our algorithm on a dataset that wasn’t used to train the algorithm.\nx = dat.FLAIR\ny = dat.GOLD_Lesions\nn = len(x)\ntrainFraction = .75\n\n## Build a training and testing set\n## Prob of being in the train set is trainFraction\nsample = np.random.uniform(size = n) &lt; trainFraction\n\n## Get the training and testing sets\nxtrain = x[ sample]\nytrain = y[ sample]\nxtest =  x[~sample]\nytest =  y[~sample]\n## Starting values, just set it to \n## 0 so that it improves on the first\n## try\nbestAccuracySoFar = 0\n\nfor t in np.sort(xtrain):\n  ## Strictly greater than the threshold is\n  ## our algorithm\n  predictions = (xtrain &gt; t)\n  accuracy = np.mean(ytrain == predictions)\n  if (accuracy &gt; bestAccuracySoFar):\n    bestThresholdSoFar = t \n    bestAccuracySoFar = accuracy \n\nthreshold = bestThresholdSoFar\nNow let’s test our our “algorithm”, on the test set. We’ll look at the test set accuracy, but also how it breaks up into the sensisitivity and specificity."
  },
  {
    "objectID": "supervised_binary_classification.html#definitions",
    "href": "supervised_binary_classification.html#definitions",
    "title": "25  Introduction to binary classification",
    "section": "26.1 Definitions",
    "text": "26.1 Definitions\ntest set accuracy = proportion of correct classifications on the test data\ntest set sensitivity = proportion declared diseased among those that are actually diseased. (In this case lesion = disease)\ntest set specificity = proportion declared not diseased among those that are actually not diseased.\nTo interpret the sensitivity and specificity, imagine setting the threshold nearly to zero. Then we’ll declare almost every voxel a lesion and we’ll have nearly 100% sensitivity and nearly 0% specificity. If we declare a voxel as a lesion it’s not that interesting. If we declare a voxel as not lesions, then it’s probably not a lesion.\nIf we set the threshold really high, then we’ll have nearly 0% sensitivity and 100% specificity. If we say a voxel is not lesioned, it’s not that informative, since we declare nearly everything not a lesion. But if we declare a voxel a lesion, it usually is.\nSo, if you have a high sensitivity, it’s good for ruling diseases out. If you have a high specificity it’s good for ruling diseases in. If you have a high both? Then you have a very good test.\n\n## Let's test it out on the test set\ntestPredictions = (xtest &gt; threshold)\n\n## The test set accuracy\ntestAccuracy = np.mean(testPredictions == ytest)\n\n## Let's see how it specifically does on the\n## set of instances where ytest == 0 and ytest == 1\n## The % it gets correct on ytest == 0 is called\n## the specificity and the percent correct when \n## ytest == 1 is called the sensitivity.\nsub0 = ytest == 0\nsub1 = ytest == 1\n\ntestSpec = np.mean(ytest[sub0] == testPredictions[sub0])\ntestSens = np.mean(ytest[sub1] == testPredictions[sub1])\n\npd.DataFrame({\n 'Threshold': threshold,\n 'Accuracy': testAccuracy, \n 'Specificity': testSpec, \n 'Sensitivity': testSens}, index = [0])\n\n\n\n\n\n\n\n\nThreshold\nAccuracy\nSpecificity\nSensitivity\n\n\n\n\n0\n1.907889\n0.586207\n0.9375\n0.153846\n\n\n\n\n\n\n\n\nsns.kdeplot(x0, shade = True, label = 'Gold Std = 0')\nsns.kdeplot(x1, shade = True, label = 'Gold Std = 1')\nplt.axvline(x=threshold)\n            \nplt.show()\n\n/tmp/ipykernel_73405/2742163068.py:1: FutureWarning:\n\n\n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n\n/tmp/ipykernel_73405/2742163068.py:2: FutureWarning:\n\n\n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n\n\n\n\n\n\nOK, so out plot has better sensitivity than specificity and a test set accuracy of around 68%. The lower specificity is because there’s a lower percentage of blue below the line than orange above the line. Recall, we’re saying above the threshold is a lesion and orange is the distribution for voxels with lesions.\nSo, for this algorithm, the high sensitivity says that all else being equal, if you declare a voxel as not being a lesion, it probably isn’t. In other words, if you’re out in the lower part of the orange distribution, there’s a lot of blue there.\nHowever, all else isn’t equal. Most voxels aren’t lesions. This factors into our discussion in a way that we’ll discuss later.\n\nfpr, tpr, thresholds = roc_curve(ytest, xtest)\nroc_auc = auc(fpr, tpr)\n\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "supervised_regression_origin.html",
    "href": "supervised_regression_origin.html",
    "title": "27  Regression through the origin",
    "section": "",
    "text": "\\[\nl = \\sum_i ( Y_i - \\beta X_i)^2 = || Y - \\beta X||^2.\n\\]\nTaking a derivative of \\(l\\) with respect to \\(\\beta\\) yields\n\\[\nl' = - \\sum_i 2 (Y_i - \\beta X_i) X_i.\n\\]\nIf we set this equal to zero and solve for beta we obtain the classic solution:\n\\[\n\\hat \\beta = \\frac{\\sum_i Y_i X_i}{\\sum_i X_i^2} = \\frac{<Y, X>}{||X||^2}.\n\\]\nNote further, if we take a second derivative we get\n\\[\nl'' = \\sum_i 2 x_i^2  \n\\]\nwhich is strictly positive unless all of the \\(x_i\\) are zero (a case of zero variation in the predictor where regresssion is uninteresting). Regression through the origin is a very useful version of regression, but it’s quite limited in its application. Rarely do we want to fit a line that is forced to go through the origin, or stated equivalently, rarely do we want a prediction algorithm for \\(Y\\) that is simply a scale change of \\(X\\). Typically, we at least also want an intercept. In the example that follows, we’ll address this by centering the data so that the origin is the mean of the \\(Y\\) and the mean of the \\(X\\). As it turns out, this is the same as fitting the intercept, but we’ll do that more formally in the next section.\nFirst let’s load the necessary packages.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nNow let’s download and read in the data.\n\ndat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv\")\ndat.head() \n\n\n\n\n\n  \n    \n      \n      FLAIR\n      PD\n      T1\n      T2\n      FLAIR_10\n      PD_10\n      T1_10\n      T2_10\n      FLAIR_20\n      PD_20\n      T1_20\n      T2_20\n      GOLD_Lesions\n    \n  \n  \n    \n      0\n      1.143692\n      1.586219\n      -0.799859\n      1.634467\n      0.437568\n      0.823800\n      -0.002059\n      0.573663\n      0.279832\n      0.548341\n      0.219136\n      0.298662\n      0\n    \n    \n      1\n      1.652552\n      1.766672\n      -1.250992\n      0.921230\n      0.663037\n      0.880250\n      -0.422060\n      0.542597\n      0.422182\n      0.549711\n      0.061573\n      0.280972\n      0\n    \n    \n      2\n      1.036099\n      0.262042\n      -0.858565\n      -0.058211\n      -0.044280\n      -0.308569\n      0.014766\n      -0.256075\n      -0.136532\n      -0.350905\n      0.020673\n      -0.259914\n      0\n    \n    \n      3\n      1.037692\n      0.011104\n      -1.228796\n      -0.470222\n      -0.013971\n      -0.000498\n      -0.395575\n      -0.221900\n      0.000807\n      -0.003085\n      -0.193249\n      -0.139284\n      0\n    \n    \n      4\n      1.580589\n      1.730152\n      -0.860949\n      1.245609\n      0.617957\n      0.866352\n      -0.099919\n      0.384261\n      0.391133\n      0.608826\n      0.071648\n      0.340601\n      0\n    \n  \n\n\n\n\nIt’s almost always a good idea to plot the data before fitting the model.\n\nx = dat.T2\ny = dat.PD\nplt.plot(x, y, 'o')\n\n\n\n\nNow, let’s center the data as we mentioned so that it seems more reasonable to have the line go through the origin. Notice here, the middle of the data, both \\(Y\\) and \\(X\\), is right at (0, 0).\n\nx = x - np.mean(x)\ny = y - np.mean(y)\nplt.plot(x, y, 'o')\n\n\n\n\nHere’s our slope estimate according to our formula.\n\nb = sum(y * x) / sum(x ** 2 )\nb\n\n0.7831514763656\n\n\nLet’s plot it so to see how it did. It looks good. Now let’s see if we can do a line that doesn’t necessarily have to go through the origin.\n\nplt.plot(x, y, 'o')\nt = np.array([-1.5, 2.5])\nplt.plot(t, t * b)"
  },
  {
    "objectID": "supervised_regression.html#some-definitions",
    "href": "supervised_regression.html#some-definitions",
    "title": "28  Prediction with regression",
    "section": "28.1 Some definitions",
    "text": "28.1 Some definitions\n\nThe covariance is defined as \\(Cov(X,Y) = \\sum_{i=1}^n (Y_i - \\bar Y) (X_i - \\bar X) / (N-1)\\)\nThe standard deviation of \\(X\\) is \\(SD_X\\), \\(\\sqrt{Cov(X, X)}\\)\nThe Pearson correlation is defined as \\(\\frac{Cov(X, Y)}{SD_X \\times SD_Y}\\)\n\nThe Pearson correlation measures the degree of linear association between two variables where neither is thought of as an outcome or predictor. It is a unit free quantity. If you just say “correlation” without further context, it’s understood to mean the Pearson correlation. The covariance measures the same thing, though it has the units of the units X times the units of Y. The sample standard deviation of X has the units of X and measures the spread, or variability, of X. The variance, \\(Cov(X, X)\\), is simply the square of the standard deviation and has units of X squared.\n\nx = dat['T2']\ny = dat['PD']\ntrainFraction = 0.75\n\n## Hold out data\nsample = np.random.uniform(size = 100) < trainFraction\nxtrain = x[ sample]\nytrain = y[ sample]\nxtest =  x[~sample]\nytest =  y[~sample]\n\n## get the slope on the training data\nbeta1 = st.pearsonr(xtrain, ytrain)[0] * np.std(ytrain) / np.std(xtrain)\nbeta0 = np.mean(ytrain) - np.mean(xtrain) * beta1\nprint([beta0, beta1])\n \nsns.scatterplot(x = xtrain, y = ytrain)\n## add a line\nsns.lineplot(x=xtrain, y=beta0 + beta1 * xtrain)\n\n[0.2793288719945098, 0.7972004540065037]\n\n\n<AxesSubplot:xlabel='T2', ylabel='PD'>\n\n\n\n\n\n\nprint(st.linregress(x = xtrain, y = ytrain))\nsns.regplot(x=xtrain, y=ytrain)\n\nLinregressResult(slope=0.7972004540065036, intercept=0.2793288719945098, rvalue=0.8321691075820031, pvalue=2.445944963849776e-19, stderr=0.06395141121272586, intercept_stderr=0.06362373499098695)\n\n\n<AxesSubplot:xlabel='T2', ylabel='PD'>\n\n\n\n\n\nLet’s now calculate our predictions on the test set. Recall, the test set was not used to come up with estimates of \\(\\beta_0\\) and \\(\\beta_1\\). We’ll show the training MSE and the testing MSE as well as a plot of the test set actual Ys versus the predicted ones.\n\nyhat_test = beta0 + beta1 * xtest\nyhat_train = beta0 + beta1 * xtrain\n\n## claculate the MSE in the training and test sets\nprint([ np.mean( (ytrain - yhat_train) ** 2), \n        np.mean( (ytest -  yhat_test) ** 2 ) ])\n\n \nsns.scatterplot(x = yhat_test, y = ytest)\nplt.xlabel('Predicted value from xtest T2 values')\nplt.ylabel('Actual PD value from ytest')\n\n[0.18261297203877797, 0.19154242692920137]\n\n\nText(0, 0.5, 'Actual PD value from ytest')"
  },
  {
    "objectID": "supervised_logistic.html#classification-with-one-continuous-variable",
    "href": "supervised_logistic.html#classification-with-one-continuous-variable",
    "title": "28  Logistic regression",
    "section": "28.1 Classification with one continuous variable",
    "text": "28.1 Classification with one continuous variable\nSuppose now that we want to predict the gold standard from the FLAIR values. Fitting a line seems weird, since the outcome can only be 0 or 1. A line would allow for arbitrarily small or large predictions. Similiarly, forcing the prediction to be exactly 0 or 1 leads to difficult optimization problems. A clever solution is to instead model\n\\[\nP(Y_i = 1 ~|~ X_i)\n\\]\nwhere \\(Y_i\\) is the gold standard value (0 or 1 for no lesion or lesion at that voxel, respectively) and \\(X_i\\) is the FLAIR value for voxel \\(i\\). This solves the problem somewhat nicely, but it still leaves some issues unresolved. For example, what does probability even mean in this context? And also probabilities are between 0 and 1, that’s better than exactly 0 or 1, but still would create problems.\nFor the probability, it’s generally a good idea to think about what you’re modeling as random in the context. In this case, we’re thinking of our voxels as a random sample of FLAIR and gold standard voxel values from some population. This is a meaningful benchmark even if it’s not true. We’ll find that often in statistics we model data as if it comes from a probability distribution when we know it didn’t. We simply know that the probability distribution is a useful model for thinking about the problem.\nAs for getting the probabilities from \\([0,1]\\) to \\((-\\infty, \\infty)\\), we need a function, preferably a monotonic one. The generally agreed upon choice is the logit (natural log of the odds) function. The logit function of a probability is defined as\n\\[\n\\eta = \\mathrm{logit}(p) = \\log\\{p / (1 - p)\\}\n\\]\nwhere \\(p\\) is the probability and \\(O = p/(1-p)\\) is called the odds. Note, you can go backwards from odds to probability with the function \\(p = O / (1 + O)\\). Odds are exactly as used in gambling. If the odds of bet at 1 to 99, then you are saying the probability is \\(1 / (99 + 1) = 1\\%\\).\nWhy use odds? There’s a couple of reasons why odds are uniquely interprettable. First, there are specific study designs where odds make more sense than probabilities, particularly retrospective ones. Secondly, odds are unique in binomial models where they work out to be particularly tractible to work with. Finally, odds have a unique gambling interpretation. That is, it gives the ratio of a one dollar risk to the return in a fair bet. (A fair bet is where the expected return is 0.) So, when a horse track gives the odds on a horse to be 99 to 1, they are saying that you would get $99 dollars if you bet one dollar and the horse won. This is an implied probability of 99 / (99 + 1) = 99% that the horse loses and 1% probability that the horse wins. Note they don’t usually express it as a fraction, they usually espress it as value to 1 or 1 to value. So they would say 99 to 1 (odds against) or 1 to 99 (odds for) so you can easily see how much you’d win for a dollar bet.\nYou can go backwards from the logit function to the probability with the expit function. That is, if \\(\\eta\\) is defined as above, then\n\\[\np = \\frac{e^{\\eta}}{1 + e^\\eta} = \\frac{1}{1 + e^{-\\eta}}.\n\\]\nThis is sometimes called the expit function or sigmoid.\nWe model the log of the odds as linear. This is called logistic regression.\n\\[\n\\eta = \\mathrm{logit}\\left\\{ P(Y = 1 ~|~ X) \\right\\}\n= \\beta_0 + \\beta_1 X.\n\\]\nThe nice part about this model is that \\(e^\\beta_1\\) has the nice interpretation of the odds ratio associated with a one unit change in \\(X\\).\nThis is great, but we still need a function of the probabilities to optimize. We’ll use the cross entropy.\n\\[\n-\\sum_{i=1}^n \\left[Y_i \\log\\{P(Y_i = 1 ~|~ X_i)\\} + (1 - Y_i) \\log\\{1 - P(Y_i = 1 ~|~ X_i)\\}\\right].\n\\]\nThis function has the interpretation of being the negative of the log of the probabilities assuming the \\(Y_i\\) are independent. This model doesn’t have to hold for the minimization to be useful.\nPlugging our logit model in, the cross entropy now looks like\n\\[\n-\\sum_{i=1}^n \\left[\n  Y_i \\eta_i + \\log\\left\\{\\frac{1}{1 + e^\\eta_i} \\right\\} \\right].\n\\]\nThis is the function that we minimize to perform logistic regression. Later on, we’ll worry about how to minimize this function. However, today, let’s fit logistic regression to some data.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model as lm\n## this sets some style parameters\nsns.set()\n\n## Read in the data and display a few rows\ndat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv\")\n\n\n## Plot the data\nsns.scatterplot(x = 'FLAIR', y = 'GOLD_Lesions', data = dat)\n\n&lt;AxesSubplot: xlabel='FLAIR', ylabel='GOLD_Lesions'&gt;\n\n\n\n\n\nLet’s now fit the model. Again we’re going to split into training and test data. But, now we’re not going to do it manually since we have to load a library that has a function to do this.\n\nx = dat[['FLAIR']]\ny = dat.GOLD_Lesions\ntrainFraction = .75\n\n## Once again hold out some data\nsample = np.random.uniform(size = 100) &lt; trainFraction\nxtrain = x[ sample]\nytrain = y[ sample]\nxtest =  x[~sample]\nytest =  y[~sample]\n\n\nlr = lm.LogisticRegression(fit_intercept=True, penalty='none')\nfit = lr.fit(xtrain, ytrain)\n\n/home/bcaffo/anaconda3/envs/ds4bio/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning:\n\n`penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n\n\n\nLet’s look at the parameters fit from the model\n\nbeta0, beta1 = [fit.intercept_[0], fit.coef_[0][0]]\n[beta0, beta1]\n\n[-4.1747144327516095, 2.5536395771485134]\n\n\n\nn = 1000\nxplot = np.linspace(-1, 5, n)\neta = beta0 + beta1 * xplot\np = 1 / (1 + np.exp(-eta))\n\nsns.scatterplot(x = 'FLAIR', y = 'GOLD_Lesions', data = dat, hue = 'GOLD_Lesions')\nsns.lineplot(x = xplot, y = p)\n\n## Of course, scikit has a predict\n## function so that you don't have to do this manually\n#yplot = fit.predict_proba(xplot.reshape((n, 1)))\n#sns.lineplot(xplot, yplot[:, 1])\n\n&lt;AxesSubplot: xlabel='FLAIR', ylabel='GOLD_Lesions'&gt;\n\n\n\n\n\nNow let’s evaluate the test set.\n\n## This predicts the classes using a 50% probability cutoff\nyhat_test = fit.predict(xtest)\n\n## double checking that if you want\n#all(yhat_test == (fit.predict_proba(xtest)[:, 1] &gt; .5))\n\naccuracy = np.mean(yhat_test == ytest)\nsensitivity = np.mean(yhat_test[ytest == 1] == ytest[ytest == 1])\nspecificity = np.mean(yhat_test[ytest == 0] == ytest[ytest == 0])\nnp.round([accuracy, sensitivity, specificity], 3)\n\narray([0.655, 0.632, 0.7  ])\n\n\n\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\n\nptest = fit.predict_proba(xtest)[:, 1]\nfpr, tpr, thresholds = roc_curve(ytest, ptest)\nroc_auc = auc(fpr, tpr)\nplt.figure()\nlw = 2 \nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "supervised_multivariable.html#aside-different-python-packages",
    "href": "supervised_multivariable.html#aside-different-python-packages",
    "title": "30  Linear separable models",
    "section": "30.1 Aside different python packages",
    "text": "30.1 Aside different python packages\nSo far we’ve explored several plotting libraries including: default pandas methods, matplotlib, seaborn and plotly. We’ve also looked at several fitting libraries including to some extent numpy, but especially scikitlearn and statsmodels. What’s the difference? Well, these packages are all mantained by different people and have different features and goals. For example, scikitlearn is more expansive than statsmodels, but statsmodels functions more like one is used to with statistical output. Matplotlib is very expansive, but seaborn has nicer default options and is a little easier. So, when doing data science with python, one has to get used to trying out a few packages, weighing the cost and benefits of each, and picking one.\n‘statsmodels’, what we’re using above, has multiple methods for fitting binary models including: sm.Logit, smf.logit, BinaryModel and glm. Here I’m just going to use Logit which does not use the formula syntax of logit. Note, by default, this does not add an intercept this way. So, I’m adding a column of ones, which adds an intercept.\nConsider the following which uses the formula API\n\nresults = smf.logit(formula = 'GOLD_Lesions ~ FLAIR + T1 + T2 + FLAIR_10 + T1_10 + T2_10 + FLAIR_20', data = trainingDat).fit()\nresults.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.233582\n         Iterations 9\n\n\n\n\nLogit Regression Results\n\n  Dep. Variable:     GOLD_Lesions     No. Observations:        75  \n\n\n  Model:                 Logit        Df Residuals:            67  \n\n\n  Method:                 MLE         Df Model:                 7  \n\n\n  Date:            Sat, 30 Dec 2023   Pseudo R-squ.:       0.6630  \n\n\n  Time:                11:13:49       Log-Likelihood:      -17.519 \n\n\n  converged:             True         LL-Null:             -51.979 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        2.440e-12\n\n\n\n\n               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept    -2.2650     2.012    -1.126  0.260    -6.209     1.679\n\n\n  FLAIR         1.3765     1.250     1.102  0.271    -1.073     3.826\n\n\n  T1            4.2686     1.583     2.697  0.007     1.166     7.371\n\n\n  T2            3.1742     1.547     2.052  0.040     0.143     6.205\n\n\n  FLAIR_10      4.9115     4.194     1.171  0.242    -3.309    13.132\n\n\n  T1_10        -0.2656     2.027    -0.131  0.896    -4.238     3.707\n\n\n  T2_10        -7.9347     3.463    -2.291  0.022   -14.723    -1.147\n\n\n  FLAIR_20    -16.3760     8.107    -2.020  0.043   -32.265    -0.487"
  },
  {
    "objectID": "supervised_multivariable.html#a-classic-example",
    "href": "supervised_multivariable.html#a-classic-example",
    "title": "30  Linear separable models",
    "section": "30.2 A classic example",
    "text": "30.2 A classic example\n\nfrom sklearn.linear_model import LinearRegression\n\n\ndat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/swiss.csv\")\ndat.head()\n\n\n\n\n\n  \n    \n      \n      Region\n      Fertility\n      Agriculture\n      Examination\n      Education\n      Catholic\n      Infant.Mortality\n    \n  \n  \n    \n      0\n      Courtelary\n      80.2\n      17.0\n      15\n      12\n      9.96\n      22.2\n    \n    \n      1\n      Delemont\n      83.1\n      45.1\n      6\n      9\n      84.84\n      22.2\n    \n    \n      2\n      Franches-Mnt\n      92.5\n      39.7\n      5\n      5\n      93.40\n      20.2\n    \n    \n      3\n      Moutier\n      85.8\n      36.5\n      12\n      7\n      33.77\n      20.3\n    \n    \n      4\n      Neuveville\n      76.9\n      43.5\n      17\n      15\n      5.16\n      20.6\n    \n  \n\n\n\n\n\ny = dat.Fertility\nx = dat.drop(['Region', 'Fertility'], axis=1)\nfit = LinearRegression().fit(x, y)\nyhat = fit.predict(x)\n[fit.intercept_, fit.coef_]\n\n[66.9151816789687,\n array([-0.17211397, -0.25800824, -0.87094006,  0.10411533,  1.07704814])]\n\n\n\nx2 = x\nx2['Test'] = x2.Agriculture + x2.Examination\nfit2 = LinearRegression().fit(x2, y)\nyhat2 = fit2.predict(x2)\n\n\nplt.plot(yhat, yhat2)\n\n\n\n\n\nx3 = x2.drop(['Agriculture'], axis = 1)\nfit3 = LinearRegression().fit(x3, y)\nyhat3 = fit3.predict(x3)\nplt.plot(yhat, yhat3)"
  },
  {
    "objectID": "supervised_lm_interpretation.html",
    "href": "supervised_lm_interpretation.html",
    "title": "30  Regression interpretation",
    "section": "",
    "text": "Let’s consider how adjustment works in regression by considering a so called ANCOVA (analysis of covariance) setting. Imagine, there’s treatment variable that we’re ineterested in, \\(T_i\\), and a regression variable that we have to adjust for, \\(X_i\\). Consider this specific variation of this setting:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 T_i + \\epsilon_i\n\\]\nTo tie ourselves down with a context, consider \\(Y_i\\) is blood pressure, \\(T_i\\) is a medication and \\(X_i\\) is BMI. Let’s look at different settings that could arise using plots.\nSince I’m going to be making the same plot over and over, I defined a function that\n\nfit the ANCOVA model using sklearn\nplotted the data as \\(X\\) versus \\(Y\\) with orange versus blue for treated versus not\nadded the fitted ANCOVA lines plus the marginal means (the means for each group disregarding \\(X\\)) as horizontal lines\n\nNote, the adjusted estimated treatment effect is the difference between the two parallel sloped lines. The unadjusted estimated treatment effect is the difference between the two horizontal lines. Let’s look at how adjustment changes things depending on the setting. First we’ll do our imports and then define a function that will make our plot for us and fit the ANCOVA model.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport copy\n\nsns.set()\n\n\ndef myplot(x, y, t):\n    x1 = x[t == 1]\n    x0 = x[t == 0]\n    y1 = y[t == 1]\n    y0 = y[t == 0]\n    xm1 = np.mean(x1)\n    xm0 = np.mean(x0)\n    ym1 = np.mean(y1)\n    ym0 = np.mean(y0)\n\n    X = np.array([x, t]).transpose()\n    out = LinearRegression().fit(X, y)\n    b0hat = out.intercept_\n    b1hat = out.coef_[0]\n    b2hat = out.coef_[1]\n    \n    plt.scatter(x0, y0)\n    plt.scatter(x1, y1)\n\n    col = sns.color_palette()\n\n    plt.axhline(y = ym0, c = col[0])\n    plt.axhline(y = ym1, c = col[1])\n\n    xlim = [np.min(x), np.max(x)]\n\n    ylim0 = [z * b1hat + b0hat + b2hat for z in xlim]\n    ylim1 = [z * b1hat + b0hat         for z in xlim]\n\n    plt.plot( xlim, ylim1)\n    plt.plot( xlim, ylim0) \n\n    plt.show()\n\nLet’s consider out model with \\(\\beta_0 = 0\\), \\(\\beta_1 = 1\\) and \\(\\beta_2 = 4\\). So the treated have an intercept 4 units higher. Let’s consider simulating from this model where the treatment is randomized.\n\nn = 100\nx = np.random.normal(size = n)\ne = np.random.normal(size = n)\nt = np.random.binomial(1, .5, n)\n\nbeta0 = 0\nbeta1 = 1\nbeta2 = 4\n\ny = beta0 + beta1 * x + beta2 * t + e\n\nmyplot(x, y, t)\n\n\n\n\nNotice that the marginal means (horizontal lines) are about 4 units appart, same as the lines. This is due to the randomization. A goal of randomization is to make our inference for the treatment unrelated to whether or not we adjust for the confounding variable (\\(X\\)). So, we get (up to random error) the ssame answer whether we adjust for \\(X\\) or not. Let’s consider a different setting.\n\nmyplot(x + t * 4, y, t)\n\n\n\n\nNow notice that there is a large unadjusted difference (difference between the horizontal lines) whereas there is not much of a difference between the lines. That is, when adjusting for \\(X\\), the relationship goes away. Of note, treatment assignment is highly related to the \\(X\\) variable. Orange dots tend to have a larger \\(X\\) value than the blue. Because of this, there’s pratically no area of overlap between the orange and the blue to directly compare them. The adjusted model is all model, extrapolating the blue line up to the orange and the orange down to the blue assuming that they’re parallel.\n\nmyplot(x + t * 4, y  - t * 4, t)\n\n\n\n\nAbove notice that the result is the reverse. There’s little association marginally, but a large one when conditioning. Let’s look at one final case.\n\nmyplot(x + t * 6, y  - t * 2, t)\n\n\n\n\nAbove things are even worse, the relationship has reversed itself. The marginal association is that the orange is above the blue whereas the conditional association is that the blue is above the orange. That is, if you fit the treatment model without \\(X\\) you get one answer, and with \\(X\\) you get the exact opposite answer! This is an example of so-called “Simpsons paradox”. The “paradox” isn’t that paradoxical. It simply says the relationship between two variables could reverse itself when factoring in another variable. Once again, note there’s no overlap in the distributions."
  },
  {
    "objectID": "supervised_dft.html#abstract-notations",
    "href": "supervised_dft.html#abstract-notations",
    "title": "30  DFT",
    "section": "30.1 Abstract notations",
    "text": "30.1 Abstract notations\n\n30.1.1 History\nThe Fourier transform is one of the key tools in Biomedical Data Science. Its namesake is Jean Baptiste Fourier, who was a 18th century French mathemetician who made fundamental discoveries into harmonic analysis. Its fair to say that Fourier’s discoveries are some of the most fundamental in all of a mathematics and engineering and is the foundation for signal processing.\nOne of his main discoveries was the Fourier series, the idea that a function can be decomposed into building blocks of trigonometric functions.\n\n\n30.1.2 Some notation\nLet \\(&lt;,&gt;\\) be a so-called inner product. For example \\(&lt;a, b&gt; = \\sum_{m=1}^n a_m b_m\\) if \\(a\\) and \\(b\\) are two vectors. But, \\(&lt;a, b&gt;=\\int_0^1 a(t)b(t)dt\\) if \\(a\\) and \\(b\\) are two functions on \\([0,1]\\). (There is a nice generality between Fourier results on data and Fourier results on functions and other spaces. However, we’ll largely focus on discrete data, so think of the first definition.) We can define the norm as \\(&lt;a, a&gt; = ||a||^2\\), so that, the distance between two vectors is \\(||a-b||\\).\nConsider a basis, that is a set of vectors, \\(b_k\\) so that \\(||b_k|| = 1\\) and \\(&lt;b_k, b_j&gt;= I(k=j)\\) and the set of vectors, \\({\\cal H}\\), that can be written as \\(\\sum_{k=1}^k b_k c_k\\) for some constants \\(c_k\\), then for any element \\(x\\in H\\) we have that the best approximation using any subset of the indices, \\(S\\), is of the form\n\\[\n\\sum_{k\\in S} b_k &lt;b_k, x&gt;.\n\\]\nFor real vectors and the basis we consider, every vector can be written as a sum of the basis elements. You can have weird functions that can’t be written out as sums of the basis elements, but they’re weird functions."
  },
  {
    "objectID": "supervised_dft.html#more-practically",
    "href": "supervised_dft.html#more-practically",
    "title": "31  DFT",
    "section": "31.2 More practically",
    "text": "31.2 More practically\nThe basis we’re interested in is \\(b_k\\) which has element \\(m\\) equal to \\(e^{-2\\pi i m k/n} = \\cos(2\\pi mk/n) + i \\sin (2\\pi mk / n)\\) for \\(k=0,..., n-1\\). Here, notice, we quit using the index \\(i\\) since now it stands for the complex unit. This basis satisfies our rules of \\(<b_k, b_j> = I(j=k)\\) and having norm 1. So that, given any vector \\(x\\), our best approximation to it is\n\\[\n\\sum_{k=0}^{n-1} b_k <b_k, x> = \\sum_{k=0}^{n-1} b_k F_k\n\\]\nwhere\n\\[\nF_k = \\sum_{m=0}^{n-1} x_m e^{-2\\pi i m k / n}\n= \\sum_{m=0}^{n-1} x_m [\\cos(2\\pi m k / n) + i \\sin(2\\pi m k / n)].\n\\]\nThe collection of elements, \\(F = (F_0, \\ldots F_{n-1})\\) are called the (discrete) Fourier coeficients and the operation that takes \\(x\\) and converts it into \\(F\\) is called the (discrete) Fourier transform.\nLet’s consider the case where \\(x=(1 ~4 ~9 ~16)'\\). So then\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nx = np.arange(1,5,1) ** 2\nt = np.arange(0, 4, 1)\nn = 4\nF0 = np.sum(x * np.exp(-2 * 1j * np.pi * t * 0 / n))\nF1 = np.sum(x * np.exp(-2 * 1j * np.pi * t * 1 / n))\nF2 = np.sum(x * np.exp(-2 * 1j * np.pi * t * 2 / n))\nF3 = np.sum(x * np.exp(-2 * 1j * np.pi * t * 3 / n))\n\nnp.round([F0, F1, F2, F3], 3)\n\narray([ 30. +0.j,  -8.+12.j, -10. -0.j,  -8.-12.j])\n\n\n\nF = np.fft.fft(x)\nF\n\narray([ 30. +0.j,  -8.+12.j, -10. +0.j,  -8.-12.j])\n\n\nLet’s give a more realistic example. Consider two cosine waves, one fast, one slow. Let’s add them together and see if the FFT can figure out what we’ve done.\n\nn = 1000\nt = np.arange(0, n, 1)\nc1 = np.cos(2 * np.pi * t * 5 / n)\nc2 = np.cos(2 * np.pi * t * 20 / n)\nplt.plot(t, c1)\nplt.plot(t, c2)\nplt.show\n\n<function matplotlib.pyplot.show(close=None, block=None)>\n\n\n\n\n\n\nx = c1 + .5 * c2\nplt.plot(t, x)\nplt.show\n\n<function matplotlib.pyplot.show(close=None, block=None)>\n\n\n\n\n\n\na = np.fft.fft(x)\nb = a.real ** 2 + a.imag ** 2\nplt.plot(b)\nplt.show()\nnp.where(b > 1e-5)\n\n\n\n\n(array([  5,  20, 980, 995]),)\n\n\n\n31.2.1 Some notes\nWe can go backwards from the Fourier coefficients to the signal using the inverse transform. Also, for real signals sometimes people will multiply the signal by \\((-1)^t\\) in order for the plot of the norm of the coeficients (the power spectrum as its called) to look nicer.\n\na = np.fft.fft(x * (-1) ** t)\nb = a.real ** 2 + a.imag ** 2\nplt.plot(b)\nplt.show()\n\n\n\n\n\na = np.fft.fft(x)\nb = np.fft.ifft(a)\n\nplt.plot(b)\nplt.show()\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/matplotlib/cbook/__init__.py:1298: ComplexWarning:\n\nCasting complex values to real discards the imaginary part\n\n\n\n\n\n\n\n\n31.2.2 Filtering\nFiltering is the process of allowing certain frequency bands to be retained while others to be discarded. Imagine in our case that we want the low frequency band to pass and to get rid of the higher frequency. In this case we want a low pass filter. There’s a lot of ways to filter signals, but let’s just do it by simple thresholding. The slightly tricky thing about this in practical problems, is making sure that you’re filtering at the frequencies that you want to. As an example, we have 1,000 time points. Say one time point is 1/100 of a second so that we have ten second of data. We have two cosine functions, one that is at 5 oscillations per 10 seconds (0.5 Hz) and one at 20 oscillations per 10 seconds (2 hz). Let’s filter out anything ove 0.5 Hz.\n\n## demonstrating hard filtering\na = np.fft.fft(x)\nn = a.size\ntimestep = 1/100\n## a function that shows what the frequencies are in the units you want\nw = np.fft.fftfreq(n, timestep)\n\nb = a\nb[(abs(w) > .5)] = 0\nc = np.fft.ifft(b).real\nplt.plot(c)\nplt.show()"
  },
  {
    "objectID": "supervised_multivariable.html#regression-interpretation",
    "href": "supervised_multivariable.html#regression-interpretation",
    "title": "30  Linear separable models",
    "section": "30.3 Regression interpretation",
    "text": "30.3 Regression interpretation\nLet’s consider how adjustment works in regression by considering a so called ANCOVA (analysis of covariance) setting. Imagine, there’s treatment variable that we’re ineterested in, \\(T_i\\), and a regression variable that we have to adjust for, \\(X_i\\). Consider this specific variation of this setting:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 T_i + \\epsilon_i\n\\]\nTo tie ourselves down with a context, consider \\(Y_i\\) is blood pressure, \\(T_i\\) is a medication and \\(X_i\\) is BMI. Let’s look at different settings that could arise using plots.\nSince I’m going to be making the same plot over and over, I defined a function that\n\nfit the ANCOVA model using sklearn\nplotted the data as \\(X\\) versus \\(Y\\) with orange versus blue for treated versus not\nadded the fitted ANCOVA lines plus the marginal means (the means for each group disregarding \\(X\\)) as horizontal lines\n\nNote, the adjusted estimated treatment effect is the difference between the two parallel sloped lines. The unadjusted estimated treatment effect is the difference between the two horizontal lines. Let’s look at how adjustment changes things depending on the setting. First we’ll do our imports and then define a function that will make our plot for us and fit the ANCOVA model.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport copy\n\nsns.set()\n\n\ndef myplot(x, y, t):\n    x1 = x[t == 1]\n    x0 = x[t == 0]\n    y1 = y[t == 1]\n    y0 = y[t == 0]\n    xm1 = np.mean(x1)\n    xm0 = np.mean(x0)\n    ym1 = np.mean(y1)\n    ym0 = np.mean(y0)\n\n    X = np.array([x, t]).transpose()\n    out = LinearRegression().fit(X, y)\n    b0hat = out.intercept_\n    b1hat = out.coef_[0]\n    b2hat = out.coef_[1]\n    \n    plt.scatter(x0, y0)\n    plt.scatter(x1, y1)\n\n    col = sns.color_palette()\n\n    plt.axhline(y = ym0, c = col[0])\n    plt.axhline(y = ym1, c = col[1])\n\n    xlim = [np.min(x), np.max(x)]\n\n    ylim0 = [z * b1hat + b0hat + b2hat for z in xlim]\n    ylim1 = [z * b1hat + b0hat         for z in xlim]\n\n    plt.plot( xlim, ylim1)\n    plt.plot( xlim, ylim0) \n\n    plt.show()\n\nLet’s consider out model with \\(\\beta_0 = 0\\), \\(\\beta_1 = 1\\) and \\(\\beta_2 = 4\\). So the treated have an intercept 4 units higher. Let’s consider simulating from this model where the treatment is randomized.\n\nn = 100\nx = np.random.normal(size = n)\ne = np.random.normal(size = n)\nt = np.random.binomial(1, .5, n)\n\nbeta0 = 0\nbeta1 = 1\nbeta2 = 4\n\ny = beta0 + beta1 * x + beta2 * t + e\n\nmyplot(x, y, t)\n\n\n\n\nNotice that the marginal means (horizontal lines) are about 4 units appart, same as the lines. This is due to the randomization. A goal of randomization is to make our inference for the treatment unrelated to whether or not we adjust for the confounding variable (\\(X\\)). So, we get (up to random error) the ssame answer whether we adjust for \\(X\\) or not. Let’s consider a different setting.\n\nmyplot(x + t * 4, y, t)\n\n\n\n\nNow notice that there is a large unadjusted difference (difference between the horizontal lines) whereas there is not much of a difference between the lines. That is, when adjusting for \\(X\\), the relationship goes away. Of note, treatment assignment is highly related to the \\(X\\) variable. Orange dots tend to have a larger \\(X\\) value than the blue. Because of this, there’s pratically no area of overlap between the orange and the blue to directly compare them. The adjusted model is all model, extrapolating the blue line up to the orange and the orange down to the blue assuming that they’re parallel.\n\nmyplot(x + t * 4, y  - t * 4, t)\n\n\n\n\nAbove notice that the result is the reverse. There’s little association marginally, but a large one when conditioning. Let’s look at one final case.\n\nmyplot(x + t * 6, y  - t * 2, t)\n\n\n\n\nAbove things are even worse, the relationship has reversed itself. The marginal association is that the orange is above the blue whereas the conditional association is that the blue is above the orange. That is, if you fit the treatment model without \\(X\\) you get one answer, and with \\(X\\) you get the exact opposite answer! This is an example of so-called “Simpsons paradox”. The “paradox” isn’t that paradoxical. It simply says the relationship between two variables could reverse itself when factoring in another variable. Once again, note there’s no overlap in the distributions."
  },
  {
    "objectID": "supervised_dft.html#regression-and-ffts",
    "href": "supervised_dft.html#regression-and-ffts",
    "title": "31  DFT",
    "section": "31.3 Regression and FFTs",
    "text": "31.3 Regression and FFTs\nRecall regression through the origin. If \\(y\\) and \\(x\\) are \\(n\\)-vectors of the same length, the minimizer of\n\\[\n||y - \\beta x ||^2\n\\]\nis \\(\\hat \\beta = <x, y> / ||x||^2\\). Note, if \\(||x|| = 1\\) then the estimate is just \\(\\hat \\beta = <x, y>\\). Now consider a second variable, \\(w\\), such that \\(<x, w> = 0\\) and \\(||w|| = 1\\). Consider now the least squares model\n\\[\n||y - \\beta x - \\gamma w||^2.\n\\]\nWe argued that the best estimate for \\(\\beta\\) now first gets rid of \\(w\\) be regressing it out of \\(y\\) and \\(x\\). So, consider that\n\\[\n||y - <w, y> w - \\beta (x - <w, x> w)||^2 =\n||y - <w, y> w - \\beta x||^2.\n\\]\nThus, now the best estimate of \\(\\beta\\) is\n\\[\n<y - <w, y> w, x> = <y, x>.\n\\]\nOr, in other words, if \\(x\\) and \\(w\\) are orthogonal then the coefficient estimate for \\(x\\) with \\(w\\) included is the same as the coefficient of \\(x\\) by itself. This extends to more than two regressors.\nIf you have a collection of \\(n\\) mutually orthogonal vectors of norm one, they are called an orthonormal basis. For an orthonomal basis, 1. the coefficients are just the inner products between the regressors and the outcome and 2. inclusion or exclusion of other elemenents of the basis doesn’t change a basis elements estimated coefficients.\nIt’s important to note, that this works quite generally. For example, for complex numbers as well as real. So, for example, consider the possibility that \\(x\\) is \\(e^{-2\\pi i m k / n}\\) for \\(m=0,\\ldots, n-1\\) for a particular value of \\(k\\). Vectors like this are orthogonal for different values of \\(k\\) and all have norm 1. We have already seen that the Fourier coefficient is\n\\[\nf_k = <y, x> = \\sum_{m=0}^{n-1} y_m e^{-2\\pi i m k / n} =\n\\sum_{m=0}^{n-1} y_m \\cos(-2\\pi m k / n) + i \\sum_{m=0}^{n-1} y_m \\sin(-2\\pi m k / n)\n\\]\nwhere \\(y_m\\) is element \\(m\\) of \\(y\\). Thus, the Fourier coefficients are exactly just least squares coefficients applied in the complex space. Thus we have that\n\\[\nf_k = a_k + i b_k\n\\]\nwhere \\(a_k\\) and \\(b_k\\) are the coefficients from linear models with just the sine and cosine terms. Of course, we don’t actually fit Fourier transforms this way, since there’s a much faster way to do, aptly named the fast Fourier transform (FFT). However, knowing how fast discrete Fourier transforms relate to linear models allows us to use them in creative ways, like putting them into models with other covariates, or in logistic regression models.\nLet’s numerically look at FFTs and linear models using covid case counts in Italy as an example.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n\ndat = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\ndat.head()\n\n\n\n\n\n  \n    \n      \n      Province/State\n      Country/Region\n      Lat\n      Long\n      1/22/20\n      1/23/20\n      1/24/20\n      1/25/20\n      1/26/20\n      1/27/20\n      ...\n      2/28/23\n      3/1/23\n      3/2/23\n      3/3/23\n      3/4/23\n      3/5/23\n      3/6/23\n      3/7/23\n      3/8/23\n      3/9/23\n    \n  \n  \n    \n      0\n      NaN\n      Afghanistan\n      33.93911\n      67.709953\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      209322\n      209340\n      209358\n      209362\n      209369\n      209390\n      209406\n      209436\n      209451\n      209451\n    \n    \n      1\n      NaN\n      Albania\n      41.15330\n      20.168300\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      334391\n      334408\n      334408\n      334427\n      334427\n      334427\n      334427\n      334427\n      334443\n      334457\n    \n    \n      2\n      NaN\n      Algeria\n      28.03390\n      1.659600\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      271441\n      271448\n      271463\n      271469\n      271469\n      271477\n      271477\n      271490\n      271494\n      271496\n    \n    \n      3\n      NaN\n      Andorra\n      42.50630\n      1.521800\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      47866\n      47875\n      47875\n      47875\n      47875\n      47875\n      47875\n      47875\n      47890\n      47890\n    \n    \n      4\n      NaN\n      Angola\n      -11.20270\n      17.873900\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      105255\n      105277\n      105277\n      105277\n      105277\n      105277\n      105277\n      105277\n      105288\n      105288\n    \n  \n\n5 rows × 1147 columns\n\n\n\n\n## Get Italy, drop everyrthing except dates, convert to long (unstack converts to tuple)\ny=dat[dat['Country/Region'] == 'Italy'].drop([\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"], axis=1).unstack()\n## convert from tuple to array\ny = np.asarray(y)  \n## get case counts instead of cumulative counts\ny = y[1 : y.size] - y[0 : (y.size - 1)]\n## get the first non zero entry\ny =  y[np.min(np.where(y !=  0)) : y.size]\nplt.plot(y)\n\n\n\n\nLet’s look at a smoothed version of it and then take the residual. The residual is where we’d like to look at some oscillatory behavior.\n\nn = y.size\nt = np.arange(0, n, 1)\nlowess = sm.nonparametric.lowess\nyhat = lowess(y, t, frac=.05,return_sorted=False)\nplt.plot(y)\nplt.plot(yhat)\n\n\n\n\n\n## We're interested in the residual\ne = y - yhat\nplt.plot(e)\n\n\n\n\nLet’s manually create our Fourier bases. We’re just going to pick some periods to investigate. We’ll pick a fast varying and slow varying.\n\n## Create 4 elements\n## Orthonormal basis (note dividing by sqrt(n/2) makes them norm 1)\nc5  = np.cos(-2 * np.pi * t * 5 / n  ) / np.sqrt(n /2)\nc20 = np.cos(-2 * np.pi * t * 20 / n ) / np.sqrt(n /2)\ns5  = np.sin(-2 * np.pi * t * 5  / n  )/ np.sqrt(n /2)\ns20 = np.sin(-2 * np.pi * t * 20 / n  ) / np.sqrt(n /2)\n\n\nfig, axs = plt.subplots(2, 2)\naxs[0,0].plot(t, c5)\naxs[0,1].plot(t, c20)\naxs[1,0].plot(t, s5)\naxs[1,1].plot(t, s20)\nplt.show()\n\n\n\n\nLet’s verify that they are indeed orthonormal. That is, we want to show that \\(<x_i, x_j> = I(i =j)\\). We also show that they are all mean 0.\n\n## Verify that they are orthonormal mean 0, round to 6 decimal places\nnp.around( [\n np.sum(c5),\n np.sum(c20),\n np.sum(s5),\n np.sum(s20),\n np.sum(c5 * c5),\n np.sum(c20 * c20),\n np.sum(s5 * s5),\n np.sum(s20 * s20),\n np.sum(c5 * s5),\n np.sum(c5 * s20),\n np.sum(c5 * c20),\n np.sum(s5 * s20),\n], 6)\n\narray([-0.,  0., -0., -0.,  1.,  1.,  1.,  1.,  0.,  0., -0., -0.])\n\n\nLet’s take the FFT, the fast (discrete) Fourier transform th way one would normally do it. First, we use FFT in numpy. Then, there’s a convenient method, fftfreq, which gives the associated frequencies with each element of the transform. Finally, we plot the spectral density, which is the sum of the real and complex Fourier coefficients. Sorting the elements first is necessary to connect the dots on the plot. Interestingly, once we remove the trend from the Italy data, there’s some very noticeable spikes in the spectral density, which implies large coefficients on that specific frequency. This is possibly some reporting issue.\n\nf = np.fft.fft(e)\nw = np.fft.fftfreq(n)\nind = w.argsort()\nf = f[ind] \nw = w[ind]\nplt.plot(w, f.real**2 + f.imag**2)\n\n\n\n\nNow let’s manually find the coefficients using our constructed bases and the formula that the coefficients.\n\n[\n np.sum(c5 * e) * np.sqrt(n / 2),\n np.sum(c20 * e) * np.sqrt(n / 2),\n np.sum(s5 * e) * np.sqrt(n / 2),\n np.sum(s20 * e) * np.sqrt(n / 2),\n] \n\n[-42402.165397766235,\n -961069.7936856844,\n 62406.76098672958,\n 1902286.8621751778]\n\n\n\nsreg = linear_model.LinearRegression()\nx=np.c_[c5, c20, s5, s20]\nfit = sreg.fit(x, y)\nfit.coef_ * np.sqrt(n/2)\n\narray([ 1222376.2412703 , -1286448.11961955, -4881386.48874237,\n        1827887.03044428])\n\n\n\nx=np.c_[c5, s5]\nfit = sreg.fit(x, y)\nfit.coef_ * np.sqrt(n/2)\n\narray([ 1222376.2412703 , -4881386.48874237])\n\n\n\ntest = np.where( np.abs(f.real / np.sum(c5 * y) / np.sqrt(n / 2) - 1) < 1e-5) \n[test, f.real[test], w[test], 5 / n]\n\n[(array([], dtype=int64),),\n array([], dtype=float64),\n array([], dtype=float64),\n 0.004409171075837742]\n\n\n\nf.imag[test]\n\narray([], dtype=float64)"
  },
  {
    "objectID": "unsupervised_pca_ica.html#pca",
    "href": "unsupervised_pca_ica.html#pca",
    "title": "31  Unsupervised learning",
    "section": "31.1 PCA",
    "text": "31.1 PCA\nLet \\(\\{X_i\\}\\) for \\(i=1,\\ldots,n\\) be \\(p\\) random vectors with means \\((0,\\ldots,0)^t\\) and variance matrix \\(\\Sigma\\). Consider finding \\(v_1\\), a \\(p\\) dimensional vector with \\(||v_1|| = 1\\) so that \\(v_1^t \\Sigma v_1\\) is maximized. Notice this is equivalent to saying we want to maximize \\(\\mathrm{Var}( X_i^t V_1)\\). The well known solution to this equation is that \\(v_1\\) is the first eigenvector of \\(\\Sigma\\) and \\(\\lambda_1 = \\mathrm{Var}( X_i^t V_1)\\) is the associated eigenvalue. If \\(\\Sigma = V^t \\Lambda V\\) is the eigenvalue decomposition of where \\(V\\) are the eigenvectors and \\(\\Lambda\\) is a diagonal matrix of the eigenvalues ordered from greatest to least, then \\(v_1\\) corresponds to the first column of \\(V\\) and \\(\\lambda_1\\) corresponds to the first element of \\(\\Lambda\\). If one then finds \\(v_k\\) as the vector maximizing \\(v_k^t \\Sigma v_k\\) so that \\(v_k^t v_{k'} = I(k=k')\\), then the \\(v_k\\) are the columns of \\(V\\) and \\(v_k^t \\Sigma v_k = \\lambda_k\\) are the eigenvalues.\nNotice:\n\n\\(V \\Sigma V^t = \\Lambda\\) (i.e. \\(V\\) diagonalizess \\(\\Sigma\\))\n\\(\\mbox{Trace}(\\Sigma) = \\mbox{Trace}(\\Sigma V^t V) = \\mbox{Trace}(V \\Sigma V^t) = \\sum \\lambda_k\\) (i.e. the total variability is the sum of the eigenvalues)\nSince \\(V^t V = I\\), \\(V\\) is a rotation matrix. Thus, \\(V\\) rotates \\(X_i\\) in such a way that to maximize variability in the first dimension, then the second dimensions …\n\\(\\mbox{Cov}(X_i^t v_k, x_i^t v_{k'} )= \\mbox{Cov}(X_i^t v_k, x_i^t v_{k'} ) v_k^t \\mbox{Cov}(x_i, x_i^t) v_{k'} = v_k^t V v_{k'} = 0\\) if \\(k\\neq k'\\)\nAnother representation of \\(\\Sigma\\) is \\(\\sum_{k=1}^p \\lambda_i v_k v_k^t\\) by simply rewriting the matrix algebra of \\(V \\Lambda V^t\\).\nThe variables \\(U_i = V X_i\\) then: have uncorrelated elements (\\(\\mbox{Cov}(U_{ik}, U_{ik'}) = 0\\) for \\(k\\neq k'\\) by property 5), have the same total variability as the elements of \\(X_i\\) (\\(\\sum_k \\mbox{Var}(U_{ik}) = \\sum_k \\lambda_k = \\sum_k \\mbox{Var}(X_{ik})\\) by property 2), are a rotation of the \\(X_i\\), are ordered so that \\(U_{i1}\\) has the greatest amount of variability and so on.\n\nNotation:\n\nThe \\(\\lambda_k\\) are simply called the eigenvalues or principal components variation.\n\\(U_{ik} = X_i^t v_k\\) is called the principal component scores.\nThe \\(v_k\\) are called the principal component loadings or weights, with \\(v_1\\) being called the first principal component and so on.\n\nStatistical properties under the assumption that the \\(x_i\\) are iid with mean 0 and variance \\(\\Sigma\\)\n\n\\(E[U_{ik}]=0\\)\n\\(\\mbox{Var}(U_{ik}) = \\lambda_k\\)\n\\(\\mbox{Cov}(U_{ik}, U_{ik'}) = 0\\) if \\(k\\neq k'\\)\n\\(\\sum_{k=1}^p \\mbox{Var}(U_{ik}) = \\mbox{Trace}(\\Sigma)\\).\n\\(\\prod_{k=1}^p \\mbox{Var}(U_{ik}) = \\mbox{Det}(\\Sigma)\\)\n\n\n31.1.1 Sample PCA\nOf course, we’re describing PCA as a conceptual process. We realize \\(n\\) \\(p\\) dimensional vectors \\(x_1\\) to \\(x_n\\), typically organized in \\(X\\) a \\(n\\times p\\) matrix. If \\(X\\) is not mean 0, we typically demean it by calculating \\((I- J(J^t J)^{-1} J') X\\) where \\(J\\) is a vector of ones. Assume this is done. Then \\(\\frac{1}{n-1} X^t X = \\hat \\Sigma\\). Thus, our sample PCA is obtained via the eigenvalue decomposition \\(\\hat \\Sigma = \\hat V \\hat \\Lambda \\hat V^t\\) and our principal components obtained as $ X V$.\nWe can relate PCA to the SVD as follows. Let \\(\\frac{1}{\\sqrt{n-1}} X = \\hat U \\hat \\Lambda^{1/2} \\hat V^t\\) be the SVD of the scaled version of \\(X\\). Then note that \\[ \\hat \\Sigma = \\frac{1}{n-1} X^t X = \\hat V\n\\hat \\Lambda \\hat V^t \\] yields the sample covariance matrix eigenvalue decomposition.\n\n\n31.1.2 PCA with a large dimension\nConsider the case where one of \\(n\\) or \\(p\\) is large. Let’s assume \\(n\\) is large. Then \\[\n\\frac{1}{n-1} X^t X = \\frac{1}{n-1} \\sum_i x_i x_i^t\n\\] As we learned in the chapter on HDF5, we can do sums like these without loading the entirety of \\(X\\) into memory. Thus, in this case, we can calculate the eigenvectors using only the small dimension. If, on the other hand, \\(p\\) is large and \\(n\\) is smaller, then we can calculate the eigenvalue decomposition of \\[\n\\frac{1}{n-1} X X^t = \\hat U \\hat \\Lambda \\hat U^t.\n\\] In either case, whether \\(U\\) or \\(V\\) is easier to get, we can then obtain the other via vectorized multiplication.\n\n\n31.1.3 Simple example\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport numpy.linalg as la\nfrom sklearn.decomposition import PCA\nimport urllib.request\nimport PIL\nimport numpy as np\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.utils.data as data\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.decomposition import FastICA\nfrom tqdm import tqdm\nimport medmnist\nfrom medmnist import INFO, Evaluator\nimport scipy\nimport IPython\n\n\nn = 1000\nmu = (0, 0)\nSigma = np.array([[1, .5], [.5, 1]])\nX = np.random.multivariate_normal( mean = mu, cov = Sigma, size = n)\n\nplt.scatter(X[:,0], X[:,1])\n\n&lt;matplotlib.collections.PathCollection at 0x7fc632bbd390&gt;\n\n\n\n\n\n\nX = X - X.mean(0)\nprint(X.mean(0))\nSigma_hat = np.matmul(np.transpose(X), X) / (n-1) \nSigma_hat\n\n[-1.11022302e-17 -3.61932706e-17]\n\n\narray([[1.05720345, 0.51383452],\n       [0.51383452, 1.03468001]])\n\n\n\nevd = la.eig(Sigma_hat)\nlambda_ = evd[0]\nv_hat = evd[1]\nu_hat = np.matmul(X, np.transpose(v_hat))\nplt.scatter(u_hat[:,0], u_hat[:,1])\n\n&lt;matplotlib.collections.PathCollection at 0x7fc630ed4950&gt;\n\n\n\n\n\nFit using scikitlearn’s function\n\npca = PCA(n_components = 2).fit(X)\nprint(pca.explained_variance_)\nprint(lambda_ )\n\n[1.55989965 0.53198382]\n[1.55989965 0.53198382]\n\n\n\n\n31.1.4 Example\nLet’s consider the melanoma dataset that we looked at before. First we read in the data as we have done before so we don’t show that code.\n\n\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\n\n\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\n\n\nNext, let’s get the data from the torch dataloader format back into an image array and a matrix with the image part (28, 28, 3) vectorized.\n\ndef loader_to_array(dataloader):\n  ## Read one iteration to get data\n  test_input, test_target = next(iter(dataloader))\n  ## Total number of training images\n  n = np.sum([inputs.shape[0] for inputs, targets in dataloader])\n  ## The dimensions of the images\n  imgdim = (test_input.shape[2], test_input.shape[3])\n  images = np.empty( (n, imgdim[0], imgdim[1], 3))\n\n  ## Read the data from the data loader into our numpy array\n  idx = 0\n  for inputs, targets in dataloader:\n    inputs = inputs.detach().numpy()\n    for j in range(inputs.shape[0]):\n      img = inputs[j,:,:,:]\n      ## get it out of pytorch format\n      img = np.transpose(img, (1, 2, 0))\n      images[idx,:,:,:] = img\n      matrix = images.reshape(n, 3 * np.prod(imgdim))\n      idx += 1\n  return images, matrix\n\ntrain_images, train_matrix = loader_to_array(train_loader)\ntest_images, test_matrix = loader_to_array(test_loader)\n\n## Demean the matrices\ntrain_mean = train_matrix.mean(0)\ntrain_matrix = train_matrix - train_mean\ntest_mean = test_matrix.mean(0)\ntest_matrix = test_matrix - test_mean\n\nNow let’s actually perform PCA using scikitlearn. We’ll plot the eigenvalues divided by their sums, \\(\\lambda_k / \\sum_{k'} \\lambda_{k'}\\). This is called a scree plot.\n\nfrom sklearn.decomposition import PCA\nn_comp = 10\npca = PCA(n_components = n_comp).fit(train_matrix)\nplt.plot(pca.explained_variance_ratio_)\n\n\n\n\nOften this is done by plotting the cummulative sum so that you can visualize how much variance is explained by including the top \\(k\\) components. Here I fit 10 components and they explain 85% of the variation.\n\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\n\n\n\n\nNote that the weights from the eigenvectors, \\(V\\), are images. We can plot these as images.\n\neigen_moles = pca.components_\n\n\n\n\n\n\nLet’s project our testing data onto the principal component basis created by our training data and see how it does. Let \\(X_{training} = U \\Lambda^{1/2} V^t\\) is the SVD of our training data. Then, we can convert ths scores, \\(U\\) back to \\(X_{training}\\) with the map \\(U \\rightarrow U \\lambda^{1/2} V\\). Or, if our scores are normalized, \\(U \\Lambda^{1/2}\\) then we simply multiply by \\(V^t\\). If we want to represent \\(X_{training}\\) by a lower dimensional summary, we just keep fewer columns of scores, then multiply by the same columns of \\(V\\). We could write this as \\(U_s = X_{training} V_S \\lambda^{-1/2}_S\\), where \\(S\\) refers to a subset of values of \\(k\\).\nNotice that \\(\\hat X_{training} = U_{S} V^t_S \\Lambda^{-1/2}_S = X_{training} V_S V_S^t\\) , \\(\\Lambda\\) and \\(V\\). Consider then an approximation to \\(X_{test}\\) as \\(\\hat X_{test} = X_{test} V_s V_S^t\\). Written otherwise \\[\n\\hat X_{i,test} = \\sum_{k \\in S} &lt;x_{i,test}, v_k&gt; v_k\n\\] which is the projection of subject \\(i\\)’s features into the linear space spanned by the basis defined by the principal component loadings.\nLet’s try this on our mole data.\n\ntest_matrix_fit = pca.inverse_transform(pca.transform(test_matrix))\nnp.mean(np.abs( test_matrix - test_matrix_fit))\n\n0.03792390424620059"
  },
  {
    "objectID": "unsupervised_pca_ica.html#ica",
    "href": "unsupervised_pca_ica.html#ica",
    "title": "33  Unsupervised learning",
    "section": "33.2 ICA",
    "text": "33.2 ICA\nICA, independent components analysis, tries to find linear transformations of the data that are statistically independent. Usually, independence is an assumption in ICA, not actually embedded in the loss function.\nLet \\(S_t\\) be an \\(\\mathbb{R}^p\\) collection of \\(p\\) source signals. Assume that the underlying signals are independent, so that \\(S_t \\sim F = F_1 \\times F_2 \\times \\ldots \\time f_p\\). Assume that the observed data is \\(X_t = M S_t\\) and \\(X_t \\sim G\\). It is typically assumed that \\(M\\) is invertible so that \\(S_t = M^{-1} X_t\\) and \\(M\\) and \\(M^{-1}\\) are called the mixing and unmixing matrices respectively. Note that, since we observe \\(X_t\\) over many repititions of \\(t\\), we can get an estimate of \\(G\\). Typically, it is also assumed that the \\(X_t\\) are iid over \\(t\\).\nOne way to characterize the estimation problem is to parameterize \\(F_1\\), \\(F_2\\) and \\(F_3\\) and use maximum likelihood, or equivalent [citations]. Another is to minimize some distance between \\(G\\) and \\(F_1\\), \\(F_2\\) and \\(F_3\\). Yet another is to actually maximize independence between the components of \\(S_t\\) using some estimate of independence [cite Matteson].\nThe most popular approaches try to find \\(M^{-1}\\) by maximizing non-Gaussianity. The logic goes that 1) interesting features tend to be non-Gaussian and 2) an appeal to the CLT over signals suggest that the mixed signals should be more Gaussian by being linear combinations of independent things. The latter claim is heuristic relative to the formal CLT. However, maximizing non-Gaussian components tends to work well in practice, thus validating the motivation empirically.\nOne form of ICA maximizes the kurtosis. If \\(Y\\) is a random variable, then \\(E[Y^4] - 3 E[Y^2]\\) is the kurtosis. One could then find \\(M^{-1}\\) that maximizes the empirical kurtosis of the unmixed signals. Another variation of non-Gaussianity maximizes neg-entropy. The neg-entropy of a density \\(h\\) is given by \\[\n- \\int h(y) \\log(h(y)) dy = - E_h[\\log h(Y)] \\] A well known theorem states that the Gaussian distribution has the largest entropy of all distributions with the same variance. Therefore, to maximize non-Gaussianity, we can minmize entropy, or equivalently maximize neg-entropy. We could subtract the entropy of the Gaussian distribution to consider this a cross entropy problem, but that only adds a constant to the loss function. The maximization of neg-entropy can be done many ways. We need the following. For a given \\(M^{-1}\\), estimate \\(G\\) from the collection \\(M^{-1} X_t\\), then calculate the neg-entropy of \\(f_j\\). Use that to then take an opimization step of \\(M\\) is the right direction. Some versions of estimation use a polynomial expansion of the \\(f_j\\), which then typically only requires higher order moments, like kurtosis. Fast ICA is a particular implmementation of maximizing neg-entropy.\nStatistical versions of ICA don’t require \\(M\\) to be invertible. Moreover, error terms can be added in which case you can see the connection between ICA and factor analytic models. However, factor analysis models tend to assume Gaussianity.\n\n33.2.1 Example\nConsider an example that PCA would have somewhat of a hard time with. In this case, our data is from a mixture of normals with half from a normal with a strong positive correlation and half with a strong negative correlation. Because the angle between the two is not 90 degrees PCA has no chance. No rotation of the axes satisfies the obvious structure in this data.\n\nn = 1000\n\nSigma = np.array([[4, 1.8], [1.8, 1]])\na = np.random.multivariate_normal( mean = [0, 0], cov = Sigma, size = int(n/2))\nSigma = np.array([[4, -1.8], [-1.8, 1]])\nb = np.random.multivariate_normal( mean = [0, 0], cov = Sigma, size = int(n/2))\nx = np.append( a, b, axis = 0)\nplt.scatter(x[:,0], x[:,1])\nplt.xlim([-6, 6])\nplt.ylim([-6, 6])\n\n(-6.0, 6.0)\n\n\n\n\n\nLet’s try fast ICA. Notice it comes much closer to discovering the structure we’d like to discover than PCA could. It pulls appart the two components to a fair degree. Also note, there’s a random starting point of ICA, so that I get fairly different fits over re-runs of the algorithm. I had to lower the tolerance to get a good fit.\nIndpendent components are order invariant and sign invariant.\n\ntransformer = FastICA(tol = 1e-7)\nicafit = transformer.fit(x)\ns = icafit.transform(x)\nplt.scatter(s[:,0], s[:,1])\nplt.xlim( [s.min(), s.max()])\nplt.ylim( [s.min(), s.max()])\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:116: ConvergenceWarning:\n\nFastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n\n\n\n(-0.148554484444661, 0.1086886423007589)\n\n\n\n\n\n\n\n33.2.2 Cocktail party example\nThe classic ICA problem is the so called cocktail party problem. In this, you have \\(p\\) sources and \\(p\\) microphones. The microphones each pick up a mixture of signals from the different sources. The goal is to unmix the sources into the components. Independence makes sense in the cocktail party example, since logically conversations would have some independence.\n\nimport audio2numpy as a2n\ns1, i1 = a2n.audio_from_file(\"mp3s/4.mp3\")\ns2, i2 = a2n.audio_from_file(\"mp3s/2.mp3\")\ns3, i3 = a2n.audio_from_file(\"mp3s/6.mp3\")\n\n## Get everything to be the same shape and sum the two audio channels\nn = np.min((s1.shape[0], s2.shape[0], s3.shape[0]))\ns1 = s1[0:n,:].mean(axis = 1)\ns2 = s2[0:n,:].mean(axis = 1)\ns3 = s3[0:n,:].mean(axis = 1)\n\ns = np.array([s1, s2, s3])\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nMix the signals.\n\nw = np.array( [ [.7, .2, .1], [.1, .7, .2], [.2, .1, .7] ])\nx = np.transpose(np.matmul(w, s))\n\nHere’s an example mixed signal\n\nIPython.display.Audio(data = x[:,1].reshape(n), rate = i1)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nNow try to unmix using fastICA\n\ntransformer = FastICA(whiten=True, tol = 1e-7)\nicafit = transformer.fit(x)\n\n\nicafit.mixing_\n\narray([[ 16.57498461, -51.16258194, -12.70572441],\n       [ 31.47210105,  -7.4969284 , -44.07000374],\n       [108.6275997 , -14.14759529,  -5.51719878]])\n\n\nUnmixing matrix\n\nicafit.components_\n\narray([[-0.00262567, -0.00046241,  0.00974038],\n       [-0.02080963,  0.0058129 ,  0.0014911 ],\n       [ 0.00166492, -0.02401025,  0.00670232]])\n\n\nHere’s a scatterplot matrix where the real component is on the rows and the estimated component is on the columns.\n\nhat_s = np.transpose(icafit.transform(x))\n\nplt.figure(figsize=(10,10))\n\nfor i in range(3):\n  for j in range(3):\n    plt.subplot(3, 3, (3 * i + j) + 1)\n    plt.scatter(hat_s[i,:].squeeze(), np.asarray(s)[j,:])\n\n\n\n\nWe can now play the estimated sources and see how they turned out.\n\nfrom scipy.io.wavfile import write\ni = 0\ndata = (hat_s[i,:].reshape(n) / np.max(np.abs(hat_s[i,:]))) * .5\nIPython.display.Audio(data = data, rate = i1)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ni = 1\ndata = (hat_s[i,:].reshape(n) / np.max(np.abs(hat_s[i,:]))) * .5\nIPython.display.Audio(data = data, rate = i1)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ni = 2\ndata = (hat_s[i,:].reshape(n) / np.max(np.abs(hat_s[i,:]))) * .5\nIPython.display.Audio(data = data, rate = i1)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n33.2.3 Imaging example using ICA\nLet’s see what we get for the images. Logically, one would consider voxels as mixed sources and images as the iid replications. But, then the sources would not be images. Let’s try the other dimension and see what we get where subject images are mixtures of source images. This is analogous to finding a soure basis of subject images.\nThis is often done in ICA where people transpose matrices to investigate different problems.\n\ntransformer = FastICA(n_components=10, random_state=0,whiten='unit-variance', tol = 1e-7)\nicafit = transformer.fit_transform(np.transpose(train_matrix))\nicafit.shape\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:116: ConvergenceWarning:\n\nFastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n\n\n\n(2352, 10)"
  },
  {
    "objectID": "nns_intro.html#basics",
    "href": "nns_intro.html#basics",
    "title": "32  Neural networks, introduction",
    "section": "32.1 Basics",
    "text": "32.1 Basics\nLet’s start by relating neural networks to regression. Consider a simple case where we have two nodes, \\(1\\) and \\(X\\) pointing to an outcome \\(Y\\). What does this mean? Let’s first put some context around the problem. Imagine that we want to use a subject’s BMI \\(X\\) to predict their blood pressure, \\(Y\\). This diagram represents that.\n\n\n\n\n\nTo interpret this diagram as a neural network, consider the following rule:\n\n\n\n\n\n\nNote\n\n\n\nParent nodes that point to a child node are multiplied by weights then added together then operated on by an activation function to form the child node.\n\n\nIf the parent nodes point to the outcome, then the nodes are combined the operated on by a known function, called the activation function to form a prediction. So, in this case, this is saying that the intercept (node labeled \\(1\\))times a weight plus BMI (node labeled \\(X\\)) times a different weight get combined to form a prediction for SBP \\(Y\\). Or, in other words\n\\[\n\\hat Y = g(w_0 \\times 1 + w_1 \\times X)\n\\]\nwhere \\(g\\) is a function that we specify. So in this case, if \\(w_0 = 120\\), \\(w_1 = .1\\) and \\(g\\) is an idenity function, \\(g(a) = a\\), and a subject had a BMI of 30, then the prediction would be\n\\[\n\\hat Y = g(120 + .1 * 30) = 120.3\n\\]\nNote \\(g\\) is not shown in the diagram (though maybe you could with the shape of the child node) or something like that0. Also not shown in the daigram is:\n\nThe loss function, i.e. how to measure the different between \\(\\hat Y\\) and \\(Y\\).\nThe way the loss function combines subjects; we have multiple BMIs and SBPs\nHow we obtain the weights, \\(W_0\\) and \\(W_1\\); this is done by minmizing the loss function using an algorithm\n\nSo, imagine the case where \\(g\\) is an identity function, our loss function for different subjects is squared error and we combine different losses by adding them up. Then, our weights are obtained by minmizing\n\\[\n\\sum_{i=1}^N (Y_i - \\hat Y_i)^2\n\\]\nand so, presuming our optimization algorithm works well, it should be idential to linear regression.\nConsider a different setting. Imagine if our \\(Y\\) is 0 or 1 based on whether or not the subject is taking anti-hypertensive mediations. Further, let \\(g\\) be the sigmoid function, \\(g(a) = 1 / \\{1 + \\exp(-a)\\}\\). Our prediction is\n\\[\n\\hat Y = \\{1 + \\exp(-W_0 - W_1 X)\\}^{-1}\n\\]\nwhich is the logistic regression prediction with intercept \\(W_0\\) and slope \\(W_1\\). Consider a case where \\(W_0 = -4\\), \\(W_1 = .1\\) and \\(X=30\\), then our \\(\\hat Y = 1 / \\{1 + \\exp[-(-4 + .1\\times 30)\\}]\\approx .27\\). Thus, this model estimates a 27% probability that a subject with a BMI of 30 has hypertension.\nFurther, if we specify that the loss function is binary cross entropy\n\\[\n- \\sum_{i=1}^n \\{ Y_i \\log(\\hat Y_i) + (1 - Y_i) \\log(1 - \\hat Y_i)\\} / N\n\\]\nthen minmizing our loss function is identical to maximizing the likelihood for logistic regression.\n\n1 / (1 + np.exp(-(-4 + .1 * 30)))\n\n0.2689414213699951"
  },
  {
    "objectID": "nns_intro.html#more-layers",
    "href": "nns_intro.html#more-layers",
    "title": "32  Neural networks, introduction",
    "section": "32.2 More layers",
    "text": "32.2 More layers\nOf course, there’d be no point in using NNs for problems that we can just solve with generalized linear models. NNs get better when we add more layers, since then they can discover interactions and non-linearities. Consider the following model. Notice we quit explicitly adding the bias (intercept) term / node. In general assume the bias term is included unless otherwise specified.\n\n\n\n\n\nUsually, the nodes are added in so called layers. \\((X_1, X_2)\\) is the input layer, \\((H_{11}, H_{12})\\) is the first hidden layer, \\((H_{21}, H_{22})\\) is the second hidden layer and \\(Y\\) is the output layer. Imagine plugging an \\(X_1\\) and \\(X_2\\) into this network. It would feed forward through the network as\n\\[\n\\begin{align}\nH_{11} = & g_1(W_{011} + W_{111} X_1 + W_{211} X_2) \\\\\nH_{12} = & g_1(W_{012} + W_{112} X_1 + W_{212} X_2) \\\\\nH_{21} = & g_2(W_{021} + W_{121} H_{11} + W_{221} H_{12}) \\\\\nH_{22} = & g_2(W_{022} + W_{122} H_{12} + W_{222} H_{12}) \\\\\n\\hat Y = & g_3(W_{031} + W_{131} H_{21} + W_{231} H_{22})\n\\end{align}\n\\]\nwhere \\(g_k\\) are specified activation functions. Typically, we would have a different activation function for the output layer than the others, and the other would have the same activation function. So, for example, if \\(Y\\) was binary, like hypertension diagnosis, then \\(g_1=g_2\\) and \\(g_3\\) would be a sigmoid."
  },
  {
    "objectID": "nns_intro.html#activation-functions",
    "href": "nns_intro.html#activation-functions",
    "title": "32  Neural networks, introduction",
    "section": "32.3 Activation functions",
    "text": "32.3 Activation functions\nThe output activation function tends to be based on the structure of the outcome. For example, a binary outcome would likely have a sigmoidal, or other function from \\(\\mathbb{R}\\) to \\([0, 1]\\) so as to model a probability. Historically, the internal activation functions were binary thresholds. This was owning to the fact that neural networks were models of (biological) neurons and the threshold was a model of an action potential being propigated. However, modern neural networks have less of a direct connection to their biological motivation and other activation functions tend to be used. The most popular right now is the rectified linear unit (RELU) function. This is simply:\n\\[\nRELU(a) = \\left\\{\n\\begin{array}{ll}\na & \\text{if $a&gt;0$} \\\\\n0 & \\text{otherwise}\n\\end{array}\n\\right.\n= a \\times I(a &gt; 0)\n\\]\nPlotted, this is:\n\nplt.plot( [-1, 0, 1], [0, 0, 1], linewidth = 4);\n\n\n\n\nIf a bias term is included, then the fact that the RELU is centered at zero isn’t important, since the intercept term effectively shifts the function around. These kinds of splin terms are incredibly flexible. Just to show you an example, let’s fit the sine function using a collection of shifted RELUs. This is just\n\\[\nY = \\sin(X) + \\epsilon\n\\]\nbeing fit with\n\\[\n\\sum_{i=1}^N \\left\\{ Y_i - W_{021} - \\sum_{j=1}^{d} W_{j21} g(W_{1j1} X_i- W_{0j1}) \\right\\}^2\n\\]\nwhere the \\(W_{kj}\\) are the weights for layer \\(k\\). Below, we’re just setting \\(W_{1j1} = 1\\) and specifying the \\(W_{0j1}\\) at a sequence of values.\n\n## Generate some data, a sine function on 0,4*pi\nn = 1000\nx = np.linspace(0, 4 * np.pi, n)\ny = np.sin(x) + .2 * np.random.normal(size = n)\n\n## Generate the spline regressors\ndf = 30\nknots = np.linspace(x.min(), x.max(), df)\nxmat = np.zeros((n, df))\nfor i in range(0, df): xmat[:,i] = (x - knots[i]) * (x &gt; knots[i])\n\n## Fit them\nfrom sklearn.linear_model import LinearRegression\nyhat = LinearRegression().fit(xmat, y).predict(xmat)\n\n## Plot them versus the data\nplt.plot(x, y);\nplt.plot(x, yhat);\n\n\n\n\nThis corresponds to a network like depicted below if there were \\(d=3\\) hidden nodes, there was a relu activation function at the first layer, then a identity activation function for the output layer and the weights for the first layer are specified.\n\n\n\n\n\nWe can actually fit this function way better using splines and a little bit more care. However, this helps show how even one layer of RELU activated nodes can start to fit complex shapes."
  },
  {
    "objectID": "nns_intro.html#optimization",
    "href": "nns_intro.html#optimization",
    "title": "32  Neural networks, introduction",
    "section": "32.4 Optimization",
    "text": "32.4 Optimization\nOne of the last bits of the puzzle we have to figure out is how to obtain the weights. A good strategy would be to minimize the loss function. However, it’s hard to minmize. If we had a derivative, we could try the following. Let \\(L(W)\\) be the loss function for weights \\(W\\). Note, we’re omitting the fact that this is a function of the data (predictors and outcome) as well, since that’s a set of fixed numbers. Consider updating parameters as\n\\[\nW^{(new)} = W^{(old}) - e * L'(W^{(old)})\n\\]\nWhat does this do? It moves the parameters by a small amount, \\(e\\), called the learning rate, in the direction the opposite of the gradient. Think of a one dimensional convex function. If the derivative at a point is positive, then that point is larger than where the minimum is. Similarily, if the derivative is negative, it’s smaller. So, the idea is to head a small amount in the opposite direction of the derivative. How much? How about along the line of the derivative? That’s all gradient descent does, just in more than one dimension.\nHow do we get the gradient? Consider the following. If \\(X\\) is our vector of predictors and \\(Y\\) is our vector of outputs, a neural network with 3 layers, can be thought of as, where \\(L_k\\) is layer \\(K\\) and \\(W_k\\) are the weights for that layer:\n\\[\nL_3(L_2(L_1(X, W_1), W_2) W_3)\n\\]\nOr a series of function compositions. Recall from calculus, if we want the derivative of composed functions we have a really simple rule called the chain rule:\n\\[\n\\frac{d}{dx}f(g(x)) = f'(g(x)) g'(x)\n\\]\nI.e. if \\(h=f(u)\\) and \\(u = g(x)\\) then \\(\\frac{dh}{dx} = \\frac{dh}{du}\\frac{du}{dx}\\). Thus, characterized this way, the chain rule formally acts like fractions (though this is a symbolic equivalence having entirely different underlying meanings).\nIf we use the chain rule on our composed loss functions, we wind up bookkeeping backwards through our neural network. That is why it’s called backwards propagation (backprop).\nSo, our algorithm goes something like this. Given, \\(W^{(new)}\\), network, \\(\\phi(X, W)\\), which depends on the predictors and the weights and loss, \\(L(Y, \\hat Y)\\), which depends on the observed and predicted outputs.\n\nSet \\(W^{(old)}=W^{(new)}\\)\nCalculate \\(\\hat Y = \\phi(X, W^{(old)})\\) and loss \\(L(Y, \\hat Y)\\).\nUse back propagation to get to get a numerical approximation to \\(\\frac{d}{dW} L\\{Y, \\phi(X, W)\\} |_{W=W^{(old)}} = L'(W^{(old)})\\)\nUpdate \\(W^{(new)} = W^{(old)} - e L'(W^{(old)})\\)\nGo to step 0."
  },
  {
    "objectID": "nns_basic_regression.html",
    "href": "nns_basic_regression.html",
    "title": "33  Basic regression as a NN",
    "section": "",
    "text": "import pandas as pd\nimport torch\nimport statsmodels.formula.api as smf\nimport statsmodels as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n## Read in the data and display a few rows\ndat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv\")\ndat.head(4)\n\n\n\n\n\n\n\n\nFLAIR\nPD\nT1\nT2\nFLAIR_10\nPD_10\nT1_10\nT2_10\nFLAIR_20\nPD_20\nT1_20\nT2_20\nGOLD_Lesions\n\n\n\n\n0\n1.143692\n1.586219\n-0.799859\n1.634467\n0.437568\n0.823800\n-0.002059\n0.573663\n0.279832\n0.548341\n0.219136\n0.298662\n0\n\n\n1\n1.652552\n1.766672\n-1.250992\n0.921230\n0.663037\n0.880250\n-0.422060\n0.542597\n0.422182\n0.549711\n0.061573\n0.280972\n0\n\n\n2\n1.036099\n0.262042\n-0.858565\n-0.058211\n-0.044280\n-0.308569\n0.014766\n-0.256075\n-0.136532\n-0.350905\n0.020673\n-0.259914\n0\n\n\n3\n1.037692\n0.011104\n-1.228796\n-0.470222\n-0.013971\n-0.000498\n-0.395575\n-0.221900\n0.000807\n-0.003085\n-0.193249\n-0.139284\n0\n\n\n\n\n\n\n\n\nsns.scatterplot(x = dat['T2'], y = dat['PD'])\n\n&lt;AxesSubplot: xlabel='T2', ylabel='PD'&gt;\n\n\n\n\n\n\nfit = smf.ols('PD ~ T2', data = dat).fit()\nfit.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nPD\nR-squared:\n0.661\n\n\nModel:\nOLS\nAdj. R-squared:\n0.657\n\n\nMethod:\nLeast Squares\nF-statistic:\n190.9\n\n\nDate:\nSun, 02 Apr 2023\nProb (F-statistic):\n9.77e-25\n\n\nTime:\n10:34:35\nLog-Likelihood:\n-57.347\n\n\nNo. Observations:\n100\nAIC:\n118.7\n\n\nDf Residuals:\n98\nBIC:\n123.9\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.3138\n0.052\n6.010\n0.000\n0.210\n0.417\n\n\nT2\n0.7832\n0.057\n13.815\n0.000\n0.671\n0.896\n\n\n\n\n\n\nOmnibus:\n1.171\nDurbin-Watson:\n1.501\n\n\nProb(Omnibus):\n0.557\nJarque-Bera (JB):\n0.972\n\n\nSkew:\n0.241\nProb(JB):\n0.615\n\n\nKurtosis:\n2.995\nCond. No.\n1.89\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# The in sample predictions\nyhat = fit.predict(dat['T2'])\n\n# Make sure that it's adding the intercept\n#test = 0.3138 + dat['T2'] * 0.7832\n#sns.scatterplot(yhat,test)\n\n## A plot of the in sample predicted values\n## versus the actual outcomes\nsns.scatterplot(x = yhat, y = dat['PD'])\nplt.plot([-1, 3], [-1, 3], linewidth=2)\n\n\n\n\n\nn = dat.shape[0]\n\n## Get the y and x from \nxtraining = torch.from_numpy(dat['T2'].values)\nytraining = torch.from_numpy(dat['PD'].values)\n\n## PT wants floats\nxtraining = xtraining.float()\nytraining = ytraining.float()\n\n## Dimension is 1xn not nx1\n## squeeze the second dimension\nxtraining = xtraining.unsqueeze(1)\nytraining = ytraining.unsqueeze(1)\n\n## Show that everything is the right size\n[xtraining.shape, \n ytraining.shape,\n [n, 1]\n ]\n\n[torch.Size([100, 1]), torch.Size([100, 1]), [100, 1]]\n\n\n\n## Show that linear regression is a pytorch \nmodel = torch.nn.Sequential(\n    torch.nn.Linear(1, 1)\n)\n\n## MSE is the loss function\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\n## Set the optimizer\n## There are lots of choices\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n\n## Loop over iterations\nfor t in range(10000):\n\n    ## Forward propagation\n  y_pred = model(xtraining)\n    \n  ## the loss for this interation\n  loss = loss_fn(y_pred, ytraining)\n\n  #print(t, loss.item() / n)\n\n  ## Zero out the gradients before adding them up \n  optimizer.zero_grad()\n  \n  ## Backprop\n  loss.backward()\n  \n  ## Optimization step\n  optimizer.step()\n\n\nytest = model(xtraining).detach().numpy().reshape(-1)\nsns.scatterplot(x = ytest, y = yhat)\nplt.plot([-1, 3], [-1, 3], linewidth=2)\n\n\n\n\n\nfor param in model.parameters():\n  print(param.data)\n\ntensor([[0.7831]])\ntensor([0.3138])"
  },
  {
    "objectID": "nns_pytorch_example.html",
    "href": "nns_pytorch_example.html",
    "title": "36  Pytorch by example",
    "section": "",
    "text": "import networkx as nx\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn as skl\n\n#plt.figure(figsize=[2, 2])\nG = nx.DiGraph()\nG.add_node(\"X1\",  pos = (0, 2) )\nG.add_node(\"X2\",  pos = (1, 2) )\nG.add_node(\"X3\",  pos = (2, 2) )\nG.add_node(\"X4\",  pos = (3, 2) )\nG.add_node(\"X5\",  pos = (4, 2) )\n\nG.add_node(\"H1\",  pos = (1, 1) )\nG.add_node(\"H2\",  pos = (2, 1) )\nG.add_node(\"H3\",  pos = (3, 1) )\nG.add_node(\"Y\" ,  pos = (2, 0) )\n\nG.add_edges_from([ (\"X1\", \"H1\"), (\"X2\", \"H1\"), (\"X3\", \"H1\"),  (\"X4\", \"H1\"), (\"X5\", \"H1\")])\nG.add_edges_from([ (\"X1\", \"H2\"), (\"X2\", \"H2\"), (\"X3\", \"H2\"),  (\"X4\", \"H2\"), (\"X5\", \"H2\")])\nG.add_edges_from([ (\"X1\", \"H3\"), (\"X2\", \"H3\"), (\"X3\", \"H3\"),  (\"X4\", \"H3\"), (\"X5\", \"H3\")])\nG.add_edges_from([ (\"H1\",  \"Y\"), (\"H2\",  \"Y\"), (\"H3\", \"Y\")])\nnx.draw(G, \n        nx.get_node_attributes(G, 'pos'), \n        with_labels=True, \n        font_weight='bold', \n        node_size = 2000,\n        node_color = \"lightblue\",\n        linewidths = 3)\nax= plt.gca()\nax.collections[0].set_edgecolor(\"#000000\")\nax.set_xlim([-.3, 4.3])\nax.set_ylim([-.3, 2.3])\nplt.show()\n\n\n\n\nBelow, we create an example. It isn’t terribly interesting, since the X and the Y aren’t related at all. But, it does show us some useful code.\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model as lm\nsns.set()\n\n\n# N is batch size; D_in is input dimension;\n# H is hidden dimension; D_out is output dimension.\nN, D_in, H, D_out = 1000, 128, 32, 8\n\n# Create random Tensors to hold inputs and outputs\nx = torch.randn(N, D_in)\ny = torch.randn(N, D_out)\n\n# Use the nn package to define our model as a sequence of layers. nn.Sequential\n# is a Module which contains other Modules, and applies them in sequence to\n# produce its output. Each Linear Module computes output from input using a\n# linear function, and holds internal Tensors for its weight and bias.\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(D_in, H),\n    torch.nn.ReLU(),\n    torch.nn.Linear(H, D_out),\n)\n\n# The nn package also contains definitions of popular loss functions; in this\n# case we will use Mean Squared Error (MSE) as our loss function.\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\nlearning_rate = 1e-4\nfor t in range(1000):\n    # Forward pass: compute predicted y by passing x to the model. Module objects\n    # override the __call__ operator so you can call them like functions. When\n    # doing so you pass a Tensor of input data to the Module and it produces\n    # a Tensor of output data.\n    y_pred = model(x)\n\n    # Compute and print loss. We pass Tensors containing the predicted and true\n    # values of y, and the loss function returns a Tensor containing the\n    # loss.\n    loss = loss_fn(y_pred, y)\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Zero the gradients before running the backward pass.\n    model.zero_grad()\n\n    # Backward pass: compute gradient of the loss with respect to all the learnable\n    # parameters of the model. Internally, the parameters of each Module are stored\n    # in Tensors with requires_grad=True, so this call will compute gradients for\n    # all learnable parameters in the model.\n    loss.backward()\n\n    # Update the weights using gradient descent. Each parameter is a Tensor, so\n    # we can access its gradients like we did before.\n    with torch.no_grad():\n        for param in model.parameters():\n            param -= learning_rate * param.grad\n\n99 4536.55712890625\n199 3196.197998046875\n299 3014.03173828125\n399 2721.951171875\n499 2684.35693359375\n\n\n599 2546.17919921875\n699 2460.678955078125\n799 2381.167236328125\n899 2525.779052734375\n999 2361.3740234375\n\n\nLet’s update that example for our setting using the voxel level data.\n\n## Read in the data and display a few rows\ndat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv\")\n\n\n\n\n\n\nLet’s set the training fraction at 75% (and thus the testing fraction at 25%). This gives us relatively little data to fit with. Thus, a relatively simple model makes sense.\n\ntrainFraction = .75\n\nsample = np.random.uniform(size = 100) < trainFraction\ntrainingDat = dat[sample]\ntestingDat = dat[~sample]\n\nNext we need to get the data into a pytorch size and fram\n\nx = torch.from_numpy(dat[['PD','T1', 'T2', 'T1_10', 'T2_10', ]].values)\ny = torch.from_numpy(dat[['FLAIR']].values)\n\n##pytorch wants type as float\nx = x.float()\ny = y.float()\n\nxtraining = x[sample]\nxtesting = x[~sample]\nytraining = y[sample]\nytesting = y[~sample]\n\n[\n xtraining.size(),\n ytraining.size(),\n xtesting.size(),\n ytesting.size(),\n]\n\n[torch.Size([78, 5]),\n torch.Size([78, 1]),\n torch.Size([22, 5]),\n torch.Size([22, 1])]\n\n\n\n## Define the model\n## Dimension of the hidden layer\nH = 3\n\n## Number of predictors\nD_in = xtraining.size()[1]\nD_out = 1\n\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(D_in, H),\n    torch.nn.ReLU(),\n    torch.nn.Linear(H, D_out),\n)\n\n\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\nlearning_rate = 1e-4\nfor t in range(100000):\n    y_pred = model(xtraining)\n    loss = loss_fn(y_pred, ytraining)\n    if t % 10000 == 0:\n        print(t, loss.item())\n    model.zero_grad()\n    loss.backward()\n    with torch.no_grad():\n        for param in model.parameters():\n            param -= learning_rate * param.grad\n\n0 183.79598999023438\n\n\n10000 19.117000579833984\n\n\n20000 17.857234954833984\n\n\n30000 16.61604881286621\n\n\n40000 16.3690242767334\n\n\n50000 16.116371154785156\n\n\n60000 16.016294479370117\n\n\n70000 15.965615272521973\n\n\n80000 15.932544708251953\n\n\n90000 15.93248462677002\n\n\n\n## try prediction\nytesting_pred = model(xtesting)\na = ytesting_pred.detach().numpy()\n\nplt.scatter(a[:,0], ytesting[:,0])\n\n<matplotlib.collections.PathCollection at 0x7ae5fac918d0>"
  },
  {
    "objectID": "specialized_autoencoder.html",
    "href": "specialized_autoencoder.html",
    "title": "36  Autoencoders",
    "section": "",
    "text": "In this exercise, we’ll build an autoencoder to model the cryptopunks. We’ll assume that you’ve already looked at the chapter on convolutional networks, where we show how we downloaded and process the data. An autoencoder can be thought of as the following. Consider a datset with 8 features and consider a network that has 4 hidden nodes on the first layer, 2 on the second, 4 on the third and 8 on the fourth. See the picture below.\n\n\n\n\n\nLet \\(\\phi\\) be the first two layers of the network and \\(\\theta\\) be the last two. So, if we wanted to pass a data row, \\(x_i\\) through the network we would do \\(\\theta(\\phi(x_i))\\). We would call the network \\(\\phi\\) as the encoding network and \\(\\theta\\) as the decoding network. Consider training the network by minimizing\n\\[\n\\sum_{i=1}^n || x_i - \\theta(\\phi(x_i)) ||^2\n\\]\nover the weights. This sort of network is called an autoencoder. Notice that the same data is the input and output of the network. This kind of learning is called unsupervised, since we’re not trying to use \\(x\\) to predict an outcome \\(y\\). Instead, we’re trying to explore variation and find interesting features in \\(x\\) as a goal in itself without a “supervising” outcome, \\(y\\), to help out.\nNotice overfitting concerns are somewhat different in this network construction. If this model fits well, then it’s suggesting that 2 numbers can explain 8. That is, the network will have reduced the inputs to only two dimensions, that we could visualize for example. That is a form of parsimony that prevents overfitting. The middle layer is called the embedding. It is called this because an autoencoder is a form of non-linear embedding of our data into a lower dimensionional space.\nThere’s nothing to prevent us from having convolutional layers if the inputs are images. That’s what we’ll work on here. For convolutional autoencoders, it’s typical to increase the number of channels and decrease the image sizes as one works through the network.\n\nimport urllib.request\nimport PIL\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\n\nImport the image of all of the cryptopunks where we’ll try to fit a convolutional autoencoder.\n\nimgURL = \"https://raw.githubusercontent.com/larvalabs/cryptopunks/master/punks.png\"\nurllib.request.urlretrieve(imgURL, \"cryptoPunksAll.jpg\")\nimg = PIL.Image.open(\"cryptoPunksAll.jpg\").convert(\"RGB\")\nimgArray = np.asarray(img)\n\nReorder the array. I couldn’t get reshape to do this right, but I think this is a one-line command waiting to happen. See if you can figure out a better way. All images are 24x24x3 and there’s 10,000 punks. (Supposedly, there will only ever be 10k punks.) Pytorch needs this in a 10,000x3x24x24 array.\n\nfinalArray = np.empty((10000, 3, 24, 24))\nfor i in range(100):\n  for j in range(100):\n    a, b = 24 * i, 24 * (i + 1)  \n    c, d = 24 * j, 24 * (j + 1) \n    idx = j + i * (100)\n    finalArray[idx,0,:,:] = imgArray[a:b,c:d,0]\n    finalArray[idx,1,:,:] = imgArray[a:b,c:d,1]\n    finalArray[idx,2,:,:] = imgArray[a:b,c:d,2]\n\nn = finalArray.shape[0]\ntrainFraction = .75\nsample = np.random.uniform(size = n) &lt; trainFraction\nx_train = finalArray[ sample, :, :, :] / 255\nx_test =  finalArray[~sample, :, :, :] / 255\nprint([x_train.shape, x_test.shape])\n\n\n## To get the trainLoader to work, I had to just load in the torch tensor\n## in the previous example we had both data and labels. This time we just \n## have data\ntrainLoader = torch.utils.data.DataLoader(torch.Tensor(x_train), batch_size = 100, shuffle = False, num_workers = 1)\n\n[(7489, 3, 24, 24), (2511, 3, 24, 24)]\n\n\nNow let’s create our encoder/decoder. Here we’re going to use a simple approach for just flattening the images, one dense layer, they decoding. Network construction is its own thing in neural networks. The way we’re doing our network is a convolutional autoencoder. Typically, these have the number of channels increase while the image size decreases in each convolutional layer. The middle layer is called the embedding. We have to get the dimensions to match up with a combination of kernels, pooling, and varying the stride length.\nOur network construction is as follows:\nChannels: 3 -&gt; 6 -&gt; 12 -&gt; 6 -&gt; 3\nImage dims: (24, 24) -&gt; (20, 20) -&gt; (10, 10) -&gt; (6, 6) -&gt; (3, 3) -&gt; (6, 6) -&gt; (10, 10) -&gt; (24, 24)\n\nkernel_size = 5\nclass autoencoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.conv1 = nn.Conv2d(3, 6, kernel_size)\n        self.conv2 = nn.Conv2d(6, 12, kernel_size)\n        self.pool  = nn.MaxPool2d(2, 2)\n        self.iconv1 = nn.ConvTranspose2d(12, 6, kernel_size+1, stride = 2)\n        self.iconv2 = nn.ConvTranspose2d(6, 3, kernel_size+1, stride = 2)\n\n    def encode(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        return x\n    \n    def decode(self, x):\n        x = F.relu(self.iconv1(x))\n        ## Use the sigmoid as the final layer \n        ## since we've normalized pixel values to be between 0 and 1\n        x = torch.sigmoid(self.iconv2(x))\n        return(x)\n    \n    def forward(self, x):\n        return self.decode(self.encode(x))\n    \nautoencoder = autoencoder()\n\nNow we’ve constructed our network, let’s try it out. I’m going to first do the encoder, then the decoder, then through the entire network. We’ll check out the dimensions to make sure everything works.\n\n## Here's some example data by grabbing one batch\ntryItOut = next(iter(trainLoader))\nprint(tryItOut.shape)\n\n## Let's encode that data\nencoded = autoencoder.encode(tryItOut)\nprint(encoded.shape)\n\n## Now let's decode the encoded data\ndecoded = autoencoder.decode(encoded)\nprint(decoded.shape)\n\n## Now let's run the whole thing through\nfedForward = autoencoder.forward(tryItOut)\nprint(fedForward.shape)\n\ntorch.Size([100, 3, 24, 24])\ntorch.Size([100, 12, 3, 3])\ntorch.Size([100, 3, 24, 24])\ntorch.Size([100, 3, 24, 24])\n\n\n\ntest = fedForward.detach().numpy()\n\n## Plot out the first 5 images, note this isn't very interesting, since\n## all of the weights haven't been trained\nplt.figure(figsize=(10,5))\nfor i in range(5): \n  plt.subplot(1, 5,i+1)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(test[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n\n\n\nLet’ see how we do on our images. We’ll run the algorithm for 500 epochs\n\n#Optimizer\noptimizer = torch.optim.Adam(autoencoder.parameters(), lr = 0.001)\n\n#Epochs\nn_epochs = 20\n\nautoencoder.train()\n\n\nfor epoch in range(n_epochs):\n    for data in trainLoader:\n        images = data\n        optimizer.zero_grad()\n        outputs = autoencoder.forward(images)\n        loss = F.mse_loss(outputs, images)\n        loss.backward()\n        optimizer.step()\n\nNow that we’ve run it, let’s feed a collection of training images through the convnet and see how we did. The top row is the first 5 images of the last training epoch, last batch, and the bottom 5 is those images passed through the algorithm.\n\n## the data from the last iteration is called images\ntrainSample = images.detach().numpy()\n\nplt.figure(figsize=(10,5))\nfor i in range(5): \n  plt.subplot(1, 5,i+1)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(trainSample[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n## the output from the last iterations (feed forward through the network) is called outputs\ntrainOutput = outputs.detach().numpy()\n\nplt.figure(figsize=(10,5))\nfor i in range(5): \n  plt.subplot(2, 5,i+6)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(trainOutput[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n\n\n\n\n\n\n\ntestLoader  = torch.utils.data.DataLoader(torch.Tensor(x_test), batch_size = 100, shuffle = False, num_workers = 1)\ntestSample =  autoencoder.forward(next(iter(testLoader))).detach().numpy()\n\n\nplt.figure(figsize=(10,4))\n\n## Plot the original data\nfor i in range(5): \n  plt.subplot(2, 5, i + 1)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(x_test[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n# Plot the data having been run throught the convolutional autoencoder\nfor i in range(5): \n  plt.subplot(2, 5, i + 6)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(testSample[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n\n\n\nNot bad? One way to think about the convolutional autoencoder is as a compression algorithm. The data starts out as 24x24x3 = 1,728 floats. In contrast, our middle layer is 12 x 12 x 3 = 432 floats (25% of the original size). That is, it requires 75% fewer numbers to store the data as the embedding. You could imagine a sender having access to the encoder and the receiver having access to the decoder and then using the 75% reduction to send the data more efficiently."
  },
  {
    "objectID": "specialized_autoencoder2.html",
    "href": "specialized_autoencoder2.html",
    "title": "37  Autoencoder example",
    "section": "",
    "text": "Autoencoders are another unsupervised learning technique. Autoencoders take in a record and spit out a prediction of the same size. The goal is to represent the records as a NN. In an incomplete autoencoder, the model is regularized by the embedding (middle) layer being much lower than the input dimension. In this way, an autoencoder is a dimension reduction technique, reducing the input dimension size downto a much lower size (the encoder) then back out to the original size (the decoder). We can represent the autoencoder with a network diagram as below.\n\n\n\n\n\nLet \\(\\phi\\) be the first two layers of the network and \\(\\theta\\) be the last two. So, if we wanted to pass a data row, \\(x_i\\) through the network we would do \\(\\theta(\\phi(x_i))\\). We would call the network \\(\\phi\\) as the encoding network and \\(\\theta\\) as the decoding network. Consider training the network by minimizing\n\\[\n\\sum_{i=1}^n || x_i - \\theta(\\phi(x_i)) ||^2\n\\]\nover the weights. This sort of network is called an autoencoder. Notice that the same data is the input and output of the network. This kind of learning is called unsupervised, since we’re not trying to use \\(x\\) to predict an outcome \\(y\\). Instead, we’re trying to explore variation and find interesting features in \\(x\\) as a goal in itself without a “supervising” outcome, \\(y\\), to help out.\nNotice overfitting concerns are somewhat different in this network construction. If this model fits well, then it’s suggesting that 2 numbers can explain 8. That is, the network will have reduced the inputs to only two dimensions, that we could visualize for example. That is a form of parsimony that prevents overfitting. The middle layer is called the embedding. It is called this because an autoencoder is a form of non-linear embedding of our data into a lower dimensionional space.\nThere’s nothing to prevent us from having convolutional layers if the inputs are images. That’s what we’ll work on here. For convolutional autoencoders, it’s typical to increase the number of channels and decrease the image sizes as one works through the network.\n\n37.0.1 PCA and autoencoders\nWithout modification, autoencoders can be programmed that span the same space as PCA/SVD (Plaut 2018). Enforcing the orthogonality requires something like adding Lagrange terms to the loss function. There’s no reason why you would do this, since PCA is well developed and works just fine. However, it does suggest why NNs are such a large class of models.\nLet \\(X_i\\) be a collection of features for record \\(i\\). Then, the SVD approximates the data matrix \\(X\\) with \\(UV^t\\), where we’ve absorbed the singular values into either \\(U\\) or \\(V\\). Per record, this model for \\(K\\) components and column \\(k\\) from \\(V\\) of \\(v_k\\).\n\\[\n\\hat x_i = \\sum_{k=1}^K &lt;x_i, v_k&gt; v_k\n\\]\nTherefore, consider a neural network that specifies that the first layer defines \\(K\\) hidden units as\n\\[\nh_{ik} = &lt;x_i, v_k&gt;.\n\\]\nThat is, it has a linear activation function with no bias term and weights \\(v_{jk}\\) where \\(v_{jk}\\) is element \\(j\\) of vector \\(v_k\\). Then consider an output layer that defines\n\\[\n\\hat x_{ij} = \\sum_{k=1}^K h_{ik} v_{jk},\n\\]\nAgain, this is a linear activation function with weights \\(v_{jk}\\). So, we arrive at the conclusion, that PCA is an example of an autoencoder with two layers, constraints on the weights being common to both layers, and constraints on the loss function that enforces the orthonormality of the \\(v_k\\). Of course, as we saw with ordinary regression, whether or not we can actually get gradient descent to converge remains a harder issue than just using PCA directly. Furthermore, the autoencoder wouldn’t necessarily order the PCs similarly.\nFinally, we see that a two layer autoencoder -without the constraints- contains the PCA fit as a special case and spans the same space as the PCA fit. Similarly we see that such a two layer encoder is overspecified, as most NNs are.\n\n\n37.0.2 Example on dermamnist\nFirst, let’s set up our autoencoder by defining a python class then initializing it. We assume the imports and data loading from the chapter on PCA and ICA.\n\n\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\n\n\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\n\n\n\nkernel_size = 5\n\nclass autoencoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.conv1 = nn.Conv2d(3, 6, kernel_size)\n        self.conv2 = nn.Conv2d(6, 12, kernel_size)\n        self.pool  = nn.MaxPool2d(2, 2)\n        self.iconv1 = nn.ConvTranspose2d(12, 6, kernel_size+1, stride = 2)\n        self.iconv2 = nn.ConvTranspose2d(6, 3, kernel_size+1, stride = 2)\n\n    def encode(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        return x\n    \n    def decode(self, x):\n        x = F.relu(self.iconv1(x))\n        ## Use the sigmoid as the final layer \n        ## since we've normalized pixel values to be between 0 and 1\n        x = torch.sigmoid(self.iconv2(x))\n        return(x)\n    \n    def forward(self, x):\n        return self.decode(self.encode(x))\n    \nautoencoder = autoencoder()\n\nWe can try out the autoencoder by\n\n## Here's some example data by grabbing one batch\ntryItOut, _ = next(iter(train_loader))\nprint(tryItOut.shape)\n\n## Let's encode that data\nencoded = autoencoder.encode(tryItOut)\nprint(encoded.shape)\n\n## Now let's decode the encoded data\ndecoded = autoencoder.decode(encoded)\nprint(decoded.shape)\n\n## Now let's run the whole thing through\nfedForward = autoencoder.forward(tryItOut)\nprint(fedForward.shape)\n\ntorch.Size([128, 3, 28, 28])\ntorch.Size([128, 12, 4, 4])\ntorch.Size([128, 3, 28, 28])\ntorch.Size([128, 3, 28, 28])\n\n\n\ntest = fedForward.detach().numpy()\n\n## Plot out the first 5 images, note this isn't very interesting, since\n## all of the weights haven't been trained\nplt.figure(figsize=(10,5))\nfor i in range(5): \n  plt.subplot(1, 5,i+1)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(test[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n\n\n\n\n#Optimizer\noptimizer = torch.optim.Adam(autoencoder.parameters(), lr = 0.001)\n\n#Epochs\nn_epochs = 20\n\nautoencoder.train()\n\nfor epoch in range(n_epochs):\n    for data, _ in train_loader:\n        images = data\n        optimizer.zero_grad()\n        outputs = autoencoder.forward(images)\n        loss = F.mse_loss(outputs, images)\n        loss.backward()\n        optimizer.step()\n\n\n## the data from the last iteration is called images\ntrainSample = images.detach().numpy()\n\nplt.figure(figsize=(10,5))\nfor i in range(5): \n  plt.subplot(1, 5,i+1)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(trainSample[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n## the output from the last iterations (feed forward through the network) is called outputs\ntrainOutput = outputs.detach().numpy()\n\nplt.figure(figsize=(10,5))\nfor i in range(5): \n  plt.subplot(2, 5,i+6)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(trainOutput[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n\n\n\n\n\n\nOn a test batch\n\ntest_batch, _ = next(iter(test_loader))\nx_test = test_batch.detach().numpy()\ntestSample = autoencoder.forward(test_batch).detach().numpy()\n\nplt.figure(figsize=(10,4))\n\n## Plot the original data\nfor i in range(5): \n  plt.subplot(2, 5, i + 1)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(x_test[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n# Plot the data having been run throught the convolutional autoencoder\nfor i in range(5): \n  plt.subplot(2, 5, i + 6)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(testSample[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n\n\n\n\n\n\n\nPlaut, Elad. 2018. “From Principal Subspaces to Principal Components with Linear Autoencoders.” arXiv Preprint arXiv:1804.10253."
  },
  {
    "objectID": "specialized_gan.html#gan-implementations",
    "href": "specialized_gan.html#gan-implementations",
    "title": "38  GANs",
    "section": "38.1 GAN implementations",
    "text": "38.1 GAN implementations\nGANs work by two parts. I’ll describe this by imagining breaking our autoencoder into two parts. Recall, our autoencoder diagram as seen below.\n\n\n\n\n\nAgain, consider breaking this digram into two parts. One that takes the embedding and spits out images (a generator) and one that takes in images and spits out guesses as to whether or not they are real (a discriminator). See below where the generator is on the left and the discriminator is on the right.\n\n\n\n\n\nGANs do something like the following. They generate data (H21 and H22 above) using a random number generator. These are passed through the generator to obtain simulated records (H41-H48). These fake records are concatenated with real records to be passed through the discriminator, which tries to guess whether the records are real or fake. The two networks are trained adversarially. That is, the generator has higher loss when it fails to fool the discriminator and the discriminator has higher loss when it fails to discriminate between real and fake records.\nThis sort of approach can be used for data of any type. But, it’s fun especially to do it using images. Some of the images generated from GANs are wild in how realistic looking they are. Let’s try to create a GAN to generate our cryptopunks.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport urllib.request\nimport PIL\n\n\n## Read in and organize the data\nimgURL = \"https://raw.githubusercontent.com/larvalabs/cryptopunks/master/punks.png\"\nurllib.request.urlretrieve(imgURL, \"cryptoPunksAll.jpg\")\nimg = PIL.Image.open(\"cryptoPunksAll.jpg\").convert(\"RGB\")\nimgArray = np.asarray(img)\nfinalArray = np.empty((10000, 3, 24, 24))\nfor i in range(100):\n  for j in range(100):\n    a, b = 24 * i, 24 * (i + 1)  \n    c, d = 24 * j, 24 * (j + 1) \n    idx = j + i * (100)\n    finalArray[idx,0,:,:] = imgArray[a:b,c:d,0]\n    finalArray[idx,1,:,:] = imgArray[a:b,c:d,1]\n    finalArray[idx,2,:,:] = imgArray[a:b,c:d,2]\n\nn = finalArray.shape[0]\nx_real = finalArray / 255\nx_real = torch.tensor(x_real.astype(np.float32))\nx_real.shape\n\ntorch.Size([10000, 3, 24, 24])\n\n\nFor the generator, we’ll use a similar construction as the decoding layer from our autoencoder chapter. For the discriminator, let’s use a similar network to the one we used in our convolutional NN chapter.\n\n## Define our constants for our networks\nkernel_size = 5\ngenerator_input_dim = [16, 3, 3]\n\n\nclass create_generator(nn.Module):\n    def __init__(self):\n        super().__init__()        \n        self.net = nn.Sequential(\n            nn.ConvTranspose2d(16, 128, 10, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(128, 3, 4, 2, 1, bias=False), \n            nn.Sigmoid(),\n        )\n    def forward(self, x):\n        return self.net(x)\n \n## Use the discriminator from the convnet chapter\nclass create_discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 12, 5)\n        self.fc1 = nn.Linear(12 * 3 * 3, 32)\n        self.fc2 = nn.Linear(32, 1)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x\n    \n        \ngenerator = create_generator()\ndiscriminator = create_discriminator()\n\nLet’s try out our generator. First, we’re going to generate n embeddings. Then we’ll feed them through the generator to obtain n images. Notice that they don’t look so good. This is because we haven’t trained out generator yet!\n\ntest_embedding = torch.randn([5]+generator_input_dim)\nx_fake = generator(test_embedding)\n\n## Plot out the first 5 images, note this isn't very interesting, since\n## all of the weights haven't been trained\nplt.figure(figsize=(10,5))\nfor i in range(5): \n  plt.subplot(1, 5,i+1)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(x_fake.detach().numpy()[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n\n\n\n\n# Our label convention, real_label = 1, fake_label = 0\n\nlr = 1e-4\n\n## y is n real images then n fake images\ny = torch.concat( (torch.ones(n), torch.zeros(n) ) ) \n\n## Set up optimizers\noptimizerD = optim.Adam(discriminator.parameters(), lr=lr)\noptimizerG = optim.Adam(generator.parameters(), lr=lr)\n\n## Set up the loss function\nloss_function = nn.BCELoss()\n\nA couple of details. First, to speed up the algorithm, I’m using random batches of 100. You can do this directly with torch’s dataloader, but I decided just to do it manually. Secondly, you have to run the algorithm for a long time. The code below just says 20, because that was the very last size I used. The results are of 4k epochs or so. Finally, note that we save the networks in progress and I wrote in some code that lets me restart the network with the saved states. So, if my program halts for any reason, I didn’t lose all of its progress.\n\nrandomBatchSize = .1\nn_epochs = 20\ntrainFraction = .1\n\nfor epoch in range(n_epochs):          \n    ## Generate the batch sample\n    sample = np.random.uniform(size = n) &lt; trainFraction\n    n_batch = np.sum(sample)\n\n    ## Generate the simulated embedding    \n    embedding = torch.randn([n_batch]+generator_input_dim)\n    \n    \n    ## Generate new fake images\n    x_fake = generator(embedding)\n    \n    ## train the discriminator\n    ## zero out the gradient\n    discriminator.zero_grad()\n\n    ## run the generated and fake images through the discriminator\n    yhat_fake = discriminator(x_fake.detach())\n    yhat_real = discriminator(x_real[sample,:, :, :])\n    ## Note you have to concatenate them in the same order as \n    ## the previous cell. Remember we did real then fake\n    yhat = torch.concat( (yhat_real, yhat_fake) ).reshape(-1)\n\n    ## Calculate loss on all-real batch \n    y = torch.concat( (torch.ones(n_batch), torch.zeros(n_batch) ) ) \n\n    discriminator_error = loss_function(yhat, y)\n\n    # Calculate gradients for D in backward pass\n    discriminator_error.backward(retain_graph = True)\n\n    # Update the discriminator\n    optimizerD.step()\n\n    ## Train the generator\n    ## zero out the gradient\n    generator.zero_grad()\n    ## The discriminator has been udpated, so push the data through the \n    ## new discriminator\n    yhat_fake = discriminator(x_fake)\n    ## Note the outcome for the generator is all ones even\n    ## though we're classifying real as 1 and fake as 0\n    ## In other words, we want the loss for the generator to be\n    ## based on how real-like the generated data is\n    generator_error = loss_function( yhat_fake,  torch.ones( (n_batch, 1) ) )\n    ## Calculate the backwards error\n    generator_error.backward(retain_graph = False)\n    # Update the discriminator\n    optimizerG.step()\n    \n    if (epoch + 1) % 10 == 0:  \n        ## print(epoch, end = \",\")\n        ## Save the state dictionary in progress\n        torch.save(generator.state_dict(), \"generator.pt\")\n        torch.save(discriminator.state_dict(), \"discriminator.pt\")\n\nHere’s 25 of our generated punks. It’s clearly getting there. Notice, some of the punks have an earring. Also, some have a slightly green tinge, presumably because of the green (zombie) punks in the dataset.\n\nplt.figure(figsize=(10,10))\nfor i in range(25): \n  plt.subplot(5, 5,i+1)\n#  plt.xticks([])\n#  plt.yticks([])\n  img = np.transpose(x_fake.detach().numpy()[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)"
  },
  {
    "objectID": "graphics_images.html#basics",
    "href": "graphics_images.html#basics",
    "title": "19  Working with images",
    "section": "19.1 Basics",
    "text": "19.1 Basics\nImages broadly come in two types, vector and raster. Vector graphics are in formats like pdf, eps, svg and raster graphics are like jpeg, gif, png. Vector graphics store the image constructs and shapes. So, a vector graphics renderer can zoom in indefinitely on a shape and its edges will appear sharp. Vector fonts work this way. Raster graphics basically store a matrix and the pixels on the screen show the values of that matrix. Bitmapped fonts work this way. Of course, vector graphics have to be converted to raster to be actually displayed by the computer. Finally, some rater graphics formats have compression, which we won’t really discuss."
  },
  {
    "objectID": "graphics_images.html#working-with-raster-graphics",
    "href": "graphics_images.html#working-with-raster-graphics",
    "title": "19  Working with images",
    "section": "19.2 Working with raster graphics",
    "text": "19.2 Working with raster graphics\nRaster images are typically stored as an array. Grayscale images are matrices with the image intensity as the value and color pictures are stored as 3D arrays with the two main dimensions and color channels. A library for working with regular images in python is called PIL.\nThere are different raster specifications. RGB has 3 color channels, red, green and blue. CMYK has four: cyan, magenta, yellow and black. It’s interesting to note that the use of color channels existed before color cameras, when photographers would use different filters and additive and subtractive processes. The photograph below was created in 1877 by Louis Ducos du Hauron.\n\n\n\nColor image\n\n\nReading and working with images in python is quite easy because of the Python Image Library (PIL).\n\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nimg = Image.open(\"assets/images/Duhauron1877.jpg\")\n## You can see it with this, or img.show()\nimg\n\n\n\n\nPIL objects come with a ton of methods. For example, if we want to know whether we have an RGB or CMYK image, just print its mode.\n\nprint(img.mode)\n\nRGB\n\n\n\nr, g, b = img.split()\n\nplt.figure(figsize=(10,4));\nplt.subplot(1, 3, 1);\nplt.axis('off');\nplt.imshow(r);\n\nplt.subplot(1, 3, 2);\nplt.axis('off');\nplt.imshow(g);\n\nplt.subplot(1, 3, 3);\nplt.axis('off');\nplt.imshow(b);\n\n\n\n\nIf you’re tired of working with the image as a PIL object, it’s easy to convert to a np array.\n\nimg_array = np.array(img)\nimg_array.shape\n\n(1132, 1548, 3)\n\n\nBefore we leave PIL, it should be said that most image operations can be done in it. For example, cropping.\n\nbbox = [500, 630, 700, 760]\ncropped = img.crop(bbox)\ncropped\n\n\n\n\nWe can rotate the house and put it back\n\nrot = cropped.transpose(Image.Transpose.ROTATE_180)\nrot\n\n\n\n\n\n##Note this overwrites the image\nimg.paste(rot, bbox)\nimg"
  },
  {
    "objectID": "graphics_images.html#image-mathematics",
    "href": "graphics_images.html#image-mathematics",
    "title": "19  Working with images",
    "section": "19.3 Image mathematics",
    "text": "19.3 Image mathematics\n\n19.3.1 Convolutions\n\n19.3.1.1 1D transforms\nConvolutions are an important topic in mathematics, statistics, signal processing … Let’s discuss 1D convolutions first. A real valued convolution of two continuous signals, \\(X(t)\\) and \\(K(t)\\) is defined as \\(X* K\\)\n\\[\n(X* K)(t) = \\int_{-\\infty}^{\\infty} X(u) K(t-u) du\n= \\int_{-\\infty}^{\\infty} X(t-v) K(v) dv,\n\\]\nwhere the equality is determined by a simple change of variable argument. The discrete analog is\n\\[\n(X* K)(t) = \\sum_{u = -\\infty}^{\\infty} X(u) K(t-u)\n= \\sum_{v = -\\infty}^{\\infty} X(t-v) K(v)\n\\]\nThe convolution has many, many uses in data science and statistics. For example, the convolution of densities or mass functions is the respective density or mass function for the sum of random variables from those distributions. In applied data analysis, you can think of the convolution between \\(X\\) and \\(K\\) as smearing the function \\(K\\) over the function \\(X\\). Thus, it plays a key role in smoothing. Let’s try an example using the covid data and a box kernel. We take \\(K(t) = I\\{0 \\leq t < M\\} / M\\) (i.e. is 1 for times 0 to \\(M-1\\), then rescaled so it sums to 1). Assume that \\(N\\geq M\\) and that \\(X(t)\\) and \\(K(t)\\) are \\(0\\) and for \\(t < 0\\) or \\(t > N\\). Then, our convolution works out to be\n\\[\n(X* K)(t)\n= \\sum_{u = -\\infty}^{\\infty} X(u) K(t-u)\n= \\sum_{u = 0}^{N} X(u) K(t-u)\n= \\sum_{u = t}^{t + M - 1} X(u) K(t -u)\n= \\sum_{u = t}^{t + M - 1} X(u) / M\n\\]\nThat is, our convolution is a moving average of \\(X\\) where the convolution at point \\(t\\) is the average of the points between \\(t\\) and \\(t + M - 1\\). So, the convolution, as we’ve defined it, at point \\(t\\) is the moving average at point \\(t + (M-1)/2\\) (ie. it’s shifted by \\((M-1)/2\\)). Also, at the end (\\(t \\geq N - M + 1\\)), we’re averaging in the assumed zero values of the \\(X\\). This might be reasonable to do, or maybe not. The fact that we’re padding the end and not the beginning is just because of the range of index values we defined the kernel on. We’d have the same problem only on the other end if \\(K(t) = I(-M < t \\leq 0)/M\\). Of course, the computer will start summing things at index 0 regardless. However, it can shift the kernel relative to the signal arbitrarily by zero padding one end or the other or both. A reasonable strategy is to set it so that it averages in \\((M-1)/2\\) on both ends. Numpy allows you to only look at the range of \\(N - M\\) middle values where this isn’t an issue (argument mode = \"valid\").\nNote we could make the kernel weight points differently than just a box kernel. A popular choice is a Gaussian distribution.\nAlso, the convolution has \\(N+M-1\\) points. So, it has more time points than the original signal. Numpy has options to shift the convolution back into the same space as the original signal for you (i.e. has \\(N\\) points, mode = \"same\"). Or, you can just do it yourself if you do mode = \"full\", just shift by \\((M-1)/2\\). Similarly shift for mode = \"valid\" (but the convolution has fewer points in this case, so it won’t have corresponding points with \\(X\\) at the very beginning and end).\nHere’s an example using Italy’s daily covid case count data. We plot the data and the convolution smoothed data. In the bottom panels, we show the residuals to highlight the difference.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\ndat = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n## Get Italy, drop everyrthing except dates, convert to long (unstack converts to tuple)\nX = dat[dat['Country/Region'] == 'Italy'].drop([\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"], axis=1).unstack()\n## convert from tuple to array\nX = np.asarray(X)  \n## get case counts instead of cumulative counts\nX = X[1 : X.size] - X[0 : (X.size - 1)]\n## get the first non zero entry\nX =  X[np.min(np.where(X !=  0)) : X.size]\nplt.plot(X)\n\n\n\n\nNow let’s plot the convolutions with different options in np.convolve.\n\n## 41 day moving average\nN = len(X)\nM = 41\n\nfig, axes = plt.subplots(2, 3, figsize = [12.4, 12.4])\naxes[0,0].plot(X)\naxes[0,1].plot(X)\naxes[0,2].plot(X)\n\nK = np.ones(M) / M\n\n## Plot the convolution with the argument 'same'\n## this gives N (assumed greater than M) points\nXC = np.convolve(X, K, 'same')\naxes[0,0].plot(XC)\naxes[1,0].plot(X - XC)\n\n## Plot the convolution with the argument 'full'\n## which gives N+M-1 total pionts\nXC = np.convolve(X, K, 'full')\ntemp = np.pad(X, (M-1, 0), 'constant') \naxes[0,1].plot(XC)\naxes[1,1].plot(temp- XC)\n\n\n## Plot the convolution with the convolution shifted back by (M-1)/2\nXCshifted = XC[ (int((M - 1)/2)) : int(len(XC) - (M - 1)/2) ]\naxes[0, 2].plot(XCshifted)\naxes[1, 2].plot(X - XCshifted)\n## 41 day moving average\nN = len(X)\nM = 41\n\nfig, axes = plt.subplots(2, 3, figsize = [12.4, 12.4])\naxes[0,0].plot(X)\naxes[0,1].plot(X)\naxes[0,2].plot(X)\n\nK = np.ones(M) / M\n\n## Plot the convolution with the argument 'same'\n## this gives N (assumed greater than M) points\nXC = np.convolve(X, K, 'same')\naxes[0,0].plot(XC)\naxes[1,0].plot(X - XC)\n\n## Plot the convolution with the argument 'full'\n## which gives N+M-1 total pionts\nXC = np.convolve(X, K, 'full')\ntemp = np.pad(X, (M-1, 0), 'constant') \naxes[0,1].plot(XC)\naxes[1,1].plot(temp- XC)\n\n\n## Plot the convolution with the convolution shifted back by (M-1)/2\nXCshifted = XC[ (int((M - 1)/2)) : int(len(XC) - (M - 1)/2) ]\naxes[0, 2].plot(XCshifted)\naxes[1, 2].plot(X - XCshifted)\n\n\n\n\n\n\n\nLet’s show that the first point and end point of the convolution are the averages of \\((M-1)/2\\) points and and \\((M-1)/2+1\\) zeros at the beginning or end of the original signal just to show that our intuition is correct.\n\ntemp = np.convolve(X, K, 'same')\n[\n  # the first convolution point (temp[0]) and the average of the\n  # the first (M-1) / 2 X points and (M-1)/2 + 1 zeros\n  [temp[0],     X[0 : int(    (M - 1) / 2)].sum() / M],\n  # the last convolution point (temp[N-1]) and the average of the\n  # the last (M-1) / 2 X points and (M-1)/2 + 1 zeros\n  [temp[N - 1], X[int(N - (M - 1) / 2 - 1)  : N].sum() / M]\n \n]\n\n[[0.07317073170731708, 0.07317073170731707],\n [2163.0731707317073, 2163.0731707317073]]\n\n\nAlso, I averaged a lot (41 days) in order to make the shift very apparent. Let’s look at the performance for less wide of a kernel.\n\n## 21 day moving average\nM = 21\nK = np.ones(M) / M\n\nfig, axes = plt.subplots(1, 2, figsize = [12.4, 6.2])\nXC = np.convolve(X, K, 'same')\naxes[0].plot(X)\naxes[0].plot(XC)\naxes[1].plot(X - XC)\n\n\n\n\nIt should be stated that the convolution operation is multiplication in Fourier space. So, functions like np.convolve are performing FFTs in the background. However, if you’re going to do this yourself, make sure to keep track of indices and zero padding. (I.e. the bookkeeping.) Otherwise, the FFT wraps around and you get a little of the end averaged in with the beginning and vice versa. I work out getting the same answer as mode = “same” below.\n\nfig, axes = plt.subplots(1, 2, figsize = [12.4, 6.2])\n\n## Pad the X with zeros in the back, need at least M-1 \npad_width = (0, M - 1)\nXpadded = np.pad(X, pad_width, \"constant\")\n## Pad the kernel in the back with N-1, so both the kernel\n## and the X are of length, N+M-1\nKpadded = np.pad(K, (0, N - 1))\n\n## Note we take the real part b/c the complex part is all effectively \n## machine 0\nconvolution = np.fft.ifft(np.fft.fft(Xpadded) * np.fft.fft(Kpadded)).real\n\n## At this point the convolution is of length N + M - 1\n## To get it comparable with the original X, subtract (M-1)/2 indices\n## from each end\nconvolution = convolution[ int((M-1)/2) : int(N+(M-1)/2)]\n\n## Let's see how we did\naxes[0].plot(X)\naxes[0].plot(convolution)\n\n#Show they're the same by plotting the subtraction\naxes[1].plot(convolution - XC)\n\n\n\n\n\n\n19.3.1.2 2D transforms\nFor two dimensions, the convolution is similar\n\\[\n(X ** K)(i,j) = \\sum_{u=-\\infty}^{\\infty} \\sum_{v=-\\infty}^{\\infty}\nX(u, v)  K(i -u, k - v) = \\sum_{u=-\\infty}^{\\infty} \\sum_{v=-\\infty}^{\\infty}\nK(u, v)  X(i -u, k - v)  \n\\]\nOnce again, let’s think where \\(X\\) is of dimension \\((N_1, N_2)\\) and 0 outside of that range, and\n\\[\nK(u, v) = I(0 \\leq u < M_1, 0 \\leq v < M_2) / (M_1 M_2)\n\\]\n(i.e. \\(K\\) is a box on \\(M_1 \\leq N_1\\), \\(M_2 < N_2\\)). Then, applying the exact same argument as before, the convolution is:\n\\[\n(X ** K)(i,j) = \\sum_{u=i}^{M_1 + i - 1} \\sum_{v=j}^{M_2 + j - 1}\nX(u, v) / (M_1 M_2)\n\\]\nThat is, the convolution at point \\((i,j)\\) is the average of the neighboring points. Also, all of the same bookkeeping, zero padding and Fourier transform stuff apply (using the 2D FFT).\nFor regular kernels (box kernels, 2D Gaussians), convolution smooths the image, which has the efffect of making it blurrier. The kernel width determines how blurry the image will then be. This is typically done to denoise an image (to blur out the noise). Let’s try it on a cartoon image of Brian. We’ll just stick to a black and white image so that it’s 2D. A color image has 3 color channels, so is a 3D array. (However, you see the patten; you should be able to extend this to 3D with little problem.)\n\nimport PIL\nimport scipy.signal as sp\nimport urllib.request\n\n\nimgURL = \"https://github.com/smart-stats/ds4bio_book/raw/main/book/bcCartoon.png\"\nurllib.request.urlretrieve(imgURL, \"bcCartoon.png\")\nimg = np.asarray(PIL.Image.open(\"bcCartoon.png\").convert(\"L\"))\n\nplt.xticks([])\nplt.yticks([])\nplt.imshow(img, cmap='gray', vmin=0, vmax=255)\n\n<matplotlib.image.AxesImage at 0x7e54524b3d60>\n\n\n\n\n\nNow let’s take this image and convolve it with different kernels of different window lengths.\n\ndef kernel(i, j):\n  return np.ones((i, j)) / np.prod([i, j])\n\nplt.figure(figsize=[12.4, 12.4])\nimgC = sp.convolve2d(img, kernel(4, 4))\nplt.subplot(2, 2, 1)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"4x4\")\n\nimgC = sp.convolve2d(img, kernel(8, 8))\nplt.subplot(2, 2, 2)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"8x8\")\n\nimgC = sp.convolve2d(img, kernel(16, 16))\nplt.subplot(2, 2, 3)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"16x16\")\n\nboxsize = (5, 5)\nimgC = sp.convolve2d(img, kernel(32,32))\nplt.subplot(2, 2, 4)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"32x32\")\n\nText(0.5, 1.0, '32x32')\n\n\n\n\n\n\n\n19.3.1.3 Convolutional neural networks\nOf course, your kernel doesn’t have to be a box, or a truncated, discretized bivariate Gaussian density or even be non-negative. It’s helpful for smoothers to have non-negative kernels, since they’re just taking a generalized variation of a moving average that way. But, we want to use convolutions\nmore generally. Here, let’s take a kernel that is part of the image (left eye) and convolve it. I’ll make the kernel super peaked at eye features by extracting the eye and raising it to the 4th power.\nSo a relu activation function plus a bias term would then be able to highlight different thresheld variations of this convolution image. For example, here I add a bias term to the convolution then apply a leaky relu. You can see it just highlights the one area where the eye is. A leaky relu is\n\\[\nlrelu(x, c) = \\left\\{\n  \\begin{array}{ll}\n  x & \\text{if $x > 0$} \\\\\n  x * c & \\text{otherwise}\n  \\end{array}\n  \\right.\n\\]\nwhere \\(c\\) is usually set to a small value. If \\(c=0\\) the leaky relu is just the relu. I set \\(c\\) to be 0.05 so that we can see the background image.\n\nplt.figure(figsize=[12.4, 6.2])\n\nK = img[200 : 270,225 : 322]\nplt.subplot(1, 3, 1)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(K,  cmap='gray', vmin=0, vmax=255)\n## I normalized it this way so that the convolution\n## numbers wouldn't be so big\n## Also, I put it to the 4th power, so it exactly finds \n## the eye.\nK = K ** 4\nK = K / K.sum()\nK = K - K.mean()\n\nimgC = sp.convolve2d(img, K)\nplt.subplot(1, 3, 2)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC)\nplt.title(\"Convolution\")\n\ntemp = imgC.copy()\n## Add a bias term of -15\ntemp -= 15\n## Perform a leaky relu\ntemp[np.where(temp < 0)] = temp[np.where(temp < 0)] * .05\n\nplt.subplot(1, 3, 3)\nplt.imshow(temp)\nplt.xticks([])\nplt.yticks([])\nplt.title(\"LRELU of convolution + bias\")\n\nText(0.5, 1.0, 'LRELU of convolution + bias')\n\n\n\n\n\nBecause of how convolutions work, this will find this eye anywhere in the image. Here we just add another eye somewhere else and repeat the convolution.\n\nplt.figure(figsize=[12.4, 6.2])\n\n#put another eye in the image\nimgCopy = img.copy()\nimgCopy[60 : 130, 85 : 182] = img[200 : 270,225 : 322]\nplt.subplot(1, 2, 1)\nplt.imshow(imgCopy,  cmap='gray', vmin=0, vmax=255)\nplt.xticks([])\nplt.yticks([])\n\nimgC = sp.convolve2d(imgCopy, K)\n\nplt.subplot(1, 2, 2)\ntemp = imgC.copy()\n## Add a bias term of -15\ntemp -= 15\n## Perform a leaky relu\ntemp[np.where(temp < 0)] = temp[np.where(temp < 0)] * .05\n\nplt.subplot(1, 2, 2)\nplt.imshow(temp)\nplt.xticks([])\nplt.yticks([])\nplt.title(\"LRELU of convolution + bias\")\n\nText(0.5, 1.0, 'LRELU of convolution + bias')\n\n\n\n\n\nSo, we found a custom kernel that highlights this specific feature in images. Convnets layers learn the kernel. That is, CNNs learn the image that gets convolved with the previous layer to produce the next one. Here’s a really great pictorial guide by Sumit Saha.\nNow, let’s discuss some specific vocabulary used in CNNs.\n\nPadding zero padding just like we discussed for 1D transformations\nPooling pooling, often max pooling, is a dimension reduction technique, taking the max in little blocks.\nstride length instead of sliding the kernel by moving it one pixel at a time, move it more to increase computational efficiency and reduce the size of the output convolution."
  },
  {
    "objectID": "tooling_nlp.html#on-text-encoding",
    "href": "tooling_nlp.html#on-text-encoding",
    "title": "20  NLP",
    "section": "20.1 On text encoding",
    "text": "20.1 On text encoding\nComputers represent characters via character encodings. Basically, a mapping from a binary representation to a character symbol value. There’s many character encodings and most often our text system autodetects them.\nCharacter encodings existed a long time before computers, since people seem to always want to represent letters as numbers. On a computer, the program has to interpret the character code and display it in a way that we want. ASCII represents characters as 7 bits, (unicode) UTF-8, UTF-16 and UTF-32 represent characters as 8, 16, and 32 bits. Of course the greater the bit representation, the larger the character set that can be represented. ASCII contains only the usual keyboard characters whereas Unicode contains much more.\nHere’s some info about python unicode. For example, python’s default character encoding is UTF-8. So strings automatically allow UTF-8 characters\n\nprint(\"😉\")\nprint(\"\\N{WINKING FACE}\")\nprint(\"\\U0001F609\")\n\n😉\n😉\n😉\n\n\nUnicode characters can be variable names, but emojis can’t. (However, see here where they magically do import pandas as 🐼). Greek letters are fair game.\n\nimport numpy as np\n\nx = np.array([1, 5, 8, 10, 2, 4])\nμ = x.mean()\nσ = x.std()\nprint( (μ, σ) )\n\n(5.0, 3.1622776601683795)\n\n\nThis is annoying for programming. But, useful for labeling plot axes and whatnot."
  },
  {
    "objectID": "tooling_nlp.html#regular-expressions",
    "href": "tooling_nlp.html#regular-expressions",
    "title": "20  NLP",
    "section": "20.2 Regular expressions",
    "text": "20.2 Regular expressions\nRegular expressions (regexes) are simply advanced pattern matching tools. Regular expressions represent regular characters, a, b, …, z, A, B, …, Z exactly like you’d think. But, some other characters, ., +, and others are metacharacters that are used to help us match things. A backslash, , can be used to directly reference a metacharacter, so “\\+”, references the character “+”. It can also be used to escape a regular character. So “\\d” references the set of digits rather than the character d.\nHonestly, I have to look up regex definitions everytime I use them. Here’s a table from wikipedia reduced. Here’s the python regex docs.\n\n\n\n\n\n\n\nMetacharacter\nDescription\n\n\n\n\n^\nMatches the starting position within the string.\n\n\n.\nMatches any single character.\n\n\n[ ]\nMatches a single character that is contained within the brackets.\n\n\n[^ ]\nMatches a single character that is not contained within the brackets.\n\n\n$\nMatches the ending position of the string or the position just before a string-ending newline.\n\n\n( )\nDefines a marked subexpression.\n\n\n\nMatches what the nth marked subexpression matched, where n is a digit from 1 to 9\n\n\n*\nMatches the preceding element zero or more times.\n\n\n{m,n}\nMatches the preceding element at least m and not more than n times.\n\n\n\nMany search functions in R and python allow for regexes. Especially, the re python module. Try these simple examples and look at the methods associated with the output. It contains methods for where the regex occurs in the string, the whole input string itself etc. It returns nil if there’s no match.\n\nimport re\nout = re.search('[hcb]a', 'hat')\nprint( (out.group(0), out.string) )\nout = re.search('[hcb]a', 'phat')\nprint( (out.group(0), out.string) )\nout = re.search('[hcb]a', 'rat')\nprint(out)\n\n('ha', 'hat')\n('ha', 'phat')\nNone\n\n\n\nout = re.findall('[hcb]a', 'hatcathat')\nprint(out)\n\n['ha', 'ca', 'ha']\n\n\nWe can match any letter with “.”.\n\nout = re.search('.a', 'rat')\nprint( (out.group(0), out.string) )\nout = re.search('.a', 'phat')\nprint( (out.group(0), out.string) )\n\n('ra', 'rat')\n('ha', 'phat')\n\n\nWe can search for things like any number using a dash.\n\nout = re.search('subject[0-9].txt', 'subject3.txt')\nprint( (out.group(0), out.string) )\nout = re.search('subject[0-9][0-9][0-9].txt', 'subject015.txt')\nprint( (out.group(0), out.string) )\n\n('subject3.txt', 'subject3.txt')\n('subject015.txt', 'subject015.txt')\n\n\n’CHARACTER*’ will match any number of the character, inncluding 0, whereas CHAR+ matches one or more times.\n\nout = re.search('subject0*.txt', 'subject.txt')\nprint( (out.group(0), out.string) )\nout = re.search('subject0*.txt', 'subject000.txt')\nprint( (out.group(0), out.string) )\nout = re.search('subject0*.txt', 'subject123.txt')\nprint(out)\nout = re.search('subject0+.txt', 'subject.txt')\nprint(out)\nout = re.search('subject0+.txt', 'subject000.txt')\nprint( (out.group(0), out.string) )\nout = re.search('subject0+.txt', 'subject123.txt')\nprint(out)\n\n('subject.txt', 'subject.txt')\n('subject000.txt', 'subject000.txt')\nNone\nNone\n('subject000.txt', 'subject000.txt')\nNone\n\n\nCHARACTER? matches one or zero times.\n\nout = re.search('subject0?.txt', 'subject.txt')\nprint( (out.group(0), out.string) )\nout = re.search('subject0?.txt', 'subject0.txt')\nprint( (out.group(0), out.string) )\nout = re.search('subject0?.txt', 'subject00.txt')\nprint(out)\n\n('subject.txt', 'subject.txt')\n('subject0.txt', 'subject0.txt')\nNone\n\n\nYou can string together regexes to obtain more complex searches. For example, the following searches for subject[GREATER THAN ONE NUMBER].txt\n\nout = re.search('[0-9]+.txt', 'subject501.txt')\nprint( (out.group(0), out.string) )\n\n('501.txt', 'subject501.txt')\n\n\nPython’s re library has several other methods than search including: match, findall(), finditer() (see here).\nThat’s enough regexes. My typical workflow for regexes is to simply relearn them every time I need them, since I don’t use them enough to get terribly fluent."
  },
  {
    "objectID": "tooling_nlp.html#natural-language-tool-kit",
    "href": "tooling_nlp.html#natural-language-tool-kit",
    "title": "20  NLP",
    "section": "20.3 Natural language tool kit",
    "text": "20.3 Natural language tool kit\nnltk is probably the most widely used natural language toolkit. To install nltk, just use conda or pip. However, I also had to run nltk.download() to download the extra stuff required for the package to run. I found this tutorial helpful.\n\nimport nltk\n\nTokenizing words and sentences. Consider this history and physical exam note from here.\n\nnote = \"\"\"HPI is a 76 yo man with h/o HTN, DM, and sleep apnea who presented to the ED complaining of chest pain. He states that the pain began the day before and consisted of a sharp pain that lasted around 30 seconds, followed by a dull pain that would last around 2 minutes. The pain was located over his left chest area somewhat near his shoulder. The onset of pain came while the patient was walking in his home. He did not sit and rest during the pain, but continued to do household chores. Later on in the afternoon he went to the gym where he walked 1 mile on the treadmill, rode the bike for 5 minutes, and swam in the pool. After returning from the gym he did some work out in the yard, cutting back some vines. He did not have any reoccurrences of chest pain while at the gym or later in the evening. The following morning (of his presentation to the ED) he noticed the pain as he was getting out of bed. Once again it was a dull pain, preceded by a short interval of a sharp pain. The patient did experience some tingling in his right arm after the pain ceased. He continued to have several episodes of the pain throughout the morning, so his daughter-in-law decided to take him to the ED around 12:30pm. The painful episodes did not increase in intensity or severity during this time. At the ED the patient was given nitroglycerin, which he claims helped alleviate the pain somewhat. HPI has not experienced any shortness of breath, nausea, or diaphoresis during these episodes of pain. He has never had chest pain in the past. He has been told “years ago” that he has a right bundle branch block and premature heart beats.\"\"\"\n\nLet’s tokenize this note by sentence and word.\n\nsentences = nltk.tokenize.sent_tokenize(note)\nwords = nltk.tokenize.word_tokenize(note)\nfor i in range(3):\n    print(sentences[i])\nprint(words)\n\nHPI is a 76 yo man with h/o HTN, DM, and sleep apnea who presented to the ED complaining of chest pain.\nHe states that the pain began the day before and consisted of a sharp pain that lasted around 30 seconds, followed by a dull pain that would last around 2 minutes.\nThe pain was located over his left chest area somewhat near his shoulder.\n['HPI', 'is', 'a', '76', 'yo', 'man', 'with', 'h/o', 'HTN', ',', 'DM', ',', 'and', 'sleep', 'apnea', 'who', 'presented', 'to', 'the', 'ED', 'complaining', 'of', 'chest', 'pain', '.', 'He', 'states', 'that', 'the', 'pain', 'began', 'the', 'day', 'before', 'and', 'consisted', 'of', 'a', 'sharp', 'pain', 'that', 'lasted', 'around', '30', 'seconds', ',', 'followed', 'by', 'a', 'dull', 'pain', 'that', 'would', 'last', 'around', '2', 'minutes', '.', 'The', 'pain', 'was', 'located', 'over', 'his', 'left', 'chest', 'area', 'somewhat', 'near', 'his', 'shoulder', '.', 'The', 'onset', 'of', 'pain', 'came', 'while', 'the', 'patient', 'was', 'walking', 'in', 'his', 'home', '.', 'He', 'did', 'not', 'sit', 'and', 'rest', 'during', 'the', 'pain', ',', 'but', 'continued', 'to', 'do', 'household', 'chores', '.', 'Later', 'on', 'in', 'the', 'afternoon', 'he', 'went', 'to', 'the', 'gym', 'where', 'he', 'walked', '1', 'mile', 'on', 'the', 'treadmill', ',', 'rode', 'the', 'bike', 'for', '5', 'minutes', ',', 'and', 'swam', 'in', 'the', 'pool', '.', 'After', 'returning', 'from', 'the', 'gym', 'he', 'did', 'some', 'work', 'out', 'in', 'the', 'yard', ',', 'cutting', 'back', 'some', 'vines', '.', 'He', 'did', 'not', 'have', 'any', 'reoccurrences', 'of', 'chest', 'pain', 'while', 'at', 'the', 'gym', 'or', 'later', 'in', 'the', 'evening', '.', 'The', 'following', 'morning', '(', 'of', 'his', 'presentation', 'to', 'the', 'ED', ')', 'he', 'noticed', 'the', 'pain', 'as', 'he', 'was', 'getting', 'out', 'of', 'bed', '.', 'Once', 'again', 'it', 'was', 'a', 'dull', 'pain', ',', 'preceded', 'by', 'a', 'short', 'interval', 'of', 'a', 'sharp', 'pain', '.', 'The', 'patient', 'did', 'experience', 'some', 'tingling', 'in', 'his', 'right', 'arm', 'after', 'the', 'pain', 'ceased', '.', 'He', 'continued', 'to', 'have', 'several', 'episodes', 'of', 'the', 'pain', 'throughout', 'the', 'morning', ',', 'so', 'his', 'daughter-in-law', 'decided', 'to', 'take', 'him', 'to', 'the', 'ED', 'around', '12:30pm', '.', 'The', 'painful', 'episodes', 'did', 'not', 'increase', 'in', 'intensity', 'or', 'severity', 'during', 'this', 'time', '.', 'At', 'the', 'ED', 'the', 'patient', 'was', 'given', 'nitroglycerin', ',', 'which', 'he', 'claims', 'helped', 'alleviate', 'the', 'pain', 'somewhat', '.', 'HPI', 'has', 'not', 'experienced', 'any', 'shortness', 'of', 'breath', ',', 'nausea', ',', 'or', 'diaphoresis', 'during', 'these', 'episodes', 'of', 'pain', '.', 'He', 'has', 'never', 'had', 'chest', 'pain', 'in', 'the', 'past', '.', 'He', 'has', 'been', 'told', '“', 'years', 'ago', '”', 'that', 'he', 'has', 'a', 'right', 'bundle', 'branch', 'block', 'and', 'premature', 'heart', 'beats', '.']\n\n\nLet’s filter out some common English filler words.\n\nfilter_words = set(nltk.corpus.stopwords.words(\"english\"))\nfiltered = [w for w in words if w in filter_words]\nprint(filtered[0 : 10])\n\n['is', 'a', 'with', 'and', 'who', 'to', 'the', 'of', 'that', 'the']\n\n\nWe can also reduce words to their stems, i.e. grammatical root form.\n\nstemmer = nltk.stem.PorterStemmer().stem\nprint(stemmer(\"Diagnosed\"))\nprint(stemmer(\"Diagnosing\"))\nprint(stemmer(\"diagnosed\"))\n\ndiagnos\ndiagnos\ndiagnos\n\n\nLemmatization reduces words to an underlying meaing.\n\nfrom nltk.stem import WordNetLemmatizer\nlemmatize = WordNetLemmatizer().lemmatize\n## Here v for verb\nprint(lemmatize(\"am\", pos = \"v\"))\nprint(lemmatize(\"are\", pos = \"v\"))\nprint(lemmatize(\"worst\", pos = \"n\"))\nprint(lemmatize(\"worst\", pos = \"a\"))\n\nbe\nbe\nworst\nbad"
  },
  {
    "objectID": "tooling_nlp.html#sentiment",
    "href": "tooling_nlp.html#sentiment",
    "title": "20  NLP",
    "section": "20.4 Sentiment",
    "text": "20.4 Sentiment\nSentiment is predicting a score about the tone of a text. The compound value is a numeric sentiment score.\n\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nsentiment = SentimentIntensityAnalyzer().polarity_scores\nprint(sentiment(note))\nprint(sentiment(\"Sentiment analysis is great!\"))\nprint(sentiment(\"The world is doomed.\"))\n\n{'neg': 0.214, 'neu': 0.786, 'pos': 0.0, 'compound': -0.9971}\n{'neg': 0.0, 'neu': 0.406, 'pos': 0.594, 'compound': 0.6588}\n{'neg': 0.583, 'neu': 0.417, 'pos': 0.0, 'compound': -0.6369}"
  },
  {
    "objectID": "specialized_rnns.html",
    "href": "specialized_rnns.html",
    "title": "40  RNNs",
    "section": "",
    "text": "It might be worth considering some existing common time series models first.\n\n40.0.1 Linear time sieres models\n\n40.0.1.1 Stochastic time series models\nPerhaps the most popular version of stochastic time series models is the autoregressive (AR) models. Here’s an AR(1) model \\[\nY_t = \\beta_0 + Y_{t-1}\\rho + \\epsilon_t\n\\]\nGeneralizations allow for longer lags, called AR(p) models where p is the number of lags. We could instead have the error terms have dependence in so called moving average (MA) models. Here’s an MA(1)\n\\[\nY_t = \\beta_0 + Y_{t-1}\\rho + \\epsilon_t + \\gamma \\epsilon_{t-1}\n\\] where, again, longer lags can be incorporated. We could combine these models in ARMA models. Here’s an ARMA(1,1) model\n\\[\nY_t = \\beta_0 + Y_{t-1}\\rho + \\epsilon_t + \\gamma \\epsilon_{t-1}\n\\]\nRNNs get their name since the hidden layers point to themselves. This is already well explored in time series analyses, where we can have models such as AR models:\nDifferencing, i.e. considering \\(Y_t - Y_{t-1}\\) can be thought of as looking at a linear approximation to the derivative of the \\(Y_t\\) process. Second order differencing simply looks at \\((Y_t - Y_{t-1}) - (Y_{t-1} - Y_{t-2})\\) is then approximating the second derivative and so on. ARIMA models look at ARMA models on differenced data. So, an ARIMA model can be specified with three numbers the AR part, the MA part and the differencing part.\nStochastic models are especially useful in things like finance for modeling asset prices. This is because efficient market theory suggests no explanatory variables are needed so looking at the time series and so modeling the data this way is often useful.\n\n\n40.0.1.2 Functional outcomes\nInstead of modeling the outcome time series as a stochastic process, we might be interested in modeling it as a function using a smoother. For example we might model \\[\nY_t = f(t) + \\epsilon = z_t \\beta + \\epsilon\n\\] where \\(z_t\\) is a basis element from a smoother matrix. An example is regression splines.\n\n\n40.0.1.3 Explanatory time series models\nConsider regression models for \\(Y ~|~ X\\) where \\(Y\\) and \\(X\\) are time series. We might consider concordant models\n\\[\nY_t = \\beta_0 + \\beta_1 X_t + \\epsilon_t\n\\]\nDistributed lag models \\[\nY_t = \\alpha + \\beta_0 X_t + \\beta_1 X_{t-1} + \\ldots + \\beta_p X_{t-p} + \\epsilon_t\n\\]\nMarkov models model \\(Y_t ~|~ Y_{t-1}, \\mbox{Other covariates}\\). So, we can model \\[\nY_t = \\alpha + \\beta_0 Y_{t-1} + \\epsilon_t.\n\\]\nNot unlike the MA models, in all of these cases we can model dependence in the \\(\\epsilon\\) terms.\n\n\n40.0.1.4 Functional predictors\nConsider the instance where \\(Y\\) is not a time series, but \\(X_{i}\\) is for each \\(i\\). Let’s write this as a function, \\(x_i(t)\\). We then need to relate an entire time series to each \\(Y\\) value. We might consider models of the form \\[\nY_i = \\alpha + \\int \\beta(t) x_i(t) dt + \\epsilon_i\n\\]\n\n\n40.0.1.5 Summarizing linear time series models\n(base) bcaffo@penguin:~/sandboxes/advanced_ds4bio_book$ In summary, there’s quite a few ways to model time series data lineary. And you can combine methods and extend them to generalized linear model settings.\n\n\n\n40.0.2 RNNS\nRNNs have many variations, the same as linear time series models. They are called recurrent, since they point to themselves. Let’s look at a simple RNN where we have a time series, \\(X_{it}\\) and we want to predict a concordant time series, \\(y_t\\). Consider a model \\(h_t = \\mathrm{expit}(w_0 + w_1 h_{t-1} + w_2 x_{t})\\) and \\(\\hat Y_t = \\mathrm{expit}(w_3 + w_4 h_t)\\) (Elman 1990).\n\n\n\n\n\nNotice, the prior hidden nodes point to the subsequent nodes. This builds in the history of the network.\nWe can also have networks that point to single outcomes\n\n\n\n\n\n\n\n40.0.3 Example character prediction\nI used this tutorial by Eduardo Muñoz and the pytorch character prediction tutorial here.\nLet’s predict the last letter of words from Shakespeare’s writing from the previous words. Using this, you should be able to extrapolate how to create a character prediction algorithm and from that a word prediction algorithm.\nI got the training text from here with a simple url request.\n\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport string\nimport nltk\nfrom urllib import request\n\n\nurl = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\nurlresponse = request.urlopen(url)\ntext = urlresponse.read().decode(\"ascii\")\ntext_words = nltk.tokenize.word_tokenize(text.lower())\n\n##remove all of the non alpha characters like commas, periods ...\ntext_words = [word for word in text_words if word.isalpha() and len(word) > 2]\n\n## Test whether all 26 letters are represented\n#text_letters = set([l.lower() for l in text if l.isalpha()])\n#len(text_letters)\n## All characters are represented\n\n## The lowercase letters as a list\nimport string\nletters = string.ascii_lowercase\nn_letters = len(letters)\n\n\n## one hot encode each letter then create a matrix for each word\ndef word_encode(word):\n    n_word = len(word)\n    input_tensor = torch.zeros(len(word) - 1, 1, n_letters)\n    for i in range(n_word - 1):\n        l = word[i]\n        input_tensor[i, 0, letters.find(l)] = 1\n    output_category = letters.find(word[i + 1])\n    return input_tensor, output_category\n\n\n\ntest_word = text_words[0]\ntest_predictor, test_outcome = word_encode(test_word)\nprint(test_word)\nprint(test_predictor.shape)\nprint(test_outcome)\n\nfirst\ntorch.Size([4, 1, 26])\n19\n\n\nLet’s create a list of our predictor tensor and outcome categories.\n\nN = len(text_words)\npredictor_list = []\noutcome_list = []\nfor i in range(N):\n    w = text_words[i]\n    p, o = word_encode(w)\n    predictor_list.append(p)\n    outcome_list.append(o)\n\noutcome_tensor = torch.tensor(outcome_list)\n\nHere’s the RNN from their tutorial\n\nimport torch.nn as nn\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNN, self).__init__()\n\n        self.hidden_size = hidden_size\n\n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, hidden):\n        combined = torch.cat( (input, hidden), 1)\n        hidden = self.i2h(combined)\n        output = self.i2o(combined)\n        output = self.softmax(output)\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, self.hidden_size)\n\nn_hidden = 256\nrnn = RNN(n_letters, n_hidden, n_letters)\n\ntest_hidden = rnn.initHidden()\ntest_input_val = test_predictor[0]\nprint(test_input_val.shape)\nprint(rnn.forward(test_input_val, test_hidden))\n\ntorch.Size([1, 26])\n(tensor([[-3.1803, -3.2789, -3.2533, -3.2537, -3.2891, -3.2862, -3.3189, -3.2194,\n         -3.3158, -3.2975, -3.3076, -3.2724, -3.2852, -3.2251, -3.2482, -3.2114,\n         -3.2493, -3.2677, -3.2239, -3.2592, -3.1936, -3.3327, -3.2823, -3.2639,\n         -3.2185, -3.1977]], grad_fn=<LogSoftmaxBackward0>), tensor([[-0.1070, -0.0379,  0.0169, -0.0042,  0.0628, -0.0100,  0.0315, -0.0042,\n          0.0349, -0.0448, -0.0615, -0.0234,  0.0150,  0.0250, -0.0120,  0.0105,\n          0.0598, -0.0473, -0.0358,  0.0326, -0.0115,  0.0209,  0.0089, -0.0766,\n          0.0552,  0.0444, -0.0265, -0.0194, -0.0424, -0.0161, -0.0026, -0.0084,\n          0.0525,  0.0104, -0.0715,  0.0362, -0.0867, -0.0600, -0.1001,  0.0685,\n         -0.0062,  0.0154,  0.0114, -0.0177, -0.0501,  0.0096, -0.0549,  0.0803,\n          0.0466,  0.0235, -0.0578,  0.0625, -0.0287,  0.0132, -0.0568, -0.0634,\n         -0.0392, -0.0421,  0.0487, -0.0157,  0.0187, -0.0010, -0.0473,  0.0106,\n         -0.0553,  0.0181, -0.0310, -0.0298,  0.0420, -0.0327, -0.0311, -0.0106,\n          0.0172,  0.0204, -0.0004,  0.1023, -0.0953, -0.0602, -0.0585,  0.0363,\n         -0.0886,  0.0391, -0.0340, -0.0016,  0.0297,  0.0113,  0.0096, -0.1062,\n         -0.0344, -0.0180,  0.0079,  0.0352,  0.0653,  0.0076, -0.0073, -0.0042,\n          0.0560,  0.0809, -0.0151, -0.0019,  0.0441, -0.0403, -0.0752,  0.0060,\n          0.0204,  0.0168, -0.0245,  0.0438, -0.0568,  0.0639, -0.0858,  0.0764,\n          0.0151, -0.0076,  0.0518, -0.0254,  0.0645, -0.0604, -0.0319, -0.0254,\n          0.0236, -0.0662,  0.0555, -0.0583, -0.0048,  0.0493,  0.0240,  0.0283,\n          0.0138,  0.0196,  0.0560,  0.0570,  0.0323, -0.0024, -0.0506, -0.0253,\n         -0.0077, -0.0514, -0.0235, -0.0058,  0.0887,  0.0686,  0.0094, -0.0074,\n         -0.0102, -0.0272, -0.0759,  0.0202,  0.0188, -0.0657, -0.0201, -0.0905,\n          0.0504,  0.0258,  0.0079, -0.0050,  0.0143, -0.0989, -0.0491, -0.0006,\n          0.0350, -0.0163, -0.1030,  0.0089, -0.1100, -0.0383,  0.0210, -0.0244,\n         -0.0090,  0.0884,  0.0577, -0.0117,  0.0350,  0.0237,  0.0429, -0.0472,\n         -0.0952,  0.0107,  0.0043, -0.0067, -0.0161, -0.0860, -0.0517, -0.0949,\n          0.0140,  0.1022, -0.0241,  0.0028, -0.0123, -0.0195,  0.0395, -0.0196,\n         -0.0143,  0.0256,  0.0029, -0.0144,  0.0460,  0.1099, -0.0348, -0.0863,\n         -0.0130,  0.0615, -0.0021,  0.0342,  0.0981,  0.0201,  0.0005, -0.0121,\n         -0.1108, -0.0207,  0.0781, -0.0858, -0.0295,  0.0112, -0.0067,  0.0189,\n         -0.0120,  0.0400,  0.0198, -0.0356,  0.0347, -0.0117,  0.0069,  0.0224,\n          0.0129,  0.0462, -0.0688, -0.0992, -0.0407, -0.0126,  0.0014,  0.0842,\n          0.0105, -0.0417,  0.0961, -0.0559,  0.0045, -0.0082, -0.0711,  0.0476,\n          0.0297, -0.0223, -0.0043, -0.0344,  0.0752, -0.0631,  0.0900, -0.0336,\n          0.0220, -0.0164,  0.0062,  0.0737, -0.0420,  0.0764, -0.0432,  0.0335]],\n       grad_fn=<AddmmBackward0>))\n\n\n\nimport torch.optim as optim\n\ncriterion = nn.NLLLoss()\n#criterion = nn.CrossEntropyLoss()\n\nlearning_rate = 1e-4\nepochs = 1000 \nbatch_size = 100\n\noptimizer = optim.Adam(rnn.parameters(),  lr = learning_rate)\n\n## This runs the first few characters of a word\n## through the RNN\ndef predict(input_tensor):\n    prompt_length = input_tensor.size()[0]\n    h = rnn.initHidden()\n    for j in range(prompt_length):\n        o, h = rnn(input_tensor[j], h)\n    return o, h\n\n\nfor epoch in range(epochs):\n    ## grab a random batch by grabbing random indices\n    batch_indices = np.random.choice(N, batch_size)\n\n    ## initialize the predictions\n    output = torch.zeros(batch_size, n_letters)\n    ## run through the batch\n    for i in range(batch_size):\n        index = batch_indices[i]\n        input_tensor = predictor_list[index]\n        o, h = predict(input_tensor)\n        output[i,:] = o\n\n    rnn.zero_grad()\n    loss = criterion(output, outcome_tensor[batch_indices])\n    loss.backward()\n    optimizer.step()\n\nTry some words:\n\n## The confusion matrix\ncm = np.zeros( (n_letters, n_letters) )\nfor i in range(1000):\n    input_tensor = predictor_list[i]\n    output = predict(input_tensor)[0]\n    actual = letters.find(text_words[i][-1])\n    guess = torch.argmax(output)\n    cm[actual, guess] += 1\n\nplt.imshow(cm)\nplt.xticks([])\nplt.yticks([])\n\n([], [])\n\n\n\n\n\n\nnp.sum(np.diag(cm)) / np.sum(cm)\n\n0.415\n\n\n\n\n\n\nElman, Jeffrey L. 1990. “Finding Structure in Time.” Cognitive Science 14 (2): 179–211."
  },
  {
    "objectID": "intro_html.html",
    "href": "intro_html.html",
    "title": "4  HTML, CSS and javascript",
    "section": "",
    "text": "HTML is a markup language used by web browsers. HTML stands for hypetext markup language. Like all markup languages, it gives a text set of instructions that get interpreted into a nicer looking document. Other markup languages include XML, LaTeX, Org and markdown. (Yes, mark”down” is named as such since it’s a ultra-simple mark”up” language.)\nWe’ll need a little html knowledge since so much data science output is web-page oriented. Also, we’ll need to know a little about html to scrape web content. A web page typically has three elements: the html which gives the page structure and markup, css (cascading style sheets) for style and javascript for interactivity. We’ll cover a little html and javascript so that we can better understand certain data science products. However, you should take a web development course if you want in depth treatments.\nWe won’t spend much time talking about CSS. CSS gives a set of standards for the style of a web page. With CSS one can take the skeleton (HTML/JS) and dramatically change the style in the same way you could choose to play some sheet music in different ways. A quick tutorial on CSS can be found here.\nBack to HTML. An HTML document looks something like this. Take a file, insert the following code and give it the extension .html. Then, open it up in a browser.\n<!DOCTYPE html>\n<HTML>\n    <HEAD>\n        <TITLE> This is the web page title</TITLE>\n    </HEAD>\n    <BODY>\n        <H1>Heading 1</H1>\n        <H2>Heading 2</H2>\n        <P> Paragraph </P>\n        <CODE> CODE </CODE>\n    </BODY>\n</HTML>\nThe resulting document will look like the following\n\n\n   \n      \n      \n       Paragraph \n      CODE \n   \n\nAs you probably noticed, a bit of markup is something like <COMMAND>CONTENT</COMMAND>. The latter command has a forward slash. You should close your commands, even if your browser still renders the page like you like just because it makes for bad code not to. Also, someone else’s browser may not be as forgiving. Good code editors will help remind you to close your commands."
  },
  {
    "objectID": "tooling_nlp.html",
    "href": "tooling_nlp.html",
    "title": "20  NLP",
    "section": "",
    "text": "Computers represent characters via character encodings. Basically, a mapping from a binary representation to a character symbol value. There’s many character encodings and most often our text system autodetects them.\nCharacter encodings existed a long time before computers, since people seem to always want to represent letters as numbers. On a computer, the program has to interpret the character code and display it in a way that we want. ASCII represents characters as 7 bits, (unicode) UTF-8, UTF-16 and UTF-32 represent characters as 8, 16, and 32 bits. Of course the greater the bit representation, the larger the character set that can be represented. ASCII contains only the usual keyboard characters whereas Unicode contains much more.\nHere’s some info about python unicode. For example, python’s default character encoding is UTF-8. So strings automatically allow UTF-8 characters\n\nprint(\"😉\")\nprint(\"\\N{WINKING FACE}\")\nprint(\"\\U0001F609\")\n\n😉\n😉\n😉\n\n\nUnicode characters can be variable names, but emojis can’t. (However, see here where they magically do import pandas as 🐼). Greek letters are fair game.\n\nimport numpy as np\n\nx = np.array([1, 5, 8, 10, 2, 4])\nμ = x.mean()\nσ = x.std()\nprint( (μ, σ) )\n\n(5.0, 3.1622776601683795)\n\n\nThis is annoying for programming. But, useful for labeling plot axes and whatnot."
  },
  {
    "objectID": "supervised_regression.html",
    "href": "supervised_regression.html",
    "title": "28  Prediction with regression",
    "section": "",
    "text": "Recall, we discussed a strict threshold classifier with accuracy as the loss function. Now consider continuous prediction, we need a loss function. A reasonable strategy would be to minimize the squared distances between our predictions and the observed values. In other words, \\(\\sum_{i=1}^n (Y_i - \\hat \\mu_i)^2.\\)\nIf we were to dived this by \\(n\\), it would be the average of the squared errors, or the mean squared error (MSE). We can use minimizing the squared error both as a rule for finding a good prediction and as our evaluation strategy for held out data.\nWhat’s left is to figure out how to come up with \\(\\hat \\mu_i\\), our predictions for the observation \\(Y_i\\). We previously considered just a rescaled version of \\(X\\), our predictor, using regression through the origin. In this module, we’ll try a slightly more complex model that includes a location (intercept) shift and a scale factor (slope). The consequence will be to fit the best line, in a certain sense, through our \\(X\\), \\(Y\\) paired data.\nTo tie ourselves down with an example, consider the previous lecture’s example, consider trying to get the FLAIR value from the other, non-FLAIR, imaging values.\nLet’s look at the non-smoothed data (omitting the _10 and _20) using a pair plot. I’m color coding by whether or not the specific voxel is a lesion.\nT2 and PD (proton density) look pretty linearly related. Imagine a study where a researcher collected T2 but did not collect PD. Let’s try to predict their PD values from the T2 values using a line. We’ll use least squares as the loss function. Specifically\n\\[\n\\sum_{v=1}^V (PD_v - \\beta_0 - \\beta_1 T2_v)^2\n\\]\nwhere \\(v\\) stands for voxel and \\(PD_v\\) for the PD value at voxel \\(v\\), \\(T2_v\\) as the T2 value at voxel \\(v\\) and \\(\\beta_0\\) and \\(\\beta_1\\) are parameters that we have to learn.\nA general equation for fitting a line to data is\n\\[\n\\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2\n\\]\nwhere we want to use \\(X_i\\) to predict \\(Y_i\\).\nIt turns out that \\(\\beta_0\\) and \\(\\beta_1\\) have optimal solutions that we can write down. We get\n\\[\n\\hat \\beta_1 = Cor(X, Y) \\frac{SD_Y}{SD_X}\n\\]\nwhere \\(Cor(X, Y)\\) is the (Pearson) correlation between \\(X\\) and \\(Y\\) and \\(SD_X\\) is the standard deviation of \\(X\\) (and \\(SD_Y\\) is for \\(Y\\)). The intercept satisfies\n\\[\n\\hat \\beta_0 = \\bar Y - \\bar X \\hat \\beta_1\n\\]\nwhere \\(\\bar X\\) and \\(\\bar Y\\) are the means.\nNotice this latter equation reorganized is just\n\\[\n\\bar Y = \\hat \\beta_0 + \\bar X \\hat \\beta_1\n\\]\npointing out that the fitted line has to go through the point \\((\\bar X, \\bar Y)\\)."
  },
  {
    "objectID": "supervised_logistic.html",
    "href": "supervised_logistic.html",
    "title": "29  Logistic regression",
    "section": "",
    "text": "Suppose now that we want to predict the gold standard from the FLAIR values. Fitting a line seems weird, since the outcome can only be 0 or 1. A line would allow for arbitrarily small or large predictions. Similiarly, forcing the prediction to be exactly 0 or 1 leads to difficult optimization problems. A clever solution is to instead model\n\\[\nP(Y_i = 1 ~|~ X_i)\n\\]\nwhere \\(Y_i\\) is the gold standard value (0 or 1 for no lesion or lesion at that voxel, respectively) and \\(X_i\\) is the FLAIR value for voxel \\(i\\). This solves the problem somewhat nicely, but it still leaves some issues unresolved. For example, what does probability even mean in this context? And also probabilities are between 0 and 1, that’s better than exactly 0 or 1, but still would create problems.\nFor the probability, it’s generally a good idea to think about what you’re modeling as random in the context. In this case, we’re thinking of our voxels as a random sample of FLAIR and gold standard voxel values from some population. This is a meaningful benchmark even if it’s not true. We’ll find that often in statistics we model data as if it comes from a probability distribution when we know it didn’t. We simply know that the probability distribution is a useful model for thinking about the problem.\nAs for getting the probabilities from \\([0,1]\\) to \\((-\\infty, \\infty)\\), we need a function, preferably a monotonic one. The generally agreed upon choice is the logit (natural log of the odds) function. The logit function of a probability is defined as\n\\[\n\\eta = \\mathrm{logit}(p) = \\log\\{p / (1 - p)\\}\n\\]\nwhere \\(p\\) is the probability and \\(O = p/(1-p)\\) is called the odds. Note, you can go backwards from odds to probability with the function \\(p = O / (1 + O)\\). Odds are exactly as used in gambling. If the odds of bet at 1 to 99, then you are saying the probability is \\(1 / (99 + 1) = 1\\%\\).\nWhy use odds? There’s a couple of reasons why odds are uniquely interprettable. First, there are specific study designs where odds make more sense than probabilities, particularly retrospective ones. Secondly, odds are unique in binomial models where they work out to be particularly tractible to work with. Finally, odds have a unique gambling interpretation. That is, it gives the ratio of a one dollar risk to the return in a fair bet. (A fair bet is where the expected return is 0.) So, when a horse track gives the odds on a horse to be 99 to 1, they are saying that you would get $99 dollars if you bet one dollar and the horse won. This is an implied probability of 99 / (99 + 1) = 99% that the horse loses and 1% probability that the horse wins. Note they don’t usually express it as a fraction, they usually espress it as value to 1 or 1 to value. So they would say 99 to 1 (odds against) or 1 to 99 (odds for) so you can easily see how much you’d win for a dollar bet.\nYou can go backwards from the logit function to the probability with the expit function. That is, if \\(\\eta\\) is defined as above, then\n\\[\np = \\frac{e^{\\eta}}{1 + e^\\eta} = \\frac{1}{1 + e^{-\\eta}}.\n\\]\nThis is sometimes called the expit function or sigmoid.\nWe model the log of the odds as linear. This is called logistic regression.\n\\[\n\\eta = \\mathrm{logit}\\left\\{ P(Y = 1 ~|~ X) \\right\\}\n= \\beta_0 + \\beta_1 X.\n\\]\nThe nice part about this model is that \\(e^\\beta_1\\) has the nice interpretation of the odds ratio associated with a one unit change in \\(X\\).\nThis is great, but we still need a function of the probabilities to optimize. We’ll use the cross entropy.\n\\[\n-\\sum_{i=1}^n \\left[Y_i \\log\\{P(Y_i = 1 ~|~ X_i)\\} + (1 - Y_i) \\log\\{1 - P(Y_i = 1 ~|~ X_i)\\}\\right].\n\\]\nThis function has the interpretation of being the negative of the log of the probabilities assuming the \\(Y_i\\) are independent. This model doesn’t have to hold for the minimization to be useful.\nPlugging our logit model in, the cross entropy now looks like\n\\[\n-\\sum_{i=1}^n \\left[\n  Y_i \\eta_i + \\log\\left\\{\\frac{1}{1 + e^\\eta_i} \\right\\} \\right].\n\\]\nThis is the function that we minimize to perform logistic regression. Later on, we’ll worry about how to minimize this function. However, today, let’s fit logistic regression to some data.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model as lm\n## this sets some style parameters\nsns.set()\n\n## Read in the data and display a few rows\ndat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv\")\n\n\n## Plot the data\nsns.scatterplot(x = 'FLAIR', y = 'GOLD_Lesions', data = dat)\n\n<AxesSubplot:xlabel='FLAIR', ylabel='GOLD_Lesions'>\n\n\n\n\n\nLet’s now fit the model. Again we’re going to split into training and test data. But, now we’re not going to do it manually since we have to load a library that has a function to do this.\n\nx = dat[['FLAIR']]\ny = dat.GOLD_Lesions\ntrainFraction = .75\n\n## Once again hold out some data\nsample = np.random.uniform(size = 100) < trainFraction\nxtrain = x[ sample]\nytrain = y[ sample]\nxtest =  x[~sample]\nytest =  y[~sample]\n\n\nlr = lm.LogisticRegression(fit_intercept=True, penalty='none')\nfit = lr.fit(xtrain, ytrain)\n\nLet’s look at the parameters fit from the model\n\nbeta0, beta1 = [fit.intercept_[0], fit.coef_[0][0]]\n[beta0, beta1]\n\n[-3.667006903989242, 2.3050322468114026]\n\n\n\nn = 1000\nxplot = np.linspace(-1, 5, n)\neta = beta0 + beta1 * xplot\np = 1 / (1 + np.exp(-eta))\n\nsns.scatterplot(x = 'FLAIR', y = 'GOLD_Lesions', data = dat, hue = 'GOLD_Lesions')\nsns.lineplot(x = xplot, y = p)\n\n## Of course, scikit has a predict\n## function so that you don't have to do this manually\n#yplot = fit.predict_proba(xplot.reshape((n, 1)))\n#sns.lineplot(xplot, yplot[:, 1])\n\n<AxesSubplot:xlabel='FLAIR', ylabel='GOLD_Lesions'>\n\n\n\n\n\nNow let’s evaluate the test set.\n\n## This predicts the classes using a 50% probability cutoff\nyhat_test = fit.predict(xtest)\n\n## double checking that if you want\n#all(yhat_test == (fit.predict_proba(xtest)[:, 1] > .5))\n\naccuracy = np.mean(yhat_test == ytest)\nsensitivity = np.mean(yhat_test[ytest == 1] == ytest[ytest == 1])\nspecificity = np.mean(yhat_test[ytest == 0] == ytest[ytest == 0])\nnp.round([accuracy, sensitivity, specificity], 3)\n\narray([0.625, 0.667, 0.583])\n\n\n\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\n\nptest = fit.predict_proba(xtest)[:, 1]\nfpr, tpr, thresholds = roc_curve(ytest, ptest)\nroc_auc = auc(fpr, tpr)\nplt.figure()\nlw = 2 \nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "supervised_multivariable.html",
    "href": "supervised_multivariable.html",
    "title": "30  Linear separable models",
    "section": "",
    "text": "We’ve now covered two ways to do prediction with a single variable, classification using logistic regression and prediction using a line and least squares. What if we have several predictiors?\nIn both the logistic and linear regression models, we had a linear predictor, specifically,\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_i.\n\\]\nIn the continuous case, we were modeling the expected value of the outcomes as linear. In the binary case, we were assuming that the naturual logarithm of the odds of a 1 outcome was linear.\nTo estimate the unknown parameters, \\(\\beta_0\\) and \\(\\beta_1\\) we minimized\n\\[\n\\sum_{i=1}^n || y_i - \\eta_i||^2\n\\]\nin the linear case and\n\\[\n-\\sum_{i=1}^n \\left[\n  Y_i \\eta_i + \\log\\left\\{\\frac{1}{1 + e^{\\eta_i}} \\right\\} \\right].\n\\]\nin the binary outcome case (where, recall, \\(\\eta_i\\) depends on the parameters). We can easily extend these models to multiple predictors by assuming that the impact of the multiple predictors is linear and separable. That is,\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots \\beta_{p-1} x_{p-1,i}\n\\]\nIf we think about this as vectors and matrices, we obtain\n\\[\n\\eta = X \\beta\n\\]\nwhere \\(\\eta\\) is an \\(n \\times 1\\) vector, \\(X\\) is an \\(n \\times p\\) matrix with \\(i,j\\) entry \\(x_{ij}\\) and \\(\\beta\\) is a \\(p\\times 1\\) vector with entries \\(\\beta_j\\).\nLet’s look at the voxel-level data that we’ve been working with. First let’s load the data.\nLet’s first try to fit the proton density data from the other imaging data. I’m going to use the statsmodels version of linear models since it has a nice format for dataframes.\nNow let’s evaluate our prediction. Here, we’re not going to classify as 0 or 1, but rather estimate the prediction. Note, we then would need to pick a threshold to have a classifier. We could use .5 as our threshold. However, it’s often the case that we don’t necessarily want to threshold at specifically that level. A solution for evalution is to plot how the sensitivity and specificity change by the threshold.\nIn other words, consider the triplets \\[\n(t, sens(t), spec(t))\n\\] where \\(t\\) is the threshold, sens(t) is the sensitivity at threshold \\(t\\), spec(t) is the specificity at threshold t.\nNecessarily, the sensitivity and specificity"
  },
  {
    "objectID": "supervised_dft.html",
    "href": "supervised_dft.html",
    "title": "31  DFT",
    "section": "",
    "text": "The Fourier transform is one of the key tools in Biomedical Data Science. Its namesake is Jean Baptiste Fourier, who was a 18th century French mathemetician who made fundamental discoveries into harmonic analysis. Its fair to say that Fourier’s discoveries are some of the most fundamental in all of a mathematics and engineering and is the foundation for signal processing.\nOne of his main discoveries was the Fourier series, the idea that a function can be decomposed into building blocks of trigonometric functions.\n\n\n\nLet \\(<,>\\) be a so-called inner product. For example \\(<a, b> = \\sum_{m=1}^n a_m b_m\\) if \\(a\\) and \\(b\\) are two vectors. But, \\(<a, b>=\\int_0^1 a(t)b(t)dt\\) if \\(a\\) and \\(b\\) are two functions on \\([0,1]\\). (There is a nice generality between Fourier results on data and Fourier results on functions and other spaces. However, we’ll largely focus on discrete data, so think of the first definition.) We can define the norm as \\(<a, a> = ||a||^2\\), so that, the distance between two vectors is \\(||a-b||\\).\nConsider a basis, that is a set of vectors, \\(b_k\\) so that \\(||b_k|| = 1\\) and \\(<b_k, b_j>= I(k=j)\\) and the set of vectors, \\({\\cal H}\\), that can be written as \\(\\sum_{k=1}^k b_k c_k\\) for some constants \\(c_k\\), then for any element \\(x\\in H\\) we have that the best approximation using any subset of the indices, \\(S\\), is of the form\n\\[\n\\sum_{k\\in S} b_k <b_k, x>.\n\\]\nFor real vectors and the basis we consider, every vector can be written as a sum of the basis elements. You can have weird functions that can’t be written out as sums of the basis elements, but they’re weird functions."
  },
  {
    "objectID": "intro_git.html",
    "href": "intro_git.html",
    "title": "3  Git, github",
    "section": "",
    "text": "In the live versions of these classes, we use the version control system git and git hosting service github. If you work in data science should have a working knowledge of both git and at least one cloud hosting service (like github). For git, you work in a repository, which is basically a project directory on your computer with some extra files that help git work. Git is then used for version control so that you keep track of states of your project. Github, is a hosting service for git repositories. Typically, you have your repository on your computer and you coordinate it with the one on the server. Github is just one of several hosting services, bitbucket is another, or you could even relatively easily start your own. However, github has front end web services that allows you to interact with your remote repository easily. This is very convenient."
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "Python",
    "section": "",
    "text": "First, you’ll need a place to program python for data analysis. Python has a dizzying array of options for its use. A first choice is whether you’ll use python locally (installed on your computer) or in the cloud. The cloud options take care of a lot of installation problems, in exchange for a loss in control and typically much less computing resources unless you pay for stronger cloud computing. A second choice is whether you’ll look program in notebook environments or in a straight code editors. Notebooks mix code and documentation and are especially useful for programming for data analyses. More pure code editors and integrated development environments are preferable for writing software. Here’s a list of some of things I’ve tried and liked."
  },
  {
    "objectID": "statistics_optimization.html",
    "href": "statistics_optimization.html",
    "title": "25  Optimization",
    "section": "",
    "text": "Not all problems have closed form opimization results. Instead, we have to rely on algorithms to numerically optmize our parameteris. To fix our discussion, let \\(l(\\theta)\\) be a negative log likelihood or loss function that we want to minimize. Equivalently, they could be the log likelihood or the negative of a loss function (gain function) that we want to optimize. Since we’ll be focusing on machine learning, we’ll characterize the problem in the terms of minimization, since most ML algoirthms are focused on minmizing loss functions.\nConsider how we would normally minimize a function. We would typically find the root of the derivative. That is, solving \\(l'(\\theta) = 0\\). However, finding the root simply creates an equally hard problem. What about approximating \\(l'(\\theta)\\) with a line at the current estimate? That is, \\[\nl'(\\theta) \\approx l'\\left(\\theta^{(0)}\\right) + \\left(\\theta - \\theta^{(0)}\\right)l''\\left(\\theta^{(0)}\\right)  = 0,\n\\] with the approximation being motivated by Taylor’s theorem, where \\(l''\\) is the Hessian (second derivative matrix). Solving the right hand equality implies \\[\n\\theta^{(1)} = \\theta^{(0)} - l''\\left(\\theta^{(0)}\\right)^{-1} l'\\left(\\theta^{(0)}\\right).\n\\] The algorithm that then recenters the approximation at \\(\\theta^{(1)}\\) and performs another update and so on, is called Newton’s algorithm, which, as its name implies is a very old technique. This algorithm takes the form of heading in the opposite direction of the gradient (\\(- l'\\left(\\theta^{(0)}\\right)\\)) where the scale of the move is governed by the inverse of the second derivative.\nAs an example, consider the function \\(l(\\theta) = \\theta^p\\) for \\(p\\) an even number \\(\\geq 2\\). Then \\(l'(\\theta) = p\\theta^{p-1}\\) and \\(l''(\\theta) = p(p-1) \\theta^{p-2}\\). Then, the update is \\[\n\\theta^{(1)} = \\theta^{(0)} - \\theta^{(0)} / (p - 1) = \\theta^{(0)} (p-2)/(p-1)\n\\] implying \\(\\theta^{(n)} = \\theta^{(0)} [(p-2)/ (p - 1)]^n\\), which clearly converges to 0, the minimum. It converges in one iteration at \\(p=2\\). The size of the jump at which one moves along the linear approximation is the inverse of the second derivative, i.e. \\(1 / [p(p-1) \\theta^{p-2}]\\). Thus, one moves more the less convex \\(l\\) is around the minimum. In this case, this results in a convergence rate of \\([(p-2) / (p - 1)]^{n}\\).\nLet’s try it out for \\(p=4\\). Here’s the core of the code:"
  },
  {
    "objectID": "statistics_optimization.html#section",
    "href": "statistics_optimization.html#section",
    "title": "25  Optimization",
    "section": "25.1 ",
    "text": "25.1"
  },
  {
    "objectID": "statistics_optimization.html#gradient-descent",
    "href": "statistics_optimization.html#gradient-descent",
    "title": "25  Optimization",
    "section": "25.2 Gradient descent",
    "text": "25.2 Gradient descent\nOften, we can’t calculate a second derivative, as will be the case with neural networks. We then replace the step size in Newton’s algorithm with a less optimal step size: \\[\n\\theta^{(1)} = \\theta^{(0)} - \\epsilon \\times l'\\left(\\theta^{(0)}\\right)\n\\] where \\(\\epsilon\\) is the so-called ``learning rate’’.\nConsider our example of trying to minimize \\(\\theta^p\\). Then, our update is \\[\n\\theta^{(1)} = \\theta^{(0)} - \\epsilon \\times p \\left[\\theta^{(0)}\\right]^{p-1}.\n\\]\nLet’s try it out for a few different values of \\(\\epsilon\\). The core of the code is simply:\n\nfor i in range(noiter):\n    theta = theta - epsilon * p * theta ** (p-1)\n\nHere we show the convergence for \\(\\epsilon = .1\\) (blue line), \\(\\epsilon = .01\\) (red line) and \\(\\epsilon = .001\\) (green line)."
  },
  {
    "objectID": "statistics_optimization.html#example",
    "href": "statistics_optimization.html#example",
    "title": "25  Optimization",
    "section": "25.1 Example",
    "text": "25.1 Example\nPoisson regression makes for a good example. Consider a model where \\(Y_i \\sim \\mbox{Poisson}(\\mu_i)\\) where \\(\\log(\\mu_i) = x_i^t \\theta\\) for covariate vector \\(x_i\\). Then the negative log-likelihood associated with the data up to additive constants in \\(\\theta\\) is:\n\\[\nl(\\theta) = -\\sum_{i=1}^n \\left[\n  y_i x_i^t \\theta - e^{x_i^t \\theta} \\right]\n\\]\nThus, \\[\nl'(\\theta) = -\\sum_{i=1}^n \\left[y_i x_i -  e^{x_i^t \\theta} x_i\\right]\n= - \\mathbf{X}^t \\mathbf{y} + \\mathbf{X}^t e^{\\mathbf{X}^t \\theta}\n\\] \\[\nl''(\\theta) = \\sum_{i=1}^n e^{x_i^t \\theta} x_i x_i^t = \\mathbf{X}^t \\mathrm{Diag}\\left(e^{\\mathbf{X}^t \\theta}\\right) \\mathbf{X}\n\\] where \\(\\mathbf{X}\\) contains rows \\(x_i^t\\) and \\(e^{\\mathbf{X}^t \\theta}\\) is a vector with elements \\(\\mathbf{X}^t \\theta\\). Therefore, the update function to go from the current value of \\(\\theta\\) to the next is:\n\\[\nU(\\theta) = \\theta +\n\\left[\\mathbf{X}^t \\mathrm{Diag}\\left(e^{\\mathbf{X}^t \\theta}\\right) \\mathbf{X}\\right ]^{-1} \\mathbf{X}^t \\left[ \\mathbf{y} - e^{\\mathbf{X}^t \\theta}\\right].\n\\] Thus, our update shifts the current value by a fit from a weighted linear model. In this case, the linear model has outcome \\(\\mathbf{y} - e^{\\mathbf{X}^t \\theta}\\) (which is \\(\\mathbf{y} - E_\\theta[\\mathbf{y}]\\)) design matrix \\(\\mathbf{X}\\) and weights \\(e^{\\mathbf{X}^t \\theta}\\) (which are \\(\\mathrm{Var}_\\theta(\\mathbf{Y})\\)).\nConsider an example using the Prussian Horse Kick data. We’ll use rpy2 to load the data in from R and plot it. We’ll group the data over other variables and simply fit a Poisson loglinear model with just an intercept and slope term.\n\nfrom rpy2.robjects.packages import importr, data\nfrom rpy2.robjects.pandas2ri import py2rpy, rpy2py\nsns.set()\n\npscl = importr('pscl')\nprussian_r = data(pscl).fetch('prussian')['prussian']\nprussian = rpy2py(prussian_r).drop('corp', axis = 1).groupby('year').sum().reset_index()\n\nsns.scatterplot(x = 'year', y = 'y',  size = 'y', data = prussian);\n\n\n\n\nFirst let’s fit the model using statsmodels and print out the coeficients. We normalize the year variable as\n\\[\n\\frac{\\mbox{Year} - \\min(\\mbox{Year})}{\\max(\\mbox{Year}) - \\min(\\mbox{Year})}.\n\\]\nSo our slope coefficient is the proprotion of the total years under study rather than the raw year. This is done for several reasons. It helps us visualize the likelihood for one. Secondly, this is normal practice in deep learning to put coefficients on a common scale to help the algorithms have more unit-free starting points.\n\nfrom statsmodels.discrete.discrete_model import Poisson\nprussian['itc'] = 1\nprussian['year_normalized'] = (prussian['year'] - prussian['year'].min()) / (prussian['year'].max() - prussian['year'].min())\n\ny = prussian['y'].to_numpy()\nx = prussian[ ['itc', 'year_normalized'] ].to_numpy()\nout = Poisson(endog = y, exog = x, ).fit()\nprint(out.summary())\n\nOptimization terminated successfully.\n         Current function value: 2.921766\n         Iterations 4\n                          Poisson Regression Results                          \n==============================================================================\nDep. Variable:                      y   No. Observations:                   20\nModel:                        Poisson   Df Residuals:                       18\nMethod:                           MLE   Df Model:                            1\nDate:                Mon, 01 Jan 2024   Pseudo R-squ.:                 0.01919\nTime:                        09:23:46   Log-Likelihood:                -58.435\nconverged:                       True   LL-Null:                       -59.579\nCovariance Type:            nonrobust   LLR p-value:                    0.1305\n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.0983      0.145     14.502      0.000       1.815       2.382\nx1             0.3565      0.236      1.509      0.131      -0.106       0.819\n==============================================================================\n\n\nNow, let’s define the variables and fit the model using numpy. The error is the norm of the step size for that iteration.\n\nerror = np.inf\ntolerance = .0001\ntheta = np.array([0, 1])\nnosteps = 0\nwhile (error > tolerance):\n    eta = x @ theta\n    mu = np.exp(eta)\n    var = np.linalg.inv(x.transpose() @ np.diag(mu) @ x) \n    step =  var @ x.transpose() @ (y - mu)\n    theta = theta + step\n    error = np.linalg.norm(step)\n    nosteps += 1\nprint( [theta, nosteps])\n\n[array([2.09827498, 0.35652129]), 10]\n\n\nNotice we get the same answer as the optimization routine in statsmodels. In this setting, the second derivative is more than just useful for fast optimization. Specifically, we can get the standard error of the coefficients witht the square root of the diagonal of the \\(\\left[\\mathbf{X}^t \\mathrm{Diag}\\left(e^{\\mathbf{X}^t \\theta}\\right) \\mathbf{X}\\right ]^{-1}\\) term on convergence. This is the empirical estimate of the asymptotic standard error for the MLE.\n\nprint(np.sqrt(np.diag(var)))\n\n[0.14469197 0.23618894]\n\n\nHere’s the likelihood, which is very narrow, and the path of the estimates as Newton’s algorithm runs."
  },
  {
    "objectID": "data_sqlite.html",
    "href": "data_sqlite.html",
    "title": "11  SQL via sqlite",
    "section": "",
    "text": "In this page, we’ll cover some of the basics of SQL (structured querry language) by working through some examples. SQL is a set of language standards for databases, so we have to choose a specific implementation. We’ll use sqlite for this purpose. As its name implies, sqlite is a small implementation of SQL.\nIn my linux implementation, sqlite3 was pre-installed. Here’s a tutorial on installing for windows. Sqlite3 is a single file.\nWe’ll first create a database at the command line. Notice when we create a file\nPerforming an ls in the current working directory now shows the file class.db. Everything else we discuss below assumes working in the sqlite command prompt.\nTo work with sqlite, it’s nice to work with a development environment specifically created for sql. Specifically, one with nice highlighting and autocompletion. Since I’m writing these notes in jupyter, I’m just pasting code output.\nSqlite has SQL commands, which must be typed with a semicolon at the end, and sqlite specific commands, which begin with a period and the pragma commands, which are also sqlite specific. This is good to remember, since some things will be portable to other SQL implementations and others not. ]"
  },
  {
    "objectID": "graphics_theory.html",
    "href": "graphics_theory.html",
    "title": "18  Advanced: theory of graphical display",
    "section": "",
    "text": "One of the main design arguments for the graphical display of information is data / ink maximization Tufte (1990). This is the idea that idea that as much of the “ink” (non-background pixels) of the plot as possible should be displaying data.\nData/ink maximalization has been criticized empirically. For example, Inbar, Tractinsky, and Meyer (2007) conducted a study with 87 undergraduates and found a clear preference for the non-maximized variations. Another line of argument discusses the “paradox of simplicity” Norman (2007), Eytam, Tractinsky, and Lowengart (2017), whereby we have a strong aesthetic preference for simplicity, but also want flexibility and maximum utility.\n\nBertin (1983)\n\n\n\n\nWickham et al. (2010)\n\n\n\n\n\nCleveland (1987)\nCleveland and McGill (1984)\nCleveland and Devlin (1980)\nCarswell (1992)\nCleveland and McGill (1986)\nMagical thinking Diaconis (2006)"
  },
  {
    "objectID": "graphics_theory.html#implementation",
    "href": "graphics_theory.html#implementation",
    "title": "18  Advanced: theory of graphical display",
    "section": "18.2 Implementation",
    "text": "18.2 Implementation\n\n18.2.1 Grammar of graphics\n\nWilkinson (2012)\nWilkinson (2013)\nWickham (2010)\n\n\n\n18.2.2 Narative storytelling\nEdward and Jeffrey (Segel and Heer (2010)) argue regarding the use of modern interactive tools in data narrative storytelling. They give seven canonical genres of narrative visulation."
  },
  {
    "objectID": "graphics_theory.html#graph-galleries-and-further-reading",
    "href": "graphics_theory.html#graph-galleries-and-further-reading",
    "title": "18  Advanced: theory of graphical display",
    "section": "18.3 Graph galleries and further reading",
    "text": "18.3 Graph galleries and further reading\n\n18.3.1 Further reading\n\nKarl Broman on How to display data badly\nKarl Broman Data Vizualization\nKarl Broman 10 worst plots\nKarl Broman Data visualization\n\n\n\n18.3.2 Graph galleries\n\nR graph gallery\nMatplotlib graph gallery\nPlotly\nD3 gallery\nVega gallery\nSeaborn\n\n\n\n18.3.3 Historically famous graphics\n\nhttps://medium.com/stotle-inc/the-greatest-graph-in-history-1155e0c25671Z\nhttps://plotlygraphs.medium.com/seven-modern-remakes-of-the-most-famous-graphs-ever-made-8ef30da1ab00\nhttps://www.datavis.ca/gallery/historical.php\nhttps://towardsdatascience.com/a-short-history-of-data-visualisation-de2f81ed0b23\nhttps://www.tableau.com/learn/articles/best-beautiful-data-visualization-examples\n\n\n\n18.3.4 Infographics in the media\n\nhttps://www.nytimes.com/spotlight/graphics\nJHU covid map\n\n\n\n\n\n\nBertin, Jacques. 1983. Semiology of Graphics. University of Wisconsin press.\n\n\nCarswell, C Melody. 1992. “Choosing Specifiers: An Evaluation of the Basic Tasks Model of Graphical Perception.” Human Factors 34 (5): 535–54.\n\n\nCleveland, William S. 1987. “Research in Statistical Graphics.” Journal of the American Statistical Association 82 (398): 419–23.\n\n\nCleveland, William S, and Susan J Devlin. 1980. “Calendar Effects in Monthly Time Series: Detection by Spectrum Analysis and Graphical Methods.” Journal of the American Statistical Association 75 (371): 487–96.\n\n\nCleveland, William S, and Robert McGill. 1984. “The Many Faces of a Scatterplot.” Journal of the American Statistical Association 79 (388): 807–22.\n\n\n———. 1986. “An Experiment in Graphical Perception.” International Journal of Man-Machine Studies 25 (5): 491–500.\n\n\nDiaconis, Persi. 2006. “Theories of Data Analysis: From Magical Thinking Through Classical Statistics.” Exploring Data Tables, Trends, and Shapes, 1–36.\n\n\nEytam, Eleanor, Noam Tractinsky, and Oded Lowengart. 2017. “The Paradox of Simplicity: Effects of Role on the Preference and Choice of Product Visual Simplicity Level.” International Journal of Human-Computer Studies 105: 43–55.\n\n\nInbar, Ohad, Noam Tractinsky, and Joachim Meyer. 2007. “Minimalism in Information Visualization: Attitudes Towards Maximizing the Data-Ink Ratio.” In Proceedings of the 14th European Conference on Cognitive Ergonomics: Invent! Explore!, 185–88.\n\n\nNorman, Donald A. 2007. “Simplicity Is Highly Overrated.” Interactions 14 (2): 40–41.\n\n\nSegel, Edward, and Jeffrey Heer. 2010. “Narrative Visualization: Telling Stories with Data.” IEEE Transactions on Visualization and Computer Graphics 16 (6): 1139–48.\n\n\nTufte, ER. 1990. “Data-Ink Maximization and Graphical Design.” Oikos, 130–44.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28.\n\n\nWickham, Hadley, Dianne Cook, Heike Hofmann, and Andreas Buja. 2010. “Graphical Inference for Infovis.” IEEE Transactions on Visualization and Computer Graphics 16 (6): 973–79.\n\n\nWilkinson, Leland. 2012. “The Grammar of Graphics.” In Handbook of Computational Statistics, 375–414. Springer.\n\n\n———. 2013. The Grammar of Graphics. Springer Science & Business Media."
  },
  {
    "objectID": "graphics_images.html",
    "href": "graphics_images.html",
    "title": "19  Working with images",
    "section": "",
    "text": "Images broadly come in two types, vector and raster. Vector graphics are in formats like pdf, eps, svg and raster graphics are like jpeg, gif, png. Vector graphics store the image constructs and shapes. So, a vector graphics renderer can zoom in indefinitely on a shape and its edges will appear sharp. Vector fonts work this way. Raster graphics basically store a matrix and the pixels on the screen show the values of that matrix. Bitmapped fonts work this way. Of course, vector graphics have to be converted to raster to be actually displayed by the computer. Finally, some rater graphics formats have compression, which we won’t really discuss."
  },
  {
    "objectID": "statistics_causal.html",
    "href": "statistics_causal.html",
    "title": "26  Causal DAGs",
    "section": "",
    "text": "Causal models differ from associational models in that they codify causal directions not just associations. In our program, you might have learned of the use of propensity scores, counterfactuals or randomization to study causality. There, typically the goal is to make causal statements with as few assumptions as possible or at least understanding the assumptions. Typically, the object of study is the estimation of an effect avergated over covarites.\nCausal graphs take a different approach, even if they wind up at the same place. Here, the goal is to postulate hyptothetical causal relationships and use those hypothetical relationships to estimate causal effects.\n\n\nA graph, \\(G\\) is a collection of nodes, say \\(V=\\{1,\\ldots, p\\}\\) and a set edges between the nodes, i.e. a set of elements \\((i,j)\\). The graph is directed if \\((i,j)\\) is considered different then \\((j,i)\\).\nNode \\(i\\) is a parent of node \\(j\\) if \\((i,j) \\in E\\) and \\((j,i)\\notin E\\). Similarly, node \\(i\\) is a child of node \\(j\\) if \\((j,i) \\in E\\) and \\((i,j)\\notin E\\). A node is a descendant of another if it is a child, or a child of a child and so on.\n\n\n\nDAGs define a unique factorization (set of independence relationships) with compatible probability models. I find it useful to think of causal DAGs in the terms of structural causal models (SCMs). Such models demonstrate an example of a generative models that statisfy the DAG and the have clear connections with the probabability models. An SCM over a collection of variables, \\(X=(X_1, \\ldots, X_p)\\), postulates a set of functional relationships \\[\nX_j = f(P_j, \\epsilon_j)\n\\] where \\(P_j\\) are the antecedent causes of \\(X_j\\), called the parents of \\(X_j\\), and \\(\\epsilon_j\\) is an accumulation of variables treated as mutally independent. This defines a directed graph, \\(G\\) say, where a graph is collection of vertices corresponding to our variables, \\(V=\\{1,\\ldots, p\\}\\), corresponding to the \\(X_i\\), and edges, \\(E\\), which is a set of ordered pairs of nodes.\n\n\n\n\n\nIn this case, \\(P_1 = \\{\\}\\), \\(P_2 = \\{1\\}\\) and \\(P_3 = \\{1,2\\}\\). DAGs in general define the independence assumptions associated with compatible probability models. SCMs are such an example that clearly define compatible probability models. Of course, given a large enough cross-sectional sample, we can estimate the joint distribution of \\(P(X_1,\\ldots, X_p)\\) and all of its conditionals. Disregarding statistical variation, which can be accounted for using traditional inferential methods, these conditionals should agree with the independence relationships from the DAG, if the DAG is correct. This yields a fruitful way to consider probability models. For example one could use DAGs as a heuristic and see how the observed data agrees with the independence relationships in compatible probability models implied by the DAG.\nBy itself, this does not create any causal claims. However, the following strategy could. Postulate a causal model, like the SCM, consider the independence relationships implied by the SCM, compares those indepnence relationships with those seen in the observed data. This gives us a method to falsify causal models using the data.\nOne specific way in which we use the assumptions is to investigate how the graph changes when we fix a node at a specific value, like an intevention, thus breaking its association with its parents. This operation is conceptual, but at times we can relate probabilities associated with interventions that were not realized. Consider an instrance where where \\(X_1\\) is a collection of confounders, \\(X_2\\) is an exposure and \\(X_3\\) is an outcome. Ideally, we’d like to know \\[\nP(X_3 ~|~ do(X_2) = x_2)\n\\] That is, the impact on the response if we were to set the exposure to \\(e_0\\).\n\n\n\nBefore we talk about interventions, let’s consider discussing compatibility of the hypothetical directed graph and our observed data. Return to our previous diagram.\n\n\\(X1\\) is a confounder betweend \\(X2\\) and \\(X3\\)\n\\(X2\\) is a mediator between \\(X1\\) and \\(X3\\)\n\\(X3\\) is a collider between \\(X1\\) and \\(X2\\)\n\nConsider an example. \\(X1\\) is having a BMI > 35, \\(X2\\) is sleep disordered breathing and \\(X3\\) is hypertension.\n\n\n\n\n\nHere if we’re studying whether SDB causes HTN, BMI35 confounds the relationship as a possible common cause of both. We would need to adjust for BMI35 to make sure the association between SDB and HTN isn’t just due to this common cause.\nIf we were studying whether BMI35 causes HTN, we might be interested in how much of that effect is mediated (indirectly) through SDB and how much is directly from BMI35.\nIf we are studying the relationship between BMI35 and SDB directly, adjusting for HTN may cause an association. Consider the (fictitious) case where there is a large number of people who have SDB who are not obese, yet all have hypertension, for whatever the reason. Then, among the HTN, there could be a negative association between BMI35 and SDB, because of the large collection of patients would who have SDB and are not obese and the same for obese and not hyptertensive. That is, adjusting for HTN created an association. This is an example of Berkson’s paradox. This is a somewhat contrived example, but hopefully you get the point. The wikipedia article has a funny example where they consider \\(X_1\\) is whether or not the hamburger is good at a fast food restaurant, \\(X_2\\) is whether or not the fries are good and \\(X_3\\) is whether or not people eat there. Since few people would eat at a place where both the hamburger and fries are bad, conditioning on \\(X_3\\) can create a negative association.\nThe main point here is that adjusting for colliders may open up a pathway between the nodes.\nA path between two nodes \\(n_1\\) and \\(n_k\\) is a sequence of nodes, \\(v_1, v_2,\\ldots v_{k}\\), where \\(v_{i}\\) and \\(v_{i+1}\\) are connected. The path is directed if \\(v_{i}\\) points to \\(v_{i+1}\\) for \\(i=1,\\ldots,k\\). A graph is a Directed Acyclic Graph (DAG) if all edges are directed and there are no two nodes \\(v_i\\) and \\(v_j\\) with a directed path in both directions.\nA path between \\(v_1\\) and \\(v_k\\), \\(v_1,\\ldots, v_k\\), is blocked by a set of nodes, \\(S\\), if for some \\(v_j\\) in \\(S\\)\n\n\\(v_j\\in S\\) and \\(v_k\\) is a mediator or confounder between \\(v_{j-1}\\) and \\(v_{j+1}\\) in either direction or\n\\(v_j\\notin S\\) and all of the descendants of \\(v_j \\notin S\\) and \\(v_{j}\\) is a collider between \\(v_{j-1}\\) and \\(v_{j+1}\\).\n\nIn other words, a path is blocked if a mediator or confounder is included in \\(S\\) or a collider and all of its descendants is excluded from \\(S\\).\nFor 1. this is equivalent to saying one of\n\n\\(v_{j-1}\\rightarrow v_{j} \\rightarrow v_{j+1}\\)\n\\(v_{j-1}\\leftarrow v_{j} \\leftarrow v_{j+1}\\)\n\\(v_{j-1}\\leftarrow v_{j} \\rightarrow v_{j+1}\\)\n\nholds. For 2. recall a collider is \\(v_{j-1}\\rightarrow v_{j} \\leftarrow v_{j+1}\\).\nThis could be translated into the following statistical statement. Conditioning on a mediator or confounder or not conditioning on a collider blocks a path, conditioning on a collider opens a path.\nWe say that two nodes or groups of nodes are d-separated by a set of nodes, \\(S\\), if every path between nodes in the two groups is blocked by \\(S\\). d-separation is useful because it gives us conditional independence relationships in the sense that if \\(X_i\\) is d-separated with \\(X_j\\) given \\(S\\) then \\(X_i \\perp X_j ~|~ S\\) on all probability distribution compatible with the graph.\nConsider the following graph.\n\n\n\n\n\n\\(X_1\\) and \\(X_5\\) are conditionally indepndent given \\(X_2\\) and \\(X_3\\). Why? Conditioning on \\(X_2\\) blocks the paths \\(X_1 \\leftarrow X_2 \\rightarrow X_3 \\leftarrow X_5\\) even despite the part \\(X_2 \\rightarrow X_3 \\leftarrow X_5\\) is opened by conditioning on the collider, \\(X_3\\). Furthermore, conditioning on \\(X_2\\) or \\(X_3\\) blocks the path \\(X_1 \\leftarrow X_2 \\rightarrow X_3 \\rightarrow X_4 \\rightarrow X_6 \\leftarrow X_5\\). Finally, conditioning on \\(X_3\\) blocks the path \\(X_1 \\leftarrow X_3 \\leftarrow X_5\\).\nAnother interesting one is that \\(X_2\\) and \\(X_5\\) are marginally independent. This is because not conditioining on \\(X_3\\) blocks the path \\(X_2 \\rightarrow X_3 \\leftarrow X_5\\) and not conditioning on \\(X_6\\) blocks the path \\(X_2 \\rightarrow X_3 \\rightarrow X_4 \\rightarrow X_6 \\leftarrow X_5\\).\nHere’s the complete set of conditional independence relationships.\n\n\\(X_1\\) and \\(X_4\\) are d-separated by \\(\\{X_3\\}\\)\n\\(X_1\\) and \\(X_5\\) are d-separated by \\(\\{X_2, X_3\\}\\)\n\\(X_1\\) and \\(X_6\\) are d-separated by \\(\\{X_4, X_5\\}\\), \\(\\{X_3, X_5\\}\\), \\(\\{X_2, X_3\\}\\)\n\\(X_2\\) and \\(X_4\\) are d-separated by \\(\\{X_3\\}\\)\n\\(X_2\\) and \\(X_5\\) are d-separated by \\(\\{\\}\\) (the null set, i.e. they’re marginally independent).\n\\(X_2\\) and \\(X_6\\) are d-separated by \\(\\{X_4, X_5\\}\\), \\(\\{X_3, X_5\\}\\)\n\\(X_3\\) and \\(X_6\\) are d-separated by \\(\\{X_4, X_5\\}\\)\n\\(X_4\\) and \\(X_5\\) are d-separated by \\(X_3\\).\n\nThese all imply the independence relationships, such as \\(X_1 \\perp X_4 ~|~ X_3\\)."
  },
  {
    "objectID": "python_basic.html",
    "href": "python_basic.html",
    "title": "5  Python basics",
    "section": "",
    "text": "In this section, we will cover some basic python concepts. Python is an extremely quick language to learn, but like most programming languages, can take a long time to master. In this class, we’ll focus on a different style of programming than typical software development, programming with data. This will but less of a burden on us to be expert software developers in python, but some amount of base language knowledge is unavoidable. So, let’s get started learning some of the python programming basics. I’m going to assume you’ve programmed before in some language. If that isn’t the case, consider starting with a basic programming course of study before trying this book.\nA great resource for learning basic python is the python.org documentation https://docs.python.org/3/tutorial/index.html. My favorite programming resource of all time is the “Learn X in Y” tutorials. Here’s one for python https://learnxinyminutes.com/docs/python/.\nSome of the basic programming types in python are ints, floats, complex and Boolean. The command type tells us which. Here’s an example with 10 represented in 4 ways (int, float, string and complex) and the logical value True. Note, we’re using print to print out the result of the type command. If you’re typing directly into the python command line (called the repl, for read, evaluate, print, loop), you won’t need the print statements. But, if you’re using a notebook you probably will.\nIf you want an easy repl environment to program in, try https://replit.com/. For an easy notebook solution to try out, look at google colab, https://colab.research.google.com .\nThese types are our basic building blocks. There’s some other important basic types that build on these. We’ll cover these later, but to give you a teaser:\nTypes can be converted from one to another. For example, we might want to change our 10 into different types. Here’s some examples of converting the integer 10 into a float. First, we use the float function. Next we define a variable a that takes the integer value 10, then use a method associated with a to convert the type. If you’re unfamiliar with the second notation, don’t worry about that now, you’ll get very used to it as we work more in python.\nPython’s repl does all of the basic numerical calculations that you’d like. It does dynamic typing so that you can do things like add ints and floats. Here we show the basic operators, and note # is a comment in python.\nStrings are easy to work with in python. Type print(\"Hello World\") in the repl just to get that out of the way. Otherwise, + concatenates strings and brackets reference string elements. Here’s some examples. Remember counting starts at 0 and negative numbers count from the back.\nThe strings True and False are reserved for the respective Boolean values. The operators ==, >, <, >=, <= and != are the testing operators while and, or and is are Boolean operators. Here are some examples.\nDon’t define new variables called TRUE or FALSE or tRuE or FaLsE, or whatever, even though you can. Just get used to typing True and False the way python likes and don’t use similar named things for other reasons. Als, python as bitwise logical operators |, & and ~. On Boolean values, they work the same but differ in other circumstances. So, if you are unfamliar with bitwise operations, it’s probably better to stick to the word logical operators above."
  },
  {
    "objectID": "unsupervised_pca_ica.html",
    "href": "unsupervised_pca_ica.html",
    "title": "33  Unsupervised learning",
    "section": "",
    "text": "Let \\(\\{X_i\\}\\) for \\(i=1,\\ldots,n\\) be \\(p\\) random vectors with means \\((0,\\ldots,0)^t\\) and variance matrix \\(\\Sigma\\). Consider finding \\(v_1\\), a \\(p\\) dimensional vector with \\(||v_1|| = 1\\) so that \\(v_1^t \\Sigma v_1\\) is maximized. Notice this is equivalent to saying we want to maximize \\(\\mathrm{Var}( X_i^t V_1)\\). The well known solution to this equation is that \\(v_1\\) is the first eigenvector of \\(\\Sigma\\) and \\(\\lambda_1 = \\mathrm{Var}( X_i^t V_1)\\) is the associated eigenvalue. If \\(\\Sigma = V^t \\Lambda V\\) is the eigenvalue decomposition of where \\(V\\) are the eigenvectors and \\(\\Lambda\\) is a diagonal matrix of the eigenvalues ordered from greatest to least, then \\(v_1\\) corresponds to the first column of \\(V\\) and \\(\\lambda_1\\) corresponds to the first element of \\(\\Lambda\\). If one then finds \\(v_k\\) as the vector maximizing \\(v_k^t \\Sigma v_k\\) so that \\(v_k^t v_{k'} = I(k=k')\\), then the \\(v_k\\) are the columns of \\(V\\) and \\(v_k^t \\Sigma v_k = \\lambda_k\\) are the eigenvalues.\nNotice:\n\n\\(V \\Sigma V^t = \\Lambda\\) (i.e. \\(V\\) diagonalizess \\(\\Sigma\\))\n\\(\\mbox{Trace}(\\Sigma) = \\mbox{Trace}(\\Sigma V^t V) = \\mbox{Trace}(V \\Sigma V^t) = \\sum \\lambda_k\\) (i.e. the total variability is the sum of the eigenvalues)\nSince \\(V^t V = I\\), \\(V\\) is a rotation matrix. Thus, \\(V\\) rotates \\(X_i\\) in such a way that to maximize variability in the first dimension, then the second dimensions …\n\\(\\mbox{Cov}(X_i^t v_k, x_i^t v_{k'} )= \\mbox{Cov}(X_i^t v_k, x_i^t v_{k'} ) v_k^t \\mbox{Cov}(x_i, x_i^t) v_{k'} = v_k^t V v_{k'} = 0\\) if \\(k\\neq k'\\)\nAnother representation of \\(\\Sigma\\) is \\(\\sum_{k=1}^p \\lambda_i v_k v_k^t\\) by simply rewriting the matrix algebra of \\(V \\Lambda V^t\\).\nThe variables \\(U_i = V X_i\\) then: have uncorrelated elements (\\(\\mbox{Cov}(U_{ik}, U_{ik'}) = 0\\) for \\(k\\neq k'\\) by property 5), have the same total variability as the elements of \\(X_i\\) (\\(\\sum_k \\mbox{Var}(U_{ik}) = \\sum_k \\lambda_k = \\sum_k \\mbox{Var}(X_{ik})\\) by property 2), are a rotation of the \\(X_i\\), are ordered so that \\(U_{i1}\\) has the greatest amount of variability and so on.\n\nNotation:\n\nThe \\(\\lambda_k\\) are simply called the eigenvalues or principal components variation.\n\\(U_{ik} = X_i^t v_k\\) is called the principal component scores.\nThe \\(v_k\\) are called the principal component loadings or weights, with \\(v_1\\) being called the first principal component and so on.\n\nStatistical properties under the assumption that the \\(x_i\\) are iid with mean 0 and variance \\(\\Sigma\\)\n\n\\(E[U_{ik}]=0\\)\n\\(\\mbox{Var}(U_{ik}) = \\lambda_k\\)\n\\(\\mbox{Cov}(U_{ik}, U_{ik'}) = 0\\) if \\(k\\neq k'\\)\n\\(\\sum_{k=1}^p \\mbox{Var}(U_{ik}) = \\mbox{Trace}(\\Sigma)\\).\n\\(\\prod_{k=1}^p \\mbox{Var}(U_{ik}) = \\mbox{Det}(\\Sigma)\\)\n\n\n\nOf course, we’re describing PCA as a conceptual process. We realize \\(n\\) \\(p\\) dimensional vectors \\(x_1\\) to \\(x_n\\), typically organized in \\(X\\) a \\(n\\times p\\) matrix. If \\(X\\) is not mean 0, we typically demean it by calculating \\((I- J(J^t J)^{-1} J') X\\) where \\(J\\) is a vector of ones. Assume this is done. Then \\(\\frac{1}{n-1} X^t X = \\hat \\Sigma\\). Thus, our sample PCA is obtained via the eigenvalue decomposition \\(\\hat \\Sigma = \\hat V \\hat \\Lambda \\hat V^t\\) and our principal components obtained as $ X V$.\nWe can relate PCA to the SVD as follows. Let \\(\\frac{1}{\\sqrt{n-1}} X = \\hat U \\hat \\Lambda^{1/2} \\hat V^t\\) be the SVD of the scaled version of \\(X\\). Then note that \\[ \\hat \\Sigma = \\frac{1}{n-1} X^t X = \\hat V\n\\hat \\Lambda \\hat V^t \\] yields the sample covariance matrix eigenvalue decomposition.\n\n\n\nConsider the case where one of \\(n\\) or \\(p\\) is large. Let’s assume \\(n\\) is large. Then \\[\n\\frac{1}{n-1} X^t X = \\frac{1}{n-1} \\sum_i x_i x_i^t\n\\] As we learned in the chapter on HDF5, we can do sums like these without loading the entirety of \\(X\\) into memory. Thus, in this case, we can calculate the eigenvectors using only the small dimension. If, on the other hand, \\(p\\) is large and \\(n\\) is smaller, then we can calculate the eigenvalue decomposition of \\[\n\\frac{1}{n-1} X X^t = \\hat U \\hat \\Lambda \\hat U^t.\n\\] In either case, whether \\(U\\) or \\(V\\) is easier to get, we can then obtain the other via vectorized multiplication.\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport numpy.linalg as la\nfrom sklearn.decomposition import PCA\nimport urllib.request\nimport PIL\nimport numpy as np\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.utils.data as data\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.decomposition import FastICA\nfrom tqdm import tqdm\nimport medmnist\nfrom medmnist import INFO, Evaluator\nimport scipy\nimport IPython\n\n\nn = 1000\nmu = (0, 0)\nSigma = np.array([[1, .5], [.5, 1]])\nX = np.random.multivariate_normal( mean = mu, cov = Sigma, size = n)\n\nplt.scatter(X[:,0], X[:,1])\n\n<matplotlib.collections.PathCollection at 0x7e216e7974c0>\n\n\n\n\n\n\nX = X - X.mean(0)\nprint(X.mean(0))\nSigma_hat = np.matmul(np.transpose(X), X) / (n-1) \nSigma_hat\n\n[-1.63202785e-17  2.16493490e-17]\n\n\narray([[0.98992308, 0.51183395],\n       [0.51183395, 1.00050966]])\n\n\n\nevd = la.eig(Sigma_hat)\nlambda_ = evd[0]\nv_hat = evd[1]\nu_hat = np.matmul(X, np.transpose(v_hat))\nplt.scatter(u_hat[:,0], u_hat[:,1])\n\n<matplotlib.collections.PathCollection at 0x7e2165abc940>\n\n\n\n\n\nFit using scikitlearn’s function\n\npca = PCA(n_components = 2).fit(X)\nprint(pca.explained_variance_)\nprint(lambda_ )\n\n[1.50707769 0.48335505]\n[0.48335505 1.50707769]\n\n\n\n\n\nLet’s consider the melanoma dataset that we looked at before. First we read in the data as we have done before so we don’t show that code.\n\n\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\n\n\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\n\n\nNext, let’s get the data from the torch dataloader format back into an image array and a matrix with the image part (28, 28, 3) vectorized.\n\ndef loader_to_array(dataloader):\n  ## Read one iteration to get data\n  test_input, test_target = next(iter(dataloader))\n  ## Total number of training images\n  n = np.sum([inputs.shape[0] for inputs, targets in dataloader])\n  ## The dimensions of the images\n  imgdim = (test_input.shape[2], test_input.shape[3])\n  images = np.empty( (n, imgdim[0], imgdim[1], 3))\n\n  ## Read the data from the data loader into our numpy array\n  idx = 0\n  for inputs, targets in dataloader:\n    inputs = inputs.detach().numpy()\n    for j in range(inputs.shape[0]):\n      img = inputs[j,:,:,:]\n      ## get it out of pytorch format\n      img = np.transpose(img, (1, 2, 0))\n      images[idx,:,:,:] = img\n      matrix = images.reshape(n, 3 * np.prod(imgdim))\n      idx += 1\n  return images, matrix\n\ntrain_images, train_matrix = loader_to_array(train_loader)\ntest_images, test_matrix = loader_to_array(test_loader)\n\n## Demean the matrices\ntrain_mean = train_matrix.mean(0)\ntrain_matrix = train_matrix - train_mean\ntest_mean = test_matrix.mean(0)\ntest_matrix = test_matrix - test_mean\n\nNow let’s actually perform PCA using scikitlearn. We’ll plot the eigenvalues divided by their sums, \\(\\lambda_k / \\sum_{k'} \\lambda_{k'}\\). This is called a scree plot.\n\nfrom sklearn.decomposition import PCA\nn_comp = 10\npca = PCA(n_components = n_comp).fit(train_matrix)\nplt.plot(pca.explained_variance_ratio_)\n\n\n\n\nOften this is done by plotting the cummulative sum so that you can visualize how much variance is explained by including the top \\(k\\) components. Here I fit 10 components and they explain 85% of the variation.\n\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\n\n\n\n\nNote that the weights from the eigenvectors, \\(V\\), are images. We can plot these as images.\n\neigen_moles = pca.components_\n\n\n\n\n\n\nLet’s project our testing data onto the principal component basis created by our training data and see how it does. Let \\(X_{training} = U \\Lambda^{1/2} V^t\\) is the SVD of our training data. Then, we can convert ths scores, \\(U\\) back to \\(X_{training}\\) with the map \\(U \\rightarrow U \\lambda^{1/2} V\\). Or, if our scores are normalized, \\(U \\Lambda^{1/2}\\) then we simply multiply by \\(V^t\\). If we want to represent \\(X_{training}\\) by a lower dimensional summary, we just keep fewer columns of scores, then multiply by the same columns of \\(V\\). We could write this as \\(U_s = X_{training} V_S \\lambda^{-1/2}_S\\), where \\(S\\) refers to a subset of values of \\(k\\).\nNotice that \\(\\hat X_{training} = U_{S} V^t_S \\Lambda^{-1/2}_S = X_{training} V_S V_S^t\\) , \\(\\Lambda\\) and \\(V\\). Consider then an approximation to \\(X_{test}\\) as \\(\\hat X_{test} = X_{test} V_s V_S^t\\). Written otherwise \\[\n\\hat X_{i,test} = \\sum_{k \\in S} <x_{i,test}, v_k> v_k\n\\] which is the projection of subject \\(i\\)’s features into the linear space spanned by the basis defined by the principal component loadings.\nLet’s try this on our mole data.\n\ntest_matrix_fit = pca.inverse_transform(pca.transform(test_matrix))\nnp.mean(np.abs( test_matrix - test_matrix_fit))\n\n0.03792390910578823"
  },
  {
    "objectID": "graphics_advanced_interactive.html",
    "href": "graphics_advanced_interactive.html",
    "title": "17  Advanced interactive graphics: D3",
    "section": "",
    "text": "In your other DS courses, you’ve learned how to create static graphics uses R, ggplot, matplotlib, seaborn … You’ve probably also learned how to create client side interactive graphics using libraries like plotly and maybe also learned client-server interactivity with shiny, dash …\nIn this section we’re going to dig deeper into client side graphics, which are almost always done via html, css, javascript and a javascript plotting library. We’re going to focus on d3.js, a well known javascript library for creating interactive data visulalizations.\nTools like d3 are mostly for creating professional data web graphics. So, most of our daily graphics use will just use python/R/julia/matlab … or plotting libraries like plotly. Usually, you want to prototype graphics outside of d3. Here, we’ll give you a smidge of using d3 to get you started if your goal is to become a graphics expert."
  },
  {
    "objectID": "graphics_advanced_interactive.html#introduction-to-d3",
    "href": "graphics_advanced_interactive.html#introduction-to-d3",
    "title": "17  Advanced interactive graphics: D3",
    "section": "17.1 Introduction to D3",
    "text": "17.1 Introduction to D3\nLet’s get started. I’m going to assume that you have a basic knowledge of html, css and a little bit of javascript. D3 works by manipulating html elements. Let’s select every paragraph element in a document.\n<!DOCTYPE html>\n<html lang=\"en\">\n\n<head>\n    <script src=\"https://d3js.org/d3.v5.min.js\"></script>\n</head>\n\n<body>\n    <p> Advanced </p>\n    <p> Data science </p> \n        <script>\n            let pselect = d3.selectAll(\"p\")\n            //let pselect = d3.select(\"p\").style(\"color\", \"green\");\n            //let pselect = d3.selectAll(\"p\").style(\"color\", \"green\");\n        </script>\n    </body>\n</html>\nGoing forward, we’ll omit most of the html commands.\n\nThe command <script src=\"https://d3js.org/d3.v5.min.js\"></script> loads d3 from a CDN. You could also download it locally if you’d like.\nThe script let pselect = d3.selectAll(\"p\").style(\"color\", \"green\"); creates a variable pselect that is all of the html paragraph elements\nTry doing this, loading the web page, then try uncommenting each other script line in turn and refreshing\nIn chrome do Ctrl-shift-i to get the developer console and inspect the variable pselect.\nNesting select or selectAll will select elements within the selected elements.\nYou can also select by id or class."
  },
  {
    "objectID": "graphics_advanced_interactive.html#a-simple-example",
    "href": "graphics_advanced_interactive.html#a-simple-example",
    "title": "17  Advanced interactive graphics: D3",
    "section": "17.2 A simple example",
    "text": "17.2 A simple example\nLet’s go through an example where we plot brain volumetric ROI data on the log scale using D3.\n<style>\n    .bar {\n        background: #f5b634;\n        border: 4px solid #0769ad;\n        height: 20px;\n    }\n</style>\n<body>\n        <script>\n            let roiData = [\n                {\"roi\": \"Telencephalon_L\", \"volume\" : 531111},\n                {\"roi\": \"Telencephalon_R\", \"volume\" : 543404},\n                {\"roi\": \"Diencephalon_L\",  \"volume\" : 9683  },\n                {\"roi\": \"Diencephalon_R\",  \"volume\" : 9678  },\n                {\"roi\": \"Mesencephalon\",   \"volume\" : 10268 },\n                {\"roi\": \"Metencephalon\",   \"volume\" : 159402},\n                {\"roi\": \"Myelencephalon\",  \"volume\" : 4973  },\n                {\"roi\": \"CSF\",             \"volume\" : 109776}\n            ];\n    \n            let divSelection = d3.select(\"body\") \n                    .selectAll(\"div\")\n                    .data(roiData)\n                    .enter()\n                    .append('div')\n                    .attr(\"class\", \"bar\")\n                    .style(\"width\", (d) => {return Math.log(d.volume) * 20 + \"px\"; })\n                    .text(d => d.roi)\n                    .on(\"mouseover\", function(){\n                        d3.select(this)\n                        .style(\"background-color\", \"orange\");\n                    })\n                    .on(\"mouseout\", function(){\n                        d3.select(this)\n                        .style(\"background-color\",\"#33A2FF\" )\n                    })        </script>\n    </body>\n\nThe data(roiDat) selects our dataset\nThe enter() and append('div') commands add div elements to the html document, one per data element.\nThe attr method considers our bar stylesheet style\nThe style method changes the style so that the bars have the width of our data. The notation (d) => {return d.volume * .001 + \"px\"} is a function that selects the ROI element of the data, multiplies it by .001 then converts it to text with px at the end.\nThe text method at the end appends the text to our plot\nThe on methods say what to do when one mouses over and off the bars. You can see now that they turn orange then back. Remove the mouseout .on call and see what happens.\n\nThe output looks like this. Hover over a bar to test. (Look at the file in d3/roi1.html)"
  },
  {
    "objectID": "graphics_advanced_interactive.html#working-through-a-realistic-example",
    "href": "graphics_advanced_interactive.html#working-through-a-realistic-example",
    "title": "17  Advanced interactive graphics: D3",
    "section": "17.3 Working through a realistic example",
    "text": "17.3 Working through a realistic example\nUnder assets/kirby_pivot.csv is a dataset with the kirby 21 data pivoted to have regions as columns. Let’s work through a d3 example of ploting right versus left asymmetry in the telencephalon (the largest area of the brain including the cortex and central white matter).\nHere’s the scatterplot that I’ve got so far. For HW, add text labels to the point, or a tooltip that gives point information when you hover over it.\n\nThe code for the plot is in d3/roi2.html. Let’s go over some of the main parts of the d3 code here. First, we set up the graphic\nconst h = 500\nconst w = 500\n\n// create the background\nlet svg = d3.select(\"body\")\n    .append(\"svg\")\n    .attr(\"width\" , h)\n    .attr(\"height\", w);\nNext we load in the data. First, we create a function that does a little row processing for us. Honestly, it’s probably better to just do this in python/R/julia … beforehand, but it’s worth showing here. We create variables for the log ratio between the right and left hemispheres and the log of the geometric mean. We’ll use this to create a Tukey mean/difference plot of the log of the volumes.\n//create the variables we're interested in\nlet rowConverter = function(d) {\n    return {\n        id : d.id,\n        //y is going to be the log difference R-L\n        logratio : Math.log(parseFloat(d.Telencephalon_R)) - Math.log(parseFloat(d.Telencephalon_L)),\n        //x is going to be the average log \n        loggm : (Math.log(parseFloat(d.Telencephalon_L)) + Math.log(parseFloat(d.Telencephalon_R))) * .5\n    };\n    }\n\n//the location where I'm pulling the csv from\nlet dataloc = \"https://raw.githubusercontent.com/smart-stats/advanced_ds4bio_book/main/qbook/assets/kirby_pivot.csv\"\n\n//read in the data and parse the rows \nkirby_pivot = d3.csv(dataloc, rowConverter)\nModern js uses something called ‘promises’, which alllows for asynchronous evaluation. When we read in our csv file, it gets created as a promise and not an array like we need. The result is that our plotting commands need to then be called as a method from the promise object. The reason for this is so that it only uses the data when the data is actually loaded (i.e. promise fulfilled.) So, the plotting commmands for us look like this.\nkirby_pivot.then(dat => {\n    PLOTTING COMMANDS\n})\nJust a reminder that the notation d => g(d) is JS shorthand for function(d) {return g(d);} and is used heavily in d3 coding. Now let’s fill in PLOTTING COMMANDS. First, let’s fill in some utility functions. We get the range of our x and y values to help set up our axes. d3 scales map our function values to a range we want. So let’s create scale maps for x, y and color and then also set up axes using those scales. We’ll also go ahead on plot our axes so they’re on the bottom.\nmaxx = d3.max(dat, d => d.loggm)\nminx = d3.min(dat, d => d.loggm)\nmaxy = d3.max(dat, d => d.logratio)\nminy = d3.min(dat, d => d.logratio)\n\n//fudge is the boundary otherwise points get chopped off\nlet fudge = 50\n\nlet yScale = d3.scaleLinear()\n    .domain([miny, maxy])\n    .range([h-fudge, fudge])\n\nlet pointScale = d3.scaleLinear()\n    .domain([miny, maxy])\n    .range([5, 10])\n\nlet colorScale = d3.scaleLinear()\n    .domain([miny, maxy])\n    .range([0, 1])\n\n\nlet xScale = d3.scaleLinear()\n    .domain([minx, maxx])\n    .range([w-fudge, fudge]);\n\n// define the axes\nlet xaxis = d3.axisBottom().scale(xScale)\nlet yaxis = d3.axisLeft().scale(yScale)\nsvg.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", \"translate(0,\" + (h - fudge) + \")\")\n    .call(xaxis)\n\nsvg.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", \"translate(\" + fudge + \",0)\")\n    .call(yaxis)\nNow let’s create the plot. We’re going to add circles at each location, which is attributes cx and cy. Notice we use our previous defined scales to give their locations. Also, we’ll set the color and size relative to the logratio. Finally, when we mouseover a point, let’s change the radius then change it back when we mouseoff.\nsvg.selectAll(\"circle\")\n    .data(dat)\n    .enter()\n    .append(\"circle\")\n    .attr(\"cy\", d => yScale(d.logratio))\n    .attr(\"cx\", d => xScale(d.loggm))\n    .attr(\"r\",  d => pointScale(d.logratio))\n    .attr(\"fill\", d => d3.interpolateWarm(colorScale(d.logratio)))\n    .attr(\"stroke\", \"black\")\n    .on(\"mouseover\", function() {\n        d3.select(this)\n            .attr(\"r\", 30)\n        })\n    .on(\"mouseout\", function() {\n        d3.select(this)\n        .attr(\"r\",  d => pointScale(d.logratio))\n    })\nObviously, this is a lot of work for a simple scatterplot. The difference is that here you have total control over plotting and interactivity elements."
  },
  {
    "objectID": "graphics_advanced_interactive.html#observable-and-observable-plot",
    "href": "graphics_advanced_interactive.html#observable-and-observable-plot",
    "title": "17  Advanced interactive graphics: D3",
    "section": "17.4 Observable and Observable Plot",
    "text": "17.4 Observable and Observable Plot\nObserverable is a notebook for working with d3. It’s quite neat since mixing javascript coding in a web notebook, which itself is written in javascript, makes for an interesting setup. Typically, one would do the data preprocessing in R, python, julia … then do the advanced graphing in d3. In addition to accepting d3 as inputs, observable has a slightly higher set of utility functions called observable plot. (Quarto, which this document is in, allows for observable cells.) So, let’s read in some ROI data and plot it in observable plot. Note this is the average of the Type I Level I ROIs. Notice this is much easier than using d3 directly.\n\ndata = FileAttachment(\"assets/kirby_avg.csv\").csv();\nPlot.plot({\nmarks: [Plot.barY(data, {x: \"roi\", y: \"volume\", fill : 'roi'})],\n    x: {tickRotate: 45},\n    color: {scheme: \"spectral\"},    \n    height: 400,\n    width: 400,\n    marginBottom: 100\n\n})"
  },
  {
    "objectID": "graphics_advanced_interactive.html#links",
    "href": "graphics_advanced_interactive.html#links",
    "title": "17  Advanced interactive graphics: D3",
    "section": "17.5 Links",
    "text": "17.5 Links\n\nObservable is not javascript\nd3 tutorial.\nd3 gallery"
  },
  {
    "objectID": "graphics_advanced_interactive.html#homework",
    "href": "graphics_advanced_interactive.html#homework",
    "title": "17  Advanced interactive graphics: D3",
    "section": "17.6 Homework",
    "text": "17.6 Homework\n\nCreate a D3 graphic web page that displays a scatterplot of your chosing. Show point information on hover.\nOn the same web page, create a D3 graphic web page that displays a stacked bar chart for the Kirby 21 data. Hover data should show subject information and increase the size of the bar. Here’s a plotly version to get a sense.\n\n\nimport pandas as pd\nimport plotly.express as px\nimport numpy as np\ndat = pd.read_csv(\"https://raw.githubusercontent.com/smart-stats/ds4bio_book/main/book/assetts/kirby21.csv\").drop(['Unnamed: 0'], axis = 1)\ndat = dat.assign(id_char = dat.id.astype(str))\nfig = px.bar(dat, x = \"id_char\", y = \"volume\", color = \"roi\")\nfig.show()\n\n\n                                                \n\n\n\nSubmit your webpages and all supporting code to your assignment repo\nHere’s a hint to the HW in d3/hwHint.html"
  },
  {
    "objectID": "statistics_optimization.html#example-revisited-with-gradient-descent",
    "href": "statistics_optimization.html#example-revisited-with-gradient-descent",
    "title": "25  Optimization",
    "section": "25.3 Example revisited with gradient descent",
    "text": "25.3 Example revisited with gradient descent\nLet’s run the same data throw the gradient descent algorithm rather than Newton’s method. Here we set the learning rate to b 0.001 for a thousand iterations. The gist of the code is simply:\n\nepsilon = .001\nfor i in range(1000):\n    eta = x @ theta\n    mu = np.exp(eta)\n    step =  - epsilon * x.transpose() @ (y - mu)\n    theta = theta - step\n\nStopping the algorithm is a little trickier for gradient descent, since the change at each iteration is dependent on the learning rate. Small learning rates will lead to small step sizes. Here’s a plot of the covergence on top of contours of the likelihood."
  }
]