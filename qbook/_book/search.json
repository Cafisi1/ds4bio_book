[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introductory and intermediate data science for the bio/medical sciences using python",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This is a book for the Data Science for Bio/Biostat/Public Health/medical classes."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "intro.html#markdown",
    "href": "intro.html#markdown",
    "title": "Introduction",
    "section": "Markdown",
    "text": "Markdown\nBefore getting started, you need to learn some markdown. Markdown is a markup language (like HTML) that is absurdly easy. Every data scientist needs to know markdown. Fortunately, you’re five minutes away from knowing it. A markdown file is a text file that needs to be rendered to look nice. If you want an example, this page was written in markdown. To try it, go to google colab or a jupyter lab, create a markdown cell and start editing. Try this cheat sheet."
  },
  {
    "objectID": "intro.html#some-basic-unix",
    "href": "intro.html#some-basic-unix",
    "title": "Introduction",
    "section": "Some basic unix",
    "text": "Some basic unix\nSome basic unix commands will go a long way in the course, especially for when you’re working on a remote server. On windows, you can actually install a unix environment on so-called services for unix. So, on windows, you have three options for working with the command line, i) install a linux subsystem and use that, ii) use the DOS command prompt or iii) use powershell. The commands here would only work for option i. However, when I work on a windows system, I tend to just use options ii or iii. Here, we’ll assume you’re working on a linux or unix system, or windows services for linux and you’ll have to read up elsewhere if you want to learn windows proper terminal commands.\nTo get a unix terminal, you have several options. Since we’re promoting jupyter and jupyterlab, just open up the terminal on there. (Again, assuming you’re working on a unix/linux system.)\nThe first thing you should try is figuring out where you’re at. Do this by typing\nprompt> pwd\nThis will show you where you are in the directory structure. If you want to see the contents of the directory try these\nprompt> ls\nprompt> ls -al\nprompt> ls -alh\nAdding the flags -a lists everything, including directories with a weird character in front. The l gives the long format, which gives more information and the h changes the filesize lists to a more human readable format. I also like the option --color. What you get with-alh` is as follows.\ntotal 36K\ndrwxrwxrwx+ 7 codespace root      4.0K Feb 14 14:24 .\ndrwxr-xrwx+ 5 codespace root      4.0K Oct 19 15:21 ..\ndrwxrwxrwx+ 6 codespace codespace 4.0K Feb 14 14:31 book\ndrwxrwxrwx+ 8 codespace root      4.0K Feb 15 21:34 .git\n-rw-rw-rw-  1 codespace codespace  171 Feb 14 14:24 .gitignore\n-rw-rw-rw-  1 codespace codespace    0 Feb 14 14:23 .nojekyll\n-rw-rw-rw-  1 codespace codespace  444 Feb 14 14:24 README.md\ndrwxrwxrwx+ 3 codespace codespace 4.0K Feb 14 14:24 slides\ndrwxrwxrwx+ 7 codespace codespace 4.0K Oct 19 15:21 .venv\ndrwxrwxrwx+ 2 codespace codespace 4.0K Oct 19 15:23 .vscode\nThe drwxrwxrwx+1 looking columns give permissions d=directory, r=read, w=write and x=execute, the groups are owner (you), group, everyone. So a file that is -rw------- can be read and written to by the owner, but cannot be executed by anyone and no one else can read or write to it (except the superuser, who gets to do everything).\nTo change a directory, try the following\nprompt> cd DIRECTORY\nwhere DIRECTORY is the name of the directory that you want to change into. You can hit TAB to autocomplete names. The command\nprompt> mv PATH_TO_INPUT_FILE PATH_TO_OUTPUT_FILE\nmoves the file. This is also how you rename a file, since you could just do mv FILENAME1 FILENAME2 and change the name.\nThe unix command for removing things is rm. So\nrm FILENAME\ndeletes the file. Note linux really deletes things, so do this with some care. You can’t remove directories this way, instead you could do rmdir DIRECTORY, but the directory has to be empty. If you want to use rm to remove a directory and its contents, you can do rm -rf DIRECTORY. However, use this with care.\nFinally, I find it very useful to use wget to grab files from the internet. So, for example,\nwget https://URL.../FILENAME\nwill grab the file from that link. Super useful.\nThat’s enough unix to get you started. You’ll find as you use the terminal more and more, you’ll like it better and better. Eventually, you’ll find GUIs kind of frustrating."
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "3  Git, github",
    "section": "",
    "text": "In the live versions of these classes, we use the version control system git and git hosting service github. If you work in data science should have a working knowledge of both git and at least one cloud hosting service (like github). For git, you work in a repository, which is basically a project directory on your computer with some extra files that help git work. Git is then used for version control so that you keep track of states of your project. Github, is a hosting service for git repositories. Typically, you have your repository on your computer and you coordinate it with the one on the server. Github is just one of several hosting services, bitbucket is another, or you could even relatively easily start your own. However, github has front end web services that allows you to interact with your remote repository easily. This is very convenient."
  },
  {
    "objectID": "git.html#the-least-you-need-to-know",
    "href": "git.html#the-least-you-need-to-know",
    "title": "3  Git, github",
    "section": "3.1 The least you need to know",
    "text": "3.1 The least you need to know\nI’m not going to fully recreate git / github tutorials here; here’s one I recommend by Sean Kross. Instead, I’m going to go through a typical git / github workflow.\n\nInitialization I almost always initialize my git repository on github with a readme.md file.\nClone I typically clone the repository to my local computer using the command line or a local git gui that works with github, like this one. Note that you only have to clone the repo once. After it’s cloned you have a full local copy of the repository.\nadd new files to track and stage them after I’ve worked with them.\ncommit the changes to the local repository with a meaningful commit message.\npush the changes to the repository.\nIf there’s changes on the remote repository not represented in my local repository, I pull those changes to my local repo.\n\nFor larger projects, you’re likely working with multiple people, some of whom you’ve given access to your remote repository and some of whom you have not. The ones who can’t directly push to the remote repo might have their own version of the code and their own version on github. If they think you should incorporate those changes, they might issue a pull request to you. You can then opt to pull their changes into your repo on github, then pull them from github to your local repo. One of the reasons why services like github and bitbucket are so popular is that they make this coordination fairly easy, along with having nice project messaging and management tools.\nIn our class, we use github classroom. For github classroom, you’ll get a link to a repo to put your submission files into. When you push to the remote repository, you’ll have submitted. But, up to the due date you can make changes an push again."
  },
  {
    "objectID": "git.html#a-little-more-detail",
    "href": "git.html#a-little-more-detail",
    "title": "3  Git, github",
    "section": "3.2 A little more detail",
    "text": "3.2 A little more detail\n\n3.2.1 Getting started\nGit is a version control system invented by Linus Torvalds, the invetor of the linux operating system. A github repository is a directory that the software git is using to keep track of versions. Install git, it’s already installed on most cloud services and many operaiting systems. You can initialize a repository in a directory with the command git init in that directory. Honestly, I almost never start a git repo this way, because I’ve connected my repo to a cloud server. But, more on that later. After you’ve initialized it, your git repo is a locally version controlled system.\n\n\n3.2.2 Adding files\nOK, now you want to add some of your files to your repo. You have a directory You can add files with the command\ngit add FILENAME\nThe file FILENAME is now staged to be part of the repository, but isn’t added yet! Files that you haven’t added won’t be tracked. This is good, since there’s many files, llike log files, that we don’t want to track. You only need to add the files once, then they are being tracked. Try it out with a readme markdown file in your repository.\n\n\n3.2.3 Checking status\nOne of the most useful git commands is\ngit status\nThis gives the status of which files are being tracked, which are not and which have changes.\n\n\n3.2.4 Commiting changes\nYou have to actually commit your changes to the repository to have them represented. Committing changes is a local operation, subsequently, we’ll show you how to coordinate with a remote repository. Here’s the command I use to commit most often\ngit commit -a -m \"COMMIT MESSAGE\"\nHere the flag -a means commit all files staged for commits. You can also commit individual files or a collection of specific files. The -m \"COMMIT MESSAGE\" is the message associated with your commit. Make your commit messages more informative. If you don’t add -m github will bring up an editor for you to enter your commit message.\n\n\n3.2.5 Remote repositories\nGit becomes much more useful when it’s coordinated with a remote repository. Github is a server for hosting remote git repositories, there are several others, bitbucket is an example. Github also includes a great deal of functionality for working with the remote repository through the website. There’s many git and github guis that you could use if you want. I always liked to github one and magit in emacs.\n\n3.2.5.1 Connecting your remote and local repos\nYou need to connect your local repo to the remote repo. First create a version of the repo on github (actually then it gives you instructions). Then you can either\n\nClone the repo from the server git clone REMOTE SERVER\nConnect the server to your local repository\n\nFor 2. you use a command like:\ngit remote add origin git@github.com:USERNAME/REPONAME.git\nPersonally, I find it easier just to create the repo on github and clone it. You can clone over ssh or https, if you don’t know what ssh is use https and then switch over to ssh as you learn how to use it.\n\n\n3.2.5.2 Pushing changes\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n#import numpy as np\n#import sklearn as skl\n\nplt.figure(figsize=[2, 2])\nG = nx.DiGraph()\n\nG.add_node(\"GH\",    pos = (.5, 1) )\nG.add_node(\"Local\", pos = (.5, 0))\n#G.add_edge(\"GH\", \"Local\")\nG.add_edge(\"Local\", \"GH\")\nnx.draw(G, \n        nx.get_node_attributes(G, 'pos'), \n        with_labels=True, \n        font_weight='bold', \n        node_size = 2000,\n        node_color = \"lightblue\",\n        linewidths = 3)\nax= plt.gca()\nax.collections[0].set_edgecolor(\"#000000\")\nax.set_xlim([-.3, 1.3])\nax.set_ylim([-.3, 1.3])\nplt.show()\n\n\n\n\nYou push from the repo you’re working on to another (remote repo). So, for me, this typically means pushing from my local computer repo to github or to a scientific computing cluster.\ngit push origin main\nThe above command could be read as “push my new committed changes from my origin to the main branch on my remote repo”. Older repos had the default branch master instead of main. But, github switched in 2020 or so. So, some older documentation may not have this switch.\n\n\n3.2.5.3 Pulling changes\n\nplt.figure(figsize=[2, 2])\nG = nx.DiGraph()\n\nG.add_node(\"GH\",    pos = (.5, 1) )\nG.add_node(\"Local\", pos = (.5, 0))\nG.add_edge(\"GH\", \"Local\")\n#G.add_edge(\"Local\", \"GH\")\nnx.draw(G, \n        nx.get_node_attributes(G, 'pos'), \n        with_labels=True, \n        font_weight='bold', \n        node_size = 2000,\n        node_color = \"lightblue\",\n        linewidths = 3)\nax= plt.gca()\nax.collections[0].set_edgecolor(\"#000000\")\nax.set_xlim([-.3, 1.3])\nax.set_ylim([-.3, 1.3])\nplt.show()\n\n\n\n\nThere might be changes that you want represented locally that were changed on the remote repository; for example, you might edit a readme file through github’s web interface. The command for this is really easy, it’s just pull\ngit pull\nThe pain occurs if you have made local changes that are in conflict with the remote changes. I’d write about merge conflicts, but I can’t, I just can’t (shudder). [Here’s some docs on managing merge conflicts] (https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/addressing-merge-conflicts/resolving-a-merge-conflict-using-the-command-line).\n\n\n3.2.5.4 Cloning\n\nplt.figure(figsize=[3, 2])\nG = nx.DiGraph()\n\nG.add_node(\"GH1\",    pos = (.5, 1) )\nG.add_node(\"Local1\", pos = (.5, 0))\nG.add_node(\"Local2\", pos = (1.5, 0))\n\nG.add_edge(\"GH1\", \"Local1\")\nG.add_edge(\"Local1\", \"GH1\")\nG.add_edge(\"GH1\", \"Local2\")\n\n#G.add_edge(\"Local\", \"GH\")\nnx.draw(G, \n        nx.get_node_attributes(G, 'pos'), \n        with_labels=True, \n        font_weight='bold', \n        node_size = 2500,\n        node_color = \"lightblue\",\n        linewidths = 3)\nax= plt.gca()\nax.collections[0].set_edgecolor(\"#000000\")\nax.set_xlim([-.3, 2.3])\nax.set_ylim([-.3, 1.3])\nplt.show()\n\n\n\n\nTo clone a remote repository is to simply create a local copy where you can pull changes from the remote. If a repository is public and you own it, then you can clone it and push changes back to the remote. If you don’t, you can clone and pull changes, but then can’t push back to the remote.\nTry with the book repo, clone it so you have access to the files. In the figure below the owner can pull and push to the repo. In the picture below, consider that you clone GH1 off of github to you. You can pull updates from GH1 but can’t push changes to GH1. The owner of GH1 of course can pull and push.\nAs an example, close this book (via https) with:\ngit clone https://github.com/smart-stats/ds4bio_book.git\n\n\n3.2.5.5 Forking\n\nplt.figure(figsize=[3, 2])\nG = nx.DiGraph()\n\nG.add_node(\"GH1\",    pos = (.5, 1) )\nG.add_node(\"GH2\",    pos = (1.5, 1) )\nG.add_node(\"Local1\", pos = (.5, 0))\nG.add_node(\"Local2\", pos = (1.5, 0))\n\nG.add_edge(\"GH1\", \"Local1\")\nG.add_edge(\"Local1\", \"GH1\")\nG.add_edge(\"GH1\", \"GH2\")\nG.add_edge(\"GH2\", \"Local2\")\nG.add_edge(\"Local2\", \"GH2\")\n\n\n#G.add_edge(\"Local\", \"GH\")\nnx.draw(G, \n        nx.get_node_attributes(G, 'pos'), \n        with_labels=True, \n        font_weight='bold', \n        node_size = 2500,\n        node_color = \"lightblue\",\n        linewidths = 3)\nax= plt.gca()\nax.collections[0].set_edgecolor(\"#000000\")\nax.set_xlim([-.3, 2.3])\nax.set_ylim([-.3, 1.3])\nplt.show()\n\n\n\n\nA common workflow for github is to “fork” the repository. This differs from cloning in the sense that your fork is an entirely separate new copy. In the picture above, GH2 is a fork of GH1. It can pull upstream changes from GH1, but can’t write to it, since you don’t own that repo. You can push and pull from Local2 which is connected to GH2.\nForking on github is easy. Just click the “fork” button in the upper right corner.\n\n\n3.2.5.6 Pull requests\n\nplt.figure(figsize=[3, 2])\nG = nx.DiGraph()\n\nG.add_node(\"GH1\",    pos = (.5, 1) )\nG.add_node(\"GH2\",    pos = (1.5, 1) )\nG.add_node(\"Local1\", pos = (.5, 0))\nG.add_node(\"Local2\", pos = (1.5, 0))\n\nG.add_edge(\"GH1\", \"Local1\")\nG.add_edge(\"Local1\", \"GH1\")\nG.add_edge(\"GH2\", \"GH1\")\nG.add_edge(\"GH2\", \"Local2\")\nG.add_edge(\"Local2\", \"GH2\")\n\n\n#G.add_edge(\"Local\", \"GH\")\nnx.draw(G, \n        nx.get_node_attributes(G, 'pos'), \n        with_labels=True, \n        font_weight='bold', \n        node_size = 2500,\n        node_color = \"lightblue\",\n        linewidths = 3)\nax= plt.gca()\nax.collections[0].set_edgecolor(\"#000000\")\nax.set_xlim([-.3, 2.3])\nax.set_ylim([-.3, 1.3])\nplt.show()\n\n\n\n\nImagine a case where you forked the book repo and have some great additions (maybe a section on merge conflicts?). You think that these changes you think should be represented for other students. You can’t push changes to my repo; the very idea would be perposterous! How do you get your great changes represented in my repo?\nYou issue a pull request in the sense of requesting GH1 to pull your changes from GH2. Then, I would get notified of the pull request and would decide whether or not to merge them into my repo. This system has many benefits, and particularly, is great for operating on open source projects.\nI like to do pull requests directly on github through the web interface."
  },
  {
    "objectID": "git.html#branching",
    "href": "git.html#branching",
    "title": "3  Git, github",
    "section": "3.3 Branching",
    "text": "3.3 Branching\nYou often want multiple versions of a repository. For example, you might want a development version, a working version and a stable version. This is what branches are for in git. To create and switch to a branch called dev, use:\ngit checkout -b dev\nThis creates a new branch called dev and switches to that branch. If you already have dev, use checkout without -b. Suppose you want to merge your change from dev into your default branch (main)\ngit checkout main\ngit merge dev\nStart very basic with branches, then when you get the hang of it you can use it like a pro. Here’s more documentation."
  },
  {
    "objectID": "git.html#clients",
    "href": "git.html#clients",
    "title": "3  Git, github",
    "section": "3.4 Clients",
    "text": "3.4 Clients\nYou can avoid the command line stuff with a git client, of which there are many. It’s still good to know the command line options, for remote work if nothing else. I’ve tried several git clients and my favorite is the github client software and magit in emacs."
  },
  {
    "objectID": "git.html#setting-up-ssh",
    "href": "git.html#setting-up-ssh",
    "title": "3  Git, github",
    "section": "3.5 Setting up ssh",
    "text": "3.5 Setting up ssh\nIf you want to connect to a remote repository to github on a unix or linux system, it’s probably easiest to set up using so called secure shell. You can also set it up a variety of ways on Windows, but the rules are a little less universal. If you find this very hard to follow, just use a git and github gui (like github desktop, which is what I like to recommend).\nSecure shell works as follows. It creates a public key (think like a lock, everyone can see it) and a private key (like the key for the lock, only you have it). When you connect to github, it has your public key and you use the private key to “unlock” it. First, you need a .ssh directory from your personal directory. If you don’t have that, try ssh username@server to a server that you know and it will automatically create one.\nwe’ll go through this tutorial.\nThen, cd into your .ssh director and type the following and follow the instructions\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\nThis generates your public/prviate key pair. Then you need to take the public key file, and upload it to github under settings. This is described here.\nIn some cases I’ve also had to add this to a file config in my .ssh directory:\nHost github.com\n     ForwardX11 no\n     HostName github.com\n     User git\n     IdentityFile ~/.ssh/YOUR_PRIVATE_KEY_FILENAME\nOnce this is setup, then you should be able to push and pull from github from the command line without passwords."
  },
  {
    "objectID": "git.html#github-pages",
    "href": "git.html#github-pages",
    "title": "3  Git, github",
    "section": "3.6 Github pages",
    "text": "3.6 Github pages\nIt’s possible to host web pages on github. By hosting I mean serving the web page on the internet so that others can view it. There’s a couple of steps to doing this. First, you need an html file to serve. If you look in the chapter on html, there’s the code for a basic wepage. Save a web page as say “index.html”. You can double check that it works by simply double clicking on the file on your local computer. It should display as a web page in your browser, but it’s only viewable by you. Notice the addres begins with file:///.... To serve the file so that others can view it, we need it to exist on a server.\nGithub will serve the file for us, but first we need a repository. Create a public repository; I typically do this on github. Then you need to add an empty file called .nojekyll. Don’t forget the period at the beginning. This prevents github from thinking you’re using the jekyll framework, which is the default. Since we’re serving a basic webpage, we don’t need this framework. I create this file on github just by clicking Add File then Create new file.\nNext we need to tell github that this repo serves web pages. Click on settings then pages on the left. Then, under source choose the branch that you want to serve the pages from. Add your html file, commit and push to github. Give it a few minutes, then your file will be served from\nhttps:///GITHUBUSERNAME.github.io/REPONAME/FILENAME.html\nwhere here GITHUBUSERNAME is either your github username or the organization. REPONAME is the name of your repository and FILENAME.html is your html file, with the path if it’s in subdirectories. As an example, here’s a version I did https://bcaffo.github.io/testHtmlRepo/index.html."
  },
  {
    "objectID": "markdown.html",
    "href": "markdown.html",
    "title": "1  Markdown",
    "section": "",
    "text": "Before getting started, you need to learn some markdown. Markdown is a markup language (like HTML) that is absurdly easy. Every data scientist needs to know markdown. Fortunately, you’re five minutes away from knowing it. A markdown file is a text file that needs to be rendered to look nice. If you want an example, this page was written in markdown. To try it, go to google colab or a jupyter lab, create a markdown cell and start editing. Try this cheat sheet."
  },
  {
    "objectID": "unix.html",
    "href": "unix.html",
    "title": "2  Unix",
    "section": "",
    "text": "Some basic unix commands will go a long way in the course, especially for when you’re working on a remote server. On windows, you can actually install a unix environment on so-called services for unix. So, on windows, you have three options for working with the command line, i) install a linux subsystem and use that, ii) use the DOS command prompt or iii) use powershell. The commands here would only work for option i. However, when I work on a windows system, I tend to just use options ii or iii. Here, we’ll assume you’re working on a linux or unix system, or windows services for linux and you’ll have to read up elsewhere if you want to learn windows proper terminal commands.\nTo get a unix terminal, you have several options. Since we’re promoting jupyter and jupyterlab, just open up the terminal on there. (Again, assuming you’re working on a unix/linux system.)\nThe first thing you should try is figuring out where you’re at. Do this by typing\nprompt> pwd\nThis will show you where you are in the directory structure. If you want to see the contents of the directory try these\nprompt> ls\nprompt> ls -al\nprompt> ls -alh\nAdding the flags -a lists everything, including directories with a weird character in front. The l gives the long format, which gives more information and the h changes the filesize lists to a more human readable format. I also like the option --color. What you get with-alh` is as follows.\ntotal 36K\ndrwxrwxrwx+ 7 codespace root      4.0K Feb 14 14:24 .\ndrwxr-xrwx+ 5 codespace root      4.0K Oct 19 15:21 ..\ndrwxrwxrwx+ 6 codespace codespace 4.0K Feb 14 14:31 book\ndrwxrwxrwx+ 8 codespace root      4.0K Feb 15 21:34 .git\n-rw-rw-rw-  1 codespace codespace  171 Feb 14 14:24 .gitignore\n-rw-rw-rw-  1 codespace codespace    0 Feb 14 14:23 .nojekyll\n-rw-rw-rw-  1 codespace codespace  444 Feb 14 14:24 README.md\ndrwxrwxrwx+ 3 codespace codespace 4.0K Feb 14 14:24 slides\ndrwxrwxrwx+ 7 codespace codespace 4.0K Oct 19 15:21 .venv\ndrwxrwxrwx+ 2 codespace codespace 4.0K Oct 19 15:23 .vscode\nThe drwxrwxrwx+1 looking columns give permissions d=directory, r=read, w=write and x=execute, the groups are owner (you), group, everyone. So a file that is -rw------- can be read and written to by the owner, but cannot be executed by anyone and no one else can read or write to it (except the superuser, who gets to do everything).\nTo change a directory, try the following\nprompt> cd DIRECTORY\nwhere DIRECTORY is the name of the directory that you want to change into. You can hit TAB to autocomplete names. The command\nprompt> mv PATH_TO_INPUT_FILE PATH_TO_OUTPUT_FILE\nmoves the file. This is also how you rename a file, since you could just do mv FILENAME1 FILENAME2 and change the name.\nThe unix command for removing things is rm. So\nrm FILENAME\ndeletes the file. Note linux really deletes things, so do this with some care. You can’t remove directories this way, instead you could do rmdir DIRECTORY, but the directory has to be empty. If you want to use rm to remove a directory and its contents, you can do rm -rf DIRECTORY. However, use this with care.\nFinally, I find it very useful to use wget to grab files from the internet. So, for example,\nwget https://URL.../FILENAME\nwill grab the file from that link. Super useful.\nThat’s enough unix to get you started. You’ll find as you use the terminal more and more, you’ll like it better and better. Eventually, you’ll find GUIs kind of frustrating."
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "Python",
    "section": "",
    "text": "First, you’ll need a place to program python for data analysis. Python has a dizzying array of options for its use. A first choice is whether you’ll use python locally (installed on your computer) or in the cloud. The cloud options take care of a lot of installation problems, in exchange for a loss in control and typically much less computing resources unless you pay for stronger cloud computing. A second choice is whether you’ll look program in notebook environments or in a straight code editors. Notebooks mix code and documentation and are especially useful for programming for data analyses. More pure code editors and integrated development environments are preferable for writing software. Here’s a list of some of things I’ve tried and liked."
  },
  {
    "objectID": "python.html#notebooks",
    "href": "python.html#notebooks",
    "title": "Python",
    "section": "Notebooks",
    "text": "Notebooks\nNotebooks are going to be especially useful for us, as they’re a great way to do data analyses. With notebooks, you can merge richer documentation together with analysis code. You can take this to the extreme, and have solutions that create reproducible final documents. This book is an example, where the entire thing is written in jupyter-book. We’ll discuss this idea a little more when we discuss reproducible research. Alternatively, you can use your notebook as a working document that\nMost notebook solutions have text blocks and code blocks. The text is marked up in a markup language called “Markdown”, which we discussed eariler.\nIf you’re very new to notebooks in python, I would suggest starting with colab. The colab documentation is useful."
  },
  {
    "objectID": "python.html#weaved-text-formats",
    "href": "python.html#weaved-text-formats",
    "title": "Python",
    "section": "Weaved text formats",
    "text": "Weaved text formats\nI wrote this book in a format called quarto (see https://quarto.org/ ). This is a slightly different approach than jupyter notebooks and are perhaps better at producing final document-style output. Other approaches similar to quarto documents include R markdown, sweave and org mode."
  },
  {
    "objectID": "basic_python.html",
    "href": "basic_python.html",
    "title": "4  Python basics",
    "section": "",
    "text": "In this section, we will cover some basic python concepts. Python is an extremely quick language to learn, but like most programming languages, can take a long time to master. In this class, we’ll focus on a different style of programming than typical software development, programming with data. This will but less of a burden on us to be expert software developers in python, but some amount of base language knowledge is unavoidable. So, let’s get started learning some of the python programming basics. I’m going to assume you’ve programmed before in some language. If that isn’t the case, consider starting with a basic programming course of study before trying this book.\nA great resource for learning basic python is the python.org documentation https://docs.python.org/3/tutorial/index.html. My favorite programming resource of all time is the “Learn X in Y” tutorials. Here’s one for python https://learnxinyminutes.com/docs/python/.\nSome of the basic programming types in python are ints, floats, complex and Boolean. The command type tells us which. Here’s an example with 10 represented in 4 ways (int, float, string and complex) and the logical value True. Note, we’re using print to print out the result of the type command. If you’re typing directly into the python command line (called the repl, for read, evaluate, print, loop), you won’t need the print statements. But, if you’re using a notebook you probably will.\nIf you want an easy repl environment to program in, try https://replit.com/. For an easy notebook solution to try out, look at google colab, https://colab.research.google.com .\nThese types are our basic building blocks. There’s some other important basic types that build on these. We’ll cover these later, but to give you a teaser:\nTypes can be converted from one to another. For example, we might want to change our 10 into different types. Here’s some examples of converting the integer 10 into a float. First, we use the float function. Next we define a variable a that takes the integer value 10, then use a method associated with a to convert the type. If you’re unfamiliar with the second notation, don’t worry about that now, you’ll get very used to it as we work more in python.\nPython’s repl does all of the basic numerical calculations that you’d like. It does dynamic typing so that you can do things like add ints and floats. Here we show the basic operators, and note # is a comment in python.\nStrings are easy to work with in python. Type print(\"Hello World\") in the repl just to get that out of the way. Otherwise, + concatenates strings and brackets reference string elements. Here’s some examples. Remember counting starts at 0 and negative numbers count from the back.\nThe strings True and False are reserved for the respective Boolean values. The operators ==, >, <, >=, <= and != are the testing operators while and, or and is are Boolean operators. Here are some examples.\nDon’t define new variables called TRUE or FALSE or tRuE or FaLsE, or whatever, even though you can. Just get used to typing True and False the way python likes and don’t use similar named things for other reasons. Als, python as bitwise logical operators |, & and ~. On Boolean values, they work the same but differ in other circumstances. So, if you are unfamliar with bitwise operations, it’s probably better to stick to the word logical operators above."
  },
  {
    "objectID": "basic_python.html#data-structures",
    "href": "basic_python.html#data-structures",
    "title": "4  Python basics",
    "section": "4.1 Data structures",
    "text": "4.1 Data structures\nPython has some more advanced data structures that build on its primitive types.\n\nLists: ordered collections of objects\nSets: like lists but only have unique elements\nTuples: like lists, but not mutable, i.e. need to create a new one to modify\nDictionaries: named elements that you can reference by their name rather than position\n\nFirst, let’s look at some list operations.\n\ndat = [1, 4, 8, 10] # define a list\nprint(dat[0])       # reference an element\nprint(dat[2 : 4])   # reference elements\nprint(dat[2 : ]) \nprint(dat[:2])\ndat2 = [dat, dat]        # creating a list of lists\nprint(dat2)\nprint(dat2[1][2])        # referencing an item in a nested list\ndat3 = [dat2, \"string1\"] # mixed types\nprint(dat3)\ndat4 = dat + dat         # list concatenation\nprint(dat4)\n\n1\n[8, 10]\n[8, 10]\n[1, 4]\n[[1, 4, 8, 10], [1, 4, 8, 10]]\n8\n[[[1, 4, 8, 10], [1, 4, 8, 10]], 'string1']\n[1, 4, 8, 10, 1, 4, 8, 10]\n\n\nNow, let’s look at dictionaries.\n\ndict = {\"a\" : 1, \"b\" : 2} # Create a dictionary of two elements named a and b taking values 1 and 2 respectively\nprint(dict)\nprint(dict['a'])          # reference the element named a\n\n{'a': 1, 'b': 2}\n1\n\n\nSets and tuples are similar to lists, however with some important distinctions. Sets, contain only unique elements and tuples are immutable lists.\n\nset1 = {\"a\", \"b\", \"c\"}\nset2 = {\"a\", 1, True}\nset3 = {\"a\", \"b\", \"c\", \"c\"}\nprint(set1)\nprint(set2)\nprint(set3)\n\n{'b', 'a', 'c'}\n{1, 'a'}\n{'b', 'a', 'c'}\n\n\nHere’s an example to illustrate a tuple.\n\nlist1 = [\"a\", \"b\", \"c\"]\ntuple1 = (\"a\", \"b\", \"c\")\nlist1[0] = \"aa\" #Works just fine\nlist1\n#tuple1[0] = \"aa\" #doesn't work\n\n['aa', 'b', 'c']\n\n\n\n4.1.1 Mutable in immutable entities\nWhen working with objects in python, mutable and immutable elements act differently. Lists are mutable. So, below, the element y gets appended along with x.\n\nx = [10]\ny = x\nx.append(20)\n## Notice y has the appended element\nprint(y)\n## let's try again, as of now x = [10, 20] \nx[0] = x[0] + 11\n## Now x = [21, 20], but did y change?\nprint(y)\n\n[10, 20]\n[21, 20]\n\n\nThings like numbers and strings are immutable. Notice that changing y does not change x.\n\nx = 10\ny = x\nx = x + 10\nprint((x, y))\n\n(20, 10)"
  },
  {
    "objectID": "python_programming.html",
    "href": "python_programming.html",
    "title": "5  Python programming",
    "section": "",
    "text": "# do this if you'd like to prompt for an input\n# x = input(\"are you mean (y/n)? > \")\n# Let's just assume the user input 'n'\nx = 'n'\nif x == 'y': \n print(\"Slytherine!\")\nelse:\n print(\"Gryffindor\")\n\nGryffindor\n\n\nJust to further describe white space useage in python, consider testing whether statementA is True. Below, statementB is executed as part of the if statement whereas statementC is outside of it because it’s not indented. This is often considered an eye rolling aspect of the language, but I think it’s nice in the sense that it bakes good code identation practices into the language.\n## Some more about white space\nif statementA:\n  statementB   # Executed if statementA is True\nstatementC     # Executed regardless since it's not indented\nThe generic structure of if statements in python are\nif statement1 :\n ...\nelif statement2 :\n ...\nelse \n ...\nHere’s an example (note this is just equal to the statement (a < 0) - (a > 0)\n\na = 5\n\nif a < 0 :\n  a = -1\nelif a > 0 :\n  a = 1\nelse :\n  a = 0\n\nprint(a)\n\n1\n\n\nfor and while loops can be used for iteration. Here’s some examples\n\nfor i in range(4) :\n print(i)\n\n0\n1\n2\n3\n\n\n\nx = 4\nwhile x > 0 :\n x = x - 1\n print(x)\n\n3\n2\n1\n0\n\n\nNote for loops can iterate over list-like structures.\n\nfor w in 'word':\n print(w)\n\nw\no\nr\nd\n\n\nThe range function is useful for creating a structure to loop over. It creates a data type that can be iterated over, but isn’t itself a list. So if you want a list out of it, you have to convert it.\n\na = range(3)\nprint(a)\nprint(list(a))\n\nrange(0, 3)\n[0, 1, 2]"
  },
  {
    "objectID": "python_practice.html",
    "href": "python_practice.html",
    "title": "9  Python in practice",
    "section": "",
    "text": "The kind of programming we’ve seen so far in python isn’t how typical data programmming in python goes. Instead, we tend to rely a lot of modules that add methods to our complex data science objects. Most python objects are class objects that come with a variety of convenient methods associated with them. If you’re working in a good coding environment, then it should have some method autocompletion for your objects, which helps prevent typos and can speed up work. Let’s look at methods associated with a list object. Note that some methods change the object itself while others return things without changing the object.\nA useful working example is working with imaginary numbers.\nLet’s create our own version of a complex number, adapted from here. Complex numbers have two parts, the real part and the “imaginary” part. (Note I put imaginary in quotes, since, IMHO, complex numbers and the associated operations are simply an algebraic system, no more or less imaginary than most other algebraic systems. They simply lack the direct real world counting analogy of squaring a whole number. But there are countless algenbraic systems created for a variety of uses, most lacking the direct analogy to counting whole numbers.) There’s two popular representations of complex numbers, Cartesian and polar. We’ll use Cartesian and not discuss polar.\nA complex number is represented as \\(a + bi\\) where \\(a\\) and \\(b\\) are numbers and \\(i\\) is the symbol for the complex root of -1, i.e. \\(i^2 = -1\\). If $a + b i $ is a complex number, \\(a - bi\\) is its so-called conjugate. Conjugates are useful since \\[ (a+bi)(a-bi) = a^2 + abi -\nabi - b^2 i^2 = a^2 + b^2 \\]\nLet’s create an object called mycomplex and give it a conjugation method and some other stuff.\nLet’s now create a version that doesn’t modify the object when we conjugate."
  },
  {
    "objectID": "python_practice.html#utilizing-python-libraries",
    "href": "python_practice.html#utilizing-python-libraries",
    "title": "6  Python in practice",
    "section": "6.1 Utilizing python libraries",
    "text": "6.1 Utilizing python libraries\nWe typically load libraries that create useful data structures for us and have useful functions. We’ll use pandas a lot next. So, let’s go through another very useful data science library, numpy.\nFirst, let’s load up numpy. Here’s three separate ways\n\nimport numpy\nimport numpy as np\nfrom numpy import *\n\nOption 1. imports numpy, but then you have to type numpy.FUNCTION to access FUNCTION. The second option (my preferred) shortens this to np.FUNCTION. The third loads the numpy functions into the global namespace. This is probably ok for really core packages like numpy. But, otherwise it’s an issue since you typically load many libraries and some may have the same function names.\nLet’s load up numpy and look at some of its capabilities.\n\nimport numpy as np\n\n## Numpy has constants\nprint(np.pi)\n## Numpy has a not a number placeholder\nprint(np.nan)\n## Numpy has 1 and 2d arrays\nvector = np.array([1, 2, 3, 8])\nprint(vector)\nprint(type(vector))\n## Numpy has N-Dimensional arrays\narray = np.array([ [1, 2, 3], [4, 5, 6]])\nprint(array)\n\n3.141592653589793\nnan\n[1 2 3 8]\n<class 'numpy.ndarray'>\n[[1 2 3]\n [4 5 6]]\n\n\n\n## Arrays have operator definitions\nprint(array + array)\nprint(array * array)\n\n[[ 2  4  6]\n [ 8 10 12]]\n[[ 1  4  9]\n [16 25 36]]\n\n\nLet’s look at a common use of a library object. Our vector, vector, has some data in it. What if we wanted the mean and standard deviation?\n\nprint(vector.mean())\nprint(vector.std())\n\n3.5\n2.692582403567252\n\n\nNumpy has a separate data structure for matrices (2D arrays). Let’s creat a matrix like our previous 2D array.\n\nmymat = np.matrix([ [1, 2], [4, 5] ] )\nprint( (type(array), type(mymat) ) )\nmymat\n\n(<class 'numpy.ndarray'>, <class 'numpy.matrix'>)\n\n\nmatrix([[1, 2],\n        [4, 5]])\n\n\nNumpy’s linear algebra functions are spread across sublibraries of numpy. linalg is one. Let’s suppose we want the matrix determinant of mymat. We have several choices\n\nnp.linalg.det(mymat)\nimport numpy.linalg as la then la.det(mymat)\nfrom numpy.linalg import det then det(mymat)\n\nThe first is a little long, but just calls the sublibrary then the function of the sublibrary. The second and third are shorter, where the second gives a named reference to the sublibrary methods while the third loads the function into the namespace.\n\nfrom numpy.linalg import det \ndet(mymat)\n\n-2.9999999999999996\n\n\n\nx = np.array([1.1, 2.1, 3.1, 3.2, 8.6])\nx[1:3]\n\narray([2.1, 3.1])"
  },
  {
    "objectID": "python_practice.html#building-python-modules",
    "href": "python_practice.html#building-python-modules",
    "title": "9  Python in practice",
    "section": "9.2 Building python modules",
    "text": "9.2 Building python modules\nWith very small effort, a set of python functions can be turned into a python library. Let’s start with creating a python module, then we’ll talk about distributing with pip"
  },
  {
    "objectID": "python_practice.html#example-utilizing-python-libraries-numpy",
    "href": "python_practice.html#example-utilizing-python-libraries-numpy",
    "title": "9  Python in practice",
    "section": "9.1 Example utilizing python libraries, numpy",
    "text": "9.1 Example utilizing python libraries, numpy\nWe typically load libraries that create useful data structures for us and have useful functions. We’ll use pandas a lot next. So, let’s go through another very useful data science library, numpy.\nFirst, let’s load up numpy. Here’s three separate ways\n\nimport numpy\nimport numpy as np\nfrom numpy import *\n\nOption 1. imports numpy, but then you have to type numpy.FUNCTION to access FUNCTION. The second option (my preferred) shortens this to np.FUNCTION. The third loads the numpy functions into the global namespace. This is probably ok for really core packages like numpy. But, otherwise it’s an issue since you typically load many libraries and some may have the same function names.\nLet’s load up numpy and look at some of its capabilities.\n\nimport numpy as np\n\n## Numpy has constants\nprint(np.pi)\n## Numpy has a not a number placeholder\nprint(np.nan)\n## Numpy has 1 and 2d arrays\nvector = np.array([1, 2, 3, 8])\nprint(vector)\nprint(type(vector))\n## Numpy has N-Dimensional arrays\narray = np.array([ [1, 2, 3], [4, 5, 6]])\nprint(array)\n\n3.141592653589793\nnan\n[1 2 3 8]\n<class 'numpy.ndarray'>\n[[1 2 3]\n [4 5 6]]\n\n\n\n## Arrays have operator definitions\nprint(array + array)\nprint(array * array)\n\n[[ 2  4  6]\n [ 8 10 12]]\n[[ 1  4  9]\n [16 25 36]]\n\n\nLet’s look at a common use of a library object. Our vector, vector, has some data in it. What if we wanted the mean and standard deviation?\n\nprint(vector.mean())\nprint(vector.std())\n\n3.5\n2.692582403567252\n\n\nNumpy has a separate data structure for matrices (2D arrays). Let’s creat a matrix like our previous 2D array.\n\nmymat = np.matrix([ [1, 2], [4, 5] ] )\nprint( (type(array), type(mymat) ) )\nmymat\n\n(<class 'numpy.ndarray'>, <class 'numpy.matrix'>)\n\n\nmatrix([[1, 2],\n        [4, 5]])\n\n\nNumpy’s linear algebra functions are spread across sublibraries of numpy. linalg is one. Let’s suppose we want the matrix determinant of mymat. We have several choices\n\nnp.linalg.det(mymat)\nimport numpy.linalg as la then la.det(mymat)\nfrom numpy.linalg import det then det(mymat)\n\nThe first is a little long, but just calls the sublibrary then the function of the sublibrary. The second and third are shorter, where the second gives a named reference to the sublibrary methods while the third loads the function into the namespace.\n\nfrom numpy.linalg import det \ndet(mymat)\n\n-2.9999999999999996\n\n\n\nx = np.array([1.1, 2.1, 3.1, 3.2, 8.6])\nx[1:3]\n\narray([2.1, 3.1])"
  },
  {
    "objectID": "intro_markdown.html",
    "href": "intro_markdown.html",
    "title": "1  Markdown",
    "section": "",
    "text": "As mentioned, markdown is markup language. So, you write in plain text and then it needs to be rendered into a pretty document or page. For example, all of these notes were written in markdown, but then converted to HTML. There are different flavors of markdown. So, syntax can change a bit. I’m using the one that works in quarto.\nHere’s how you section in markdown.\n# Top level heading\n## Second level heading\n### Third level heading\nand so on. It should look something like this, though the style can change depending on how it is being rendered.\n Top level heading \n Second level heading \n Third level heading \nYou can can bold text and italicize text like this:\n**bold** text and *italicize*\nIf you want an ordered list, like this:\n\nItem one.\nItem two.\nItem three.\n\nthen you can just do:\n1. Item one.\n2. Item two.\n3. Item three.\nNote, if you move around the order of the list in the markdown code, the markup engine will just reorder them starting at whatever the first number is. An unordered lists can be done with asterisks or plus signs or dashes. (I tend to use asterisks.) Also, putting brackets with an x makes for a check mark.\n* [ ] Pick up broccoli\n* [ ] Pick up oat milk\n* [x] Pick up golden berries\n* [x] Pick up tea\n\nPick up broccoli\nPick up oat milk\nPick up golden berries\nPick up tea\n\nInline code, like lambda x: x ** 2 can be written with backticks like this:\n`lambda x: x ** 2`\nBlock code is written in between three backticks.\n```\nlike this\n```\nLinks can be done like this:\n[Markdown cheat sheet](https://www.markdownguide.org/cheat-sheet/).\nwhich renders like this: Markdown cheat sheet. (Also, that’s a real link to a nice MD cheat sheet.) Images can be done like this\n![Image alt text](assets/images/book_graphic.png)\nIf your converter can use mathjax, or some other LaTeX math rendering library, you can insert LaTeX equations. For example,\n\\[\n\\int_{-\\infty}^\\infty (2\\pi\\sigma^2)^{-1/2} \\exp\\left\\{-\\frac{1}{2\\sigma^2} (x - \\mu)^2\\right\\} dx = 1\n\\]\ncan be written as\n$$\n\\int_{-\\infty}^\\infty (2\\pi\\sigma^2)^{-1/2} \\exp\\left\\{-\\frac{1}{2\\sigma^2} (x - \\mu)^2\\right\\} dx = 1\n$$\nI noticed the github markdown renderer doesn’t load mathjax, but most of the data science things do, like jupyter-lab, colab and quarto.\nThat’s plenty of markdown to start. Try it out. You’ll find that you pick it up really fast."
  },
  {
    "objectID": "intro_unix.html",
    "href": "intro_unix.html",
    "title": "2  Unix",
    "section": "",
    "text": "To get a unix terminal, you have several options. Since we’re promoting jupyter and jupyterlab, just open up the terminal on there. (Again, assuming you’re working on a unix/linux system.)\nThe first thing you should try is figuring out where you’re at. Do this by typing\nprompt> pwd\nThis will show you where you are in the directory structure. If you want to see the contents of the directory try these\nprompt> ls\nprompt> ls -al\nprompt> ls -alh\nAdding the flags -a lists everything, including directories with a weird character in front. The l gives the long format, which gives more information and the h changes the filesize lists to a more human readable format. I also like the option --color. What you get with-alh` is as follows.\ntotal 36K\ndrwxrwxrwx+ 7 codespace root      4.0K Feb 14 14:24 .\ndrwxr-xrwx+ 5 codespace root      4.0K Oct 19 15:21 ..\ndrwxrwxrwx+ 6 codespace codespace 4.0K Feb 14 14:31 book\ndrwxrwxrwx+ 8 codespace root      4.0K Feb 15 21:34 .git\n-rw-rw-rw-  1 codespace codespace  171 Feb 14 14:24 .gitignore\n-rw-rw-rw-  1 codespace codespace    0 Feb 14 14:23 .nojekyll\n-rw-rw-rw-  1 codespace codespace  444 Feb 14 14:24 README.md\ndrwxrwxrwx+ 3 codespace codespace 4.0K Feb 14 14:24 slides\ndrwxrwxrwx+ 7 codespace codespace 4.0K Oct 19 15:21 .venv\ndrwxrwxrwx+ 2 codespace codespace 4.0K Oct 19 15:23 .vscode\nThe drwxrwxrwx+1 looking columns give permissions d=directory, r=read, w=write and x=execute, the groups are owner (you), group, everyone. So a file that is -rw------- can be read and written to by the owner, but cannot be executed by anyone and no one else can read or write to it (except the superuser, who gets to do everything).\nTo change a directory, try the following\nprompt> cd DIRECTORY\nwhere DIRECTORY is the name of the directory that you want to change into. You can hit TAB to autocomplete names. The command\nprompt> mv PATH_TO_INPUT_FILE PATH_TO_OUTPUT_FILE\nmoves the file. This is also how you rename a file, since you could just do mv FILENAME1 FILENAME2 and change the name.\nThe unix command for removing things is rm. So\nrm FILENAME\ndeletes the file. Note linux really deletes things, so do this with some care. You can’t remove directories this way, instead you could do rmdir DIRECTORY, but the directory has to be empty. If you want to use rm to remove a directory and its contents, you can do rm -rf DIRECTORY. However, use this with care.\nFinally, I find it very useful to use wget to grab files from the internet. So, for example,\nwget https://URL.../FILENAME\nwill grab the file from that link. Super useful.\nThat’s enough unix to get you started. You’ll find as you use the terminal more and more, you’ll like it better and better. Eventually, you’ll find GUIs kind of frustrating."
  },
  {
    "objectID": "intro_git.html",
    "href": "intro_git.html",
    "title": "3  Git, github",
    "section": "",
    "text": "In the live versions of these classes, we use the version control system git and git hosting service github. If you work in data science should have a working knowledge of both git and at least one cloud hosting service (like github). For git, you work in a repository, which is basically a project directory on your computer with some extra files that help git work. Git is then used for version control so that you keep track of states of your project. Github, is a hosting service for git repositories. Typically, you have your repository on your computer and you coordinate it with the one on the server. Github is just one of several hosting services, bitbucket is another, or you could even relatively easily start your own. However, github has front end web services that allows you to interact with your remote repository easily. This is very convenient."
  },
  {
    "objectID": "intro_git.html#the-least-you-need-to-know",
    "href": "intro_git.html#the-least-you-need-to-know",
    "title": "3  Git, github",
    "section": "3.1 The least you need to know",
    "text": "3.1 The least you need to know\nI’m not going to fully recreate git / github tutorials here; here’s one I recommend by Sean Kross. Instead, I’m going to go through a typical git / github workflow.\n\nInitialization I almost always initialize my git repository on github with a readme.md file.\nClone I typically clone the repository to my local computer using the command line or a local git gui that works with github, like this one. Note that you only have to clone the repo once. After it’s cloned you have a full local copy of the repository.\nadd new files to track and stage them after I’ve worked with them.\ncommit the changes to the local repository with a meaningful commit message.\npush the changes to the repository.\nIf there’s changes on the remote repository not represented in my local repository, I pull those changes to my local repo.\n\nFor larger projects, you’re likely working with multiple people, some of whom you’ve given access to your remote repository and some of whom you have not. The ones who can’t directly push to the remote repo might have their own version of the code and their own version on github. If they think you should incorporate those changes, they might issue a pull request to you. You can then opt to pull their changes into your repo on github, then pull them from github to your local repo. One of the reasons why services like github and bitbucket are so popular is that they make this coordination fairly easy, along with having nice project messaging and management tools.\nIn our class, we use github classroom. For github classroom, you’ll get a link to a repo to put your submission files into. When you push to the remote repository, you’ll have submitted. But, up to the due date you can make changes an push again."
  },
  {
    "objectID": "intro_git.html#a-little-more-detail",
    "href": "intro_git.html#a-little-more-detail",
    "title": "3  Git, github",
    "section": "3.2 A little more detail",
    "text": "3.2 A little more detail\n\n3.2.1 Getting started\nGit is a version control system invented by Linus Torvalds, the invetor of the linux operating system. A github repository is a directory that the software git is using to keep track of versions. Install git, it’s already installed on most cloud services and many operaiting systems. You can initialize a repository in a directory with the command git init in that directory. Honestly, I almost never start a git repo this way, because I’ve connected my repo to a cloud server. But, more on that later. After you’ve initialized it, your git repo is a locally version controlled system.\n\n\n3.2.2 Adding files\nOK, now you want to add some of your files to your repo. You have a directory You can add files with the command\ngit add FILENAME\nThe file FILENAME is now staged to be part of the repository, but isn’t added yet! Files that you haven’t added won’t be tracked. This is good, since there’s many files, llike log files, that we don’t want to track. You only need to add the files once, then they are being tracked. Try it out with a readme markdown file in your repository.\n\n\n3.2.3 Checking status\nOne of the most useful git commands is\ngit status\nThis gives the status of which files are being tracked, which are not and which have changes.\n\n\n3.2.4 Commiting changes\nYou have to actually commit your changes to the repository to have them represented. Committing changes is a local operation, subsequently, we’ll show you how to coordinate with a remote repository. Here’s the command I use to commit most often\ngit commit -a -m \"COMMIT MESSAGE\"\nHere the flag -a means commit all files staged for commits. You can also commit individual files or a collection of specific files. The -m \"COMMIT MESSAGE\" is the message associated with your commit. Make your commit messages more informative. If you don’t add -m github will bring up an editor for you to enter your commit message.\n\n\n3.2.5 Remote repositories\nGit becomes much more useful when it’s coordinated with a remote repository. Github is a server for hosting remote git repositories, there are several others, bitbucket is an example. Github also includes a great deal of functionality for working with the remote repository through the website. There’s many git and github guis that you could use if you want. I always liked to github one and magit in emacs.\n\n3.2.5.1 Connecting your remote and local repos\nYou need to connect your local repo to the remote repo. First create a version of the repo on github (actually then it gives you instructions). Then you can either\n\nClone the repo from the server git clone REMOTE SERVER\nConnect the server to your local repository\n\nFor 2. you use a command like:\ngit remote add origin git@github.com:USERNAME/REPONAME.git\nPersonally, I find it easier just to create the repo on github and clone it. You can clone over ssh or https, if you don’t know what ssh is use https and then switch over to ssh as you learn how to use it.\n\n\n3.2.5.2 Pushing changes\n\n\n\n\n\nYou push from the repo you’re working on to another (remote repo). So, for me, this typically means pushing from my local computer repo to github or to a scientific computing cluster.\ngit push origin main\nThe above command could be read as “push my new committed changes from my origin to the main branch on my remote repo”. Older repos had the default branch master instead of main. But, github switched in 2020 or so. So, some older documentation may not have this switch.\n\n\n3.2.5.3 Pulling changes\n\n\n\n\n\nThere might be changes that you want represented locally that were changed on the remote repository; for example, you might edit a readme file through github’s web interface. The command for this is really easy, it’s just pull\ngit pull\nThe pain occurs if you have made local changes that are in conflict with the remote changes. I’d write about merge conflicts, but I can’t, I just can’t (shudder). [Here’s some docs on managing merge conflicts] (https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/addressing-merge-conflicts/resolving-a-merge-conflict-using-the-command-line).\n\n\n3.2.5.4 Cloning\n\n\n\n\n\nTo clone a remote repository is to simply create a local copy where you can pull changes from the remote. If a repository is public and you own it, then you can clone it and push changes back to the remote. If you don’t, you can clone and pull changes, but then can’t push back to the remote.\nTry with the book repo, clone it so you have access to the files. In the figure below the owner can pull and push to the repo. In the picture below, consider that you clone GH1 off of github to you. You can pull updates from GH1 but can’t push changes to GH1. The owner of GH1 of course can pull and push.\nAs an example, close this book (via https) with:\ngit clone https://github.com/smart-stats/ds4bio_book.git\n\n\n3.2.5.5 Forking\n\n\n\n\n\nA common workflow for github is to “fork” the repository. This differs from cloning in the sense that your fork is an entirely separate new copy. In the picture above, GH2 is a fork of GH1. It can pull upstream changes from GH1, but can’t write to it, since you don’t own that repo. You can push and pull from Local2 which is connected to GH2.\nForking on github is easy. Just click the “fork” button in the upper right corner.\n\n\n3.2.5.6 Pull requests\n\n\n\n\n\nImagine a case where you forked the book repo and have some great additions (maybe a section on merge conflicts?). You think that these changes you think should be represented for other students. You can’t push changes to my repo; the very idea would be perposterous! How do you get your great changes represented in my repo?\nYou issue a pull request in the sense of requesting GH1 to pull your changes from GH2. Then, I would get notified of the pull request and would decide whether or not to merge them into my repo. This system has many benefits, and particularly, is great for operating on open source projects.\nI like to do pull requests directly on github through the web interface."
  },
  {
    "objectID": "intro_git.html#branching",
    "href": "intro_git.html#branching",
    "title": "3  Git, github",
    "section": "3.3 Branching",
    "text": "3.3 Branching\nYou often want multiple versions of a repository. For example, you might want a development version, a working version and a stable version. This is what branches are for in git. To create and switch to a branch called dev, use:\ngit checkout -b dev\nThis creates a new branch called dev and switches to that branch. If you already have dev, use checkout without -b. Suppose you want to merge your change from dev into your default branch (main)\ngit checkout main\ngit merge dev\nStart very basic with branches, then when you get the hang of it you can use it like a pro. Here’s more documentation."
  },
  {
    "objectID": "intro_git.html#clients",
    "href": "intro_git.html#clients",
    "title": "3  Git, github",
    "section": "3.4 Clients",
    "text": "3.4 Clients\nYou can avoid the command line stuff with a git client, of which there are many. It’s still good to know the command line options, for remote work if nothing else. I’ve tried several git clients and my favorite is the github client software and magit in emacs."
  },
  {
    "objectID": "intro_git.html#setting-up-ssh",
    "href": "intro_git.html#setting-up-ssh",
    "title": "3  Git, github",
    "section": "3.5 Setting up ssh",
    "text": "3.5 Setting up ssh\nIf you want to connect to a remote repository to github on a unix or linux system, it’s probably easiest to set up using so called secure shell. You can also set it up a variety of ways on Windows, but the rules are a little less universal. If you find this very hard to follow, just use a git and github gui (like github desktop, which is what I like to recommend).\nSecure shell works as follows. It creates a public key (think like a lock, everyone can see it) and a private key (like the key for the lock, only you have it). When you connect to github, it has your public key and you use the private key to “unlock” it. First, you need a .ssh directory from your personal directory. If you don’t have that, try ssh username@server to a server that you know and it will automatically create one.\nwe’ll go through this tutorial.\nThen, cd into your .ssh director and type the following and follow the instructions\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\nThis generates your public/prviate key pair. Then you need to take the public key file, and upload it to github under settings. This is described here.\nIn some cases I’ve also had to add this to a file config in my .ssh directory:\nHost github.com\n     ForwardX11 no\n     HostName github.com\n     User git\n     IdentityFile ~/.ssh/YOUR_PRIVATE_KEY_FILENAME\nOnce this is setup, then you should be able to push and pull from github from the command line without passwords."
  },
  {
    "objectID": "intro_git.html#github-pages",
    "href": "intro_git.html#github-pages",
    "title": "3  Git, github",
    "section": "3.6 Github pages",
    "text": "3.6 Github pages\nIt’s possible to host web pages on github. By hosting I mean serving the web page on the internet so that others can view it. There’s a couple of steps to doing this. First, you need an html file to serve. If you look in the chapter on html, there’s the code for a basic wepage. Save a web page as say “index.html”. You can double check that it works by simply double clicking on the file on your local computer. It should display as a web page in your browser, but it’s only viewable by you. Notice the addres begins with file:///.... To serve the file so that others can view it, we need it to exist on a server.\nGithub will serve the file for us, but first we need a repository. Create a public repository; I typically do this on github. Then you need to add an empty file called .nojekyll. Don’t forget the period at the beginning. This prevents github from thinking you’re using the jekyll framework, which is the default. Since we’re serving a basic webpage, we don’t need this framework. I create this file on github just by clicking Add File then Create new file.\nNext we need to tell github that this repo serves web pages. Click on settings then pages on the left. Then, under source choose the branch that you want to serve the pages from. Add your html file, commit and push to github. Give it a few minutes, then your file will be served from\nhttps:///GITHUBUSERNAME.github.io/REPONAME/FILENAME.html\nwhere here GITHUBUSERNAME is either your github username or the organization. REPONAME is the name of your repository and FILENAME.html is your html file, with the path if it’s in subdirectories. As an example, here’s a version I did https://bcaffo.github.io/testHtmlRepo/index.html."
  },
  {
    "objectID": "python_basic.html",
    "href": "python_basic.html",
    "title": "4  Python basics",
    "section": "",
    "text": "In this section, we will cover some basic python concepts. Python is an extremely quick language to learn, but like most programming languages, can take a long time to master. In this class, we’ll focus on a different style of programming than typical software development, programming with data. This will but less of a burden on us to be expert software developers in python, but some amount of base language knowledge is unavoidable. So, let’s get started learning some of the python programming basics. I’m going to assume you’ve programmed before in some language. If that isn’t the case, consider starting with a basic programming course of study before trying this book.\nA great resource for learning basic python is the python.org documentation https://docs.python.org/3/tutorial/index.html. My favorite programming resource of all time is the “Learn X in Y” tutorials. Here’s one for python https://learnxinyminutes.com/docs/python/.\nSome of the basic programming types in python are ints, floats, complex and Boolean. The command type tells us which. Here’s an example with 10 represented in 4 ways (int, float, string and complex) and the logical value True. Note, we’re using print to print out the result of the type command. If you’re typing directly into the python command line (called the repl, for read, evaluate, print, loop), you won’t need the print statements. But, if you’re using a notebook you probably will.\nIf you want an easy repl environment to program in, try https://replit.com/. For an easy notebook solution to try out, look at google colab, https://colab.research.google.com .\nThese types are our basic building blocks. There’s some other important basic types that build on these. We’ll cover these later, but to give you a teaser:\nTypes can be converted from one to another. For example, we might want to change our 10 into different types. Here’s some examples of converting the integer 10 into a float. First, we use the float function. Next we define a variable a that takes the integer value 10, then use a method associated with a to convert the type. If you’re unfamiliar with the second notation, don’t worry about that now, you’ll get very used to it as we work more in python.\nPython’s repl does all of the basic numerical calculations that you’d like. It does dynamic typing so that you can do things like add ints and floats. Here we show the basic operators, and note # is a comment in python.\nStrings are easy to work with in python. Type print(\"Hello World\") in the repl just to get that out of the way. Otherwise, + concatenates strings and brackets reference string elements. Here’s some examples. Remember counting starts at 0 and negative numbers count from the back.\nThe strings True and False are reserved for the respective Boolean values. The operators ==, >, <, >=, <= and != are the testing operators while and, or and is are Boolean operators. Here are some examples.\nDon’t define new variables called TRUE or FALSE or tRuE or FaLsE, or whatever, even though you can. Just get used to typing True and False the way python likes and don’t use similar named things for other reasons. Als, python as bitwise logical operators |, & and ~. On Boolean values, they work the same but differ in other circumstances. So, if you are unfamliar with bitwise operations, it’s probably better to stick to the word logical operators above."
  },
  {
    "objectID": "python_basic.html#data-structures",
    "href": "python_basic.html#data-structures",
    "title": "4  Python basics",
    "section": "4.1 Data structures",
    "text": "4.1 Data structures\nPython has some more advanced data structures that build on its primitive types.\n\nLists: ordered collections of objects\nSets: like lists but only have unique elements\nTuples: like lists, but not mutable, i.e. need to create a new one to modify\nDictionaries: named elements that you can reference by their name rather than position\n\nFirst, let’s look at some list operations.\n\ndat = [1, 4, 8, 10] # define a list\nprint(dat[0])       # reference an element\nprint(dat[2 : 4])   # reference elements\nprint(dat[2 : ]) \nprint(dat[:2])\ndat2 = [dat, dat]        # creating a list of lists\nprint(dat2)\nprint(dat2[1][2])        # referencing an item in a nested list\ndat3 = [dat2, \"string1\"] # mixed types\nprint(dat3)\ndat4 = dat + dat         # list concatenation\nprint(dat4)\n\n1\n[8, 10]\n[8, 10]\n[1, 4]\n[[1, 4, 8, 10], [1, 4, 8, 10]]\n8\n[[[1, 4, 8, 10], [1, 4, 8, 10]], 'string1']\n[1, 4, 8, 10, 1, 4, 8, 10]\n\n\nNow, let’s look at dictionaries.\n\ndict = {\"a\" : 1, \"b\" : 2} # Create a dictionary of two elements named a and b taking values 1 and 2 respectively\nprint(dict)\nprint(dict['a'])          # reference the element named a\n\n{'a': 1, 'b': 2}\n1\n\n\nSets and tuples are similar to lists, however with some important distinctions. Sets, contain only unique elements and tuples are immutable lists.\n\nset1 = {\"a\", \"b\", \"c\"}\nset2 = {\"a\", 1, True}\nset3 = {\"a\", \"b\", \"c\", \"c\"}\nprint(set1)\nprint(set2)\nprint(set3)\n\n{'c', 'a', 'b'}\n{1, 'a'}\n{'c', 'a', 'b'}\n\n\nHere’s an example to illustrate a tuple.\n\nlist1 = [\"a\", \"b\", \"c\"]\ntuple1 = (\"a\", \"b\", \"c\")\nlist1[0] = \"aa\" #Works just fine\nlist1\n#tuple1[0] = \"aa\" #doesn't work\n\n['aa', 'b', 'c']\n\n\n\n4.1.1 Mutable in immutable entities\nWhen working with objects in python, mutable and immutable elements act differently. Lists are mutable. So, below, the element y gets appended along with x.\n\nx = [10]\ny = x\nx.append(20)\n## Notice y has the appended element\nprint(y)\n## let's try again, as of now x = [10, 20] \nx[0] = x[0] + 11\n## Now x = [21, 20], but did y change?\nprint(y)\n\n[10, 20]\n[21, 20]\n\n\nThings like numbers and strings are immutable. Notice that changing y does not change x.\n\nx = 10\ny = x\nx = x + 10\nprint((x, y))\n\n(20, 10)"
  },
  {
    "objectID": "python_functions.html",
    "href": "python_functions.html",
    "title": "6  Functions",
    "section": "",
    "text": "def pow(x, n = 2):\n  return x ** n\n\nprint(pow(5, 3))\n\n125\n\n\nNote our function has a mandatory arugment, x, and an optional arugment, n, that takes the default value 2. Consider this example to think about how python evaluates function arguments. These are all the same.\n\nprint(pow(3, 2))\nprint(pow(x = 3, n = 2))\nprint(pow(n = 2, x = 3))\n#pow(n = 2, 3) this returns an error, the second position is n, but it's a named argument too\n\n9\n9\n9\n\n\nYou can look here, https://docs.python.org/3/tutorial/controlflow.html, to study the rules. It doesn’t make a lot of sense to get to cute with your function calling arguments. I try to obey both the order and the naming. I argue that this is the way to go since usually functions are written with some sensible ordering of arguments and naming removes all doubt. Python has a special variable for variable length arguments. Here’s an example.\n\ndef concat(*args, sep=\"/\"):\n return sep.join(args)  \n\nprint(concat(\"a\", \"b\", \"c\"))\nprint(concat(\"a\", \"b\", \"c\", sep = \":\"))\n\na/b/c\na:b:c\n\n\nLambda can be used to create short, unnamed functions. This has a lot of uses that we’ll see later.\n\nf = lambda x: x ** 2\nprint(f(5))\n\n25\n\n\nHere’s an example useage where we use lambda to make specific “raise to the power” functions.\n\ndef makepow(n):\n return lambda x: x ** n\n\nsquare = makepow(2)\nprint(square(3))\ncube = makepow(3)\nprint(cube(2))\n\n9\n8"
  },
  {
    "objectID": "intro_markdown.html#sectioning",
    "href": "intro_markdown.html#sectioning",
    "title": "1  Markdown",
    "section": "1.1 Sectioning",
    "text": "1.1 Sectioning\nHere’s how you section in markdown.\n# Top level heading\n## Second level heading\n### Third level heading\nand so on. You can can bold text and italicize text like this:\n**bold** text and *italicize*\nIf you want an ordered list, like this:\n\nItem one.\nItem two.\nItem three.\n\nthen you can just do:\n1. Item one.\n2. Item two.\n3. Item three.\nNote, if you move around the order of the list in the markdown code, the markup engine will just reorder them starting at whatever the first number is. An unordered lists can be done with asterisks or plus signs. (I use asterisks)\n\nPick up broccoli\nPick up oat milk\nPick up golden berries\nPick up tea\n\n* Pick up broccoli\n* Pick up oat milk\n* Pick up golden berries\n+ Pick up tea\nTry this cheat sheet."
  },
  {
    "objectID": "python_virtual_environments.html",
    "href": "python_virtual_environments.html",
    "title": "7  Virtual Environments",
    "section": "",
    "text": "One of the nicer aspects of python is its virtual environment capabilities. Before we discuss virtual environments, we should discuss the Python installer program (pip). Pip is a little program to help you install python packages. In fact, it makes installing python packages trivial. The packages need to be hosted on the Python Package Index (PyPI).\nInstalling pip depends on your system and how you’d like to do it. It’s also best to know a little bit about virtual environments before you undertake this task.\nLet’s assume you have pip installed and you want to install pandas, usually something like\njust works. However, again, read below before you do this."
  },
  {
    "objectID": "python_virtual_environments.html#running-python",
    "href": "python_virtual_environments.html#running-python",
    "title": "7  Virtual Environments",
    "section": "7.1 Running python",
    "text": "7.1 Running python\nThere’s a few decisions that you have to make before you start running python. The first is, what sort of development environment will you choose? For me, there’s 3 realistic choices\n\nRunning python natively on the OS.\nRunning python within its virtual environment\nRunning python within a conda virtual environment\n\nOption 1 is to simply install (if not already installed) and run python. An example of this would be to install python from the Windows store. However, this approach has some limitations. Often you want different libraries installed for different, and maybe even different versions of python and other software. This is what virtual environments are created for. To create a virtual environment, simply do\npython -m venv myvenv\nHere python should be python 3. On some systems, the default python is python 3 and on others its python 2. If your default system is python 2, replace the above command with python3 -m .... This should have created a virtual environment in a directory. To activate it, you need to run the command\nmyvenv\\scripts\\activate.bat\non windows or\nsource myvenv/bin/activate\n\non unix type systems. Now your command or terminal prompt should look like this\n(myvenv) prompt>\nThe little (myvenv) reminds you that you’re in that enviroment. When you pip install programs now, they are installed in your environment, not in your general system. Here’s a nice tutorial on python venvs."
  },
  {
    "objectID": "python_virtual_environments.html#conda-and-anaconda",
    "href": "python_virtual_environments.html#conda-and-anaconda",
    "title": "7  Virtual Environments",
    "section": "7.2 Conda and anaconda",
    "text": "7.2 Conda and anaconda\nConda is my preferred solution for managing virtual environments. Conda platform containing miniconda, a command line version of conda, and Anaconda, a graphical variation. I’ll show the commands here, but you can do all of these things with Anaconda navigator if you would prefer. On windows, I would just use Anaconda navigator. When working remotely, you typically have to use the command line.\nWith conda, when you’re in a virtual environment, you’ll see it named at the prompt. For me, it looks something like this.\n(ds4bio) prompt>\nTo create a conda environment you can (either do it in the Anaconda GUI or) type\nconda create --name myvenv2 \nThen activate that environment by (either clicking on it in the Anaconda GUI or) typing\nconda activate myvenv2\nTo deactivate the environment (either select a different environment in Anaconda GUI or) type\nconda deactivate\nInstalls with conda are typically quite easy. Within an activated environment, you can use pip if you want. However, it’s probably easier to just use conda. For example\n(myvenv2) prompt> conda install pandas\ninstalls pandas into myenv2 (which I had already activated). In Anaconda navigator, just navigate to environments, activate the one you want, then search and click to checkmark the package you want to install. The nice thi1ng about conda, is that it installs non-python software as well. For example, if you want to install R into myenv2, try\n(myvenv2) prompt> conda install r-essentials r-base\n(or do it in Anaconda navigator). In fact, you can create an R environment from the start with\nprompt> conda create -n r-environment r-essentials r-base\nor Anaconda navigator has an R environment option on creation."
  },
  {
    "objectID": "python_virtual_environments.html#further-virtualization",
    "href": "python_virtual_environments.html#further-virtualization",
    "title": "7  Virtual Environments",
    "section": "7.3 Further virtualization",
    "text": "7.3 Further virtualization\nIf we think of pip < venv < conda in the terms of increasing virtualization and abstraction of your python environment, we should discuss further virtualization. Docker, for example, creates an entirely virtualized computational environment. In short, an entirely new operating system running within your existing operating system. Other variations of this, like virtualbox, create a nice user interface including the OS GUI.\nThese solutions virtualize the OS, but not the hardware. There are solutions to virtualize every aspect of the computer, qemu is an example. This could be useful if a sort of extreme variation of reproducibility, down to the bitwise operations, is required."
  },
  {
    "objectID": "data_cleaning_example.html",
    "href": "data_cleaning_example.html",
    "title": "9  Data cleaning, an example",
    "section": "",
    "text": "We’re going to cover data cleaning by an example. Primarily, you’re going to work in pandas, a library for manipulating tabular data."
  },
  {
    "objectID": "data_cleaning_example.html#imports-and-files",
    "href": "data_cleaning_example.html#imports-and-files",
    "title": "9  Data cleaning, an example",
    "section": "9.1 Imports and files",
    "text": "9.1 Imports and files\nThe first thing we’ll try is loading some data and plotting it. To do this, we’ll need some packages. Let’s load up pandas, a package for data management, matplotlib for plotting and numpy for numerical manipulations. The python command for this is import.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\n\nwe say as in order to not have to type out the entire module name to access its methods."
  },
  {
    "objectID": "data_cleaning_example.html#reading-data-in-with-pandas",
    "href": "data_cleaning_example.html#reading-data-in-with-pandas",
    "title": "9  Data cleaning, an example",
    "section": "9.2 Reading data in with pandas",
    "text": "9.2 Reading data in with pandas\nLet’s now read in an MRICloud dataset using pandas. We want to use the function read_csv within pandas. Notice we imported pandas as pd so the command is pd.read_csv. Also, pandas can accept URLs, so we just put the link to the file in the argument. The data we want to read in is in a github repo I created.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/kirby127a_3_1_ax_283Labels_M2_corrected_stats.csv\")\n\nYou can see the variables created with locals. However, this shows you everything and you usually have to text process it a little.\nLet’s look at the first 4 rows of our dataframe. The object dataset is a pandas object with associated methods. One is head which allows one to see the first few rows of data.\n\ndf.head(4)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      rawid\n      roi\n      volume\n      min\n      max\n      mean\n      std\n      type\n      level\n    \n  \n  \n    \n      0\n      1\n      kirby127a_3_1_ax.img\n      Telencephalon_L\n      531111\n      0\n      374\n      128.3013\n      51.8593\n      1\n      1\n    \n    \n      1\n      2\n      kirby127a_3_1_ax.img\n      Telencephalon_R\n      543404\n      0\n      300\n      135.0683\n      53.6471\n      1\n      1\n    \n    \n      2\n      3\n      kirby127a_3_1_ax.img\n      Diencephalon_L\n      9683\n      15\n      295\n      193.5488\n      32.2733\n      1\n      1\n    \n    \n      3\n      4\n      kirby127a_3_1_ax.img\n      Diencephalon_R\n      9678\n      10\n      335\n      193.7051\n      32.7869\n      1\n      1"
  },
  {
    "objectID": "data_eda.html",
    "href": "data_eda.html",
    "title": "10  Exploratory data analysis",
    "section": "",
    "text": "A picture is worth a 1,000 words\nOr saying how impactful intrer-ocular content is (i.e. when information hits you right between the eyes).\nI’m using Seaborn as the framework. There’s several plotting frameworks in python, but I find that seaborn has the nicest default plotting options. Also, it’s built on top of matplotlib, which is the main plotting library for DS for python.\nLet’s start with loading up some libraries.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()\n\nThe command sns.set sets the seaborn style. This sets the style for all matplotlib plots, even if not created in seaborn. I like the seaborn style, so I usually set it this way.\nFirst let’s download the data. Then we’ll read it in and drop some columns that aren’t needed for this analysis.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/kirby21.csv\")\ndf = df.drop(['Unnamed: 0', 'rawid', 'min', 'max', 'mean', 'std'], axis = 1)\ndf.head(4)\n\n\n\n\n\n  \n    \n      \n      roi\n      volume\n      type\n      level\n      id\n      icv\n      tbv\n    \n  \n  \n    \n      0\n      Telencephalon_L\n      531111\n      1\n      1\n      127\n      1378295\n      1268519\n    \n    \n      1\n      Telencephalon_R\n      543404\n      1\n      1\n      127\n      1378295\n      1268519\n    \n    \n      2\n      Diencephalon_L\n      9683\n      1\n      1\n      127\n      1378295\n      1268519\n    \n    \n      3\n      Diencephalon_R\n      9678\n      1\n      1\n      127\n      1378295\n      1268519\n    \n  \n\n\n\n\nLet’s look at the Type 1 Level 1 data and create a variable called comp which is brain composition, defined as the regional volumes over total brain volume. We’ll do this by selecting roi and comp then grouping by roi (region of interest) and taking the mean of the compostions.\n\n## Extract the Type 1 Level 1 data\nt1l1 = df.loc[(df['type'] == 1) & (df['level'] == 1)]\n\n\n## create a composition variable\nt1l1 = t1l1.assign(comp = t1l1['volume'] / t1l1['tbv'])\nt1l1 = t1l1.loc[t1l1['roi'] != 'CSF']\n\nLet’s get the mean of the composition variable across subjects by ROI. This is done by grouping by ROI then averaging over composition.\n\nsummary = t1l1[['roi', 'comp']].groupby('roi', as_index=False).mean()\nprint(summary)\n\n               roi      comp\n0   Diencephalon_L  0.007563\n1   Diencephalon_R  0.007634\n2    Mesencephalon  0.008647\n3    Metencephalon  0.124883\n4   Myelencephalon  0.003785\n5  Telencephalon_L  0.420305\n6  Telencephalon_R  0.427184\n\n\nOK, let’s try our first plot, a seaborn bar plot.\n\ng = sns.barplot(x='roi', y = 'comp', data = summary);\n## this is the matplotlib command for rotating \n## axis tick labels by 90 degrees.\nplt.xticks(rotation = 90);\n\n\n\n\nUnfortunately, seaborn doesn’t have a stakced bar chart. However, pandas does have one built in. To do this, however, we have to create a version of the data with ROIs as the columns. This can be done with a pivot statement. This converts our data from a “long” format to a “wide” format.\n\nt1l1pivot = t1l1.pivot(index = 'id', columns = 'roi', values = 'volume')\nt1l1pivot.head(4)\n\n\n\n\n\n  \n    \n      roi\n      Diencephalon_L\n      Diencephalon_R\n      Mesencephalon\n      Metencephalon\n      Myelencephalon\n      Telencephalon_L\n      Telencephalon_R\n    \n    \n      id\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      127\n      9683\n      9678\n      10268\n      159402\n      4973\n      531111\n      543404\n    \n    \n      142\n      9156\n      9071\n      10516\n      165803\n      4912\n      545603\n      552216\n    \n    \n      239\n      8937\n      9004\n      9070\n      124504\n      4023\n      483107\n      490805\n    \n    \n      346\n      8828\n      8933\n      9788\n      135090\n      4428\n      558849\n      568830\n    \n  \n\n\n\n\nNow that the data is in the right format, we can do our plot.\n\nt1l1pivot.plot(kind='bar', stacked=True, legend= False);\nplt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5));\n\n\n\n\nLet’s do some scatterplots. Let’s look at bilateral symmetry of the telencephalon. That is, let’s plot the right telencephalon versus the left telencephalon.\n\nsns.scatterplot(x = 'Telencephalon_L', y = 'Telencephalon_R', data = t1l1pivot);\nplt.xticks(rotation = 90);\n#plot an identity line from the data min to the data max\nx1 = min([t1l1pivot.Telencephalon_L.min(), t1l1pivot.Telencephalon_R.min()])\nx2 = max([t1l1pivot.Telencephalon_L.max(), t1l1pivot.Telencephalon_R.max()])\nplt.plot([x1, x2], [x1 , x2]);\n\n\n\n\nThis plot has the issue that there’s a lot of blank space. This is often addressed via a mean difference plot. This plot shows (X+Y) / 2 versus (X-y). This is basically just rotating the plot above by 45 degrees to get rid of all of the blank space around the diagonal line. Alternatively, you could plot (log(x) + log(y)) / 2 versus log(X) - log(Y). This plots the log of the geometric mean of the two observations versus the log of their ratio. Sometimes people use log base 2 or log base 10.\n\nt1l1pivot = t1l1pivot.assign(Tel_logmean = lambda x: (np.log(x.Telencephalon_L) * .5 +  np.log(x.Telencephalon_R)* .5))\nt1l1pivot = t1l1pivot.assign(Tel_logdiff = lambda x: (np.log(x.Telencephalon_R) -  np.log(x.Telencephalon_L)))\nsns.scatterplot(x = 'Tel_logmean', y = 'Tel_logdiff', data = t1l1pivot);\nplt.axhline(0, color='green');\nplt.xticks(rotation = 90);\n\n\n\n\nThus, apparently, the right side is always a little bigger than the left and the scale of the ratio is \\(e^{0.02}\\) while the scale of the geometric mean is \\(e^{13}\\). Note, \\(\\exp(x) \\approx 1 + x\\) for \\(x \\approx 0\\). So it’s about 2% larger. A note about right versus left in imaging. Often the labels get switched as there are different conventions (is it the right of the subject or the right of the viewer when looking straight at the subject?). Typically, it’s known that some of the areas of subject’s left hemisphere are larger and so it’s probably radiological (right of the viewer) convention here. Here’s a nicely done article about right versus left brain.\n(Also, in case you don’t believe me, here’s a plot of \\(e^x\\) versus \\(1+x\\) for values up to 0.1. This is the so-called Taylor expasion for \\(e^x\\) around 0. Notice the approximation gets worse, the curves diverge, as you get further away from 0.)\n\n## A sequence of numbers from 0 to .1 spaced by 0.001\nx = np.arange(0, .1, .001)\nex = np.exp(x)\n\nsns.lineplot(x = x, y = ex)\nplt.plot(x, x + 1)"
  },
  {
    "objectID": "data_sqlite.html",
    "href": "data_sqlite.html",
    "title": "11  SQL via sqlite",
    "section": "",
    "text": "In this page, we’ll cover some of the basics of SQL (structured querry language) by working through some examples. SQL is a set of language standards for databases, so we have to choose a specific implementation. We’ll use sqlite for this purpose. As its name implies, sqlite is a small implementation of SQL.\nIn my linux implementation, sqlite3 was pre-installed. Here’s a tutorial on installing for windows. Sqlite3 is a single file.\nWe’ll first create a database at the command line. Notice when we create a file\nPerforming an ls in the current working directory now shows the file class.db. Everything else we discuss below assumes working in the sqlite command prompt.\nTo work with sqlite, it’s nice to work with a development environment specifically created for sql. Specifically, one with nice highlighting and autocompletion. Since I’m writing these notes in jupyter, I’m just pasting code output.\nSqlite has SQL commands, which must be typed with a semicolon at the end, and sqlite specific commands, which begin with a period and the pragma commands, which are also sqlite specific. This is good to remember, since some things will be portable to other SQL implementations and others not. ]"
  },
  {
    "objectID": "data_sqlite.html#a-more-reaslistic-example",
    "href": "data_sqlite.html#a-more-reaslistic-example",
    "title": "11  SQL via sqlite",
    "section": "11.1 A more reaslistic example",
    "text": "11.1 A more reaslistic example\nLet’s create and work with a more realistic example. Consider the data Opiods in the US at Open Case Studies https://github.com/opencasestudies/ocs-bp-opioid-rural-urban as described here. Read over their writeup, as we’re mostly going to be showing how to duplicate a lot of their steps in sqlite.\nFirst, you need to download the data, which you could do by right clicking and saving the file or with a command:\nwget https://raw.githubusercontent.com/opencasestudies/ocs-bp-opioid-rural-urban/master/data/simpler_import/county_pop_arcos.csv\nwget https://raw.githubusercontent.com/opencasestudies/ocs-bp-opioid-rural-urban/master/data/simpler_import/land_area.csv\nwget https://raw.githubusercontent.com/opencasestudies/ocs-bp-opioid-rural-urban/master/data/simpler_import/county_annual.csv\nNext, let’s import them into sqlite\ncommand prompt> sqlite3 opioid.db\nsqlite> .mode csv\nsqlite> .import county_pop_arcos.csv population\nsqlite> .import county_annual.csv annual\nsqlite> .import land_area.csv land\nsqlite> .tables\nannual      land        population\nWhat variables do the tables include? The pragma command is unique to sqlite and contains a bunch of helper functions.\nsqlite> pragma table_info(population);\ncid  name          type  notnull  dflt_value  pk\n---  ------------  ----  -------  ----------  --\n0                  TEXT  0                    0 \n1    BUYER_COUNTY  TEXT  0                    0 \n2    BUYER_STATE   TEXT  0                    0 \n3    countyfips    TEXT  0                    0 \n4    STATE         TEXT  0                    0 \n5    COUNTY        TEXT  0                    0 \n6    county_name   TEXT  0                    0 \n7    NAME          TEXT  0                    0 \n8    variable      TEXT  0                    0 \n9    year          TEXT  0                    0 \n10   population    TEXT  0                    0 \nsqlite> pragma table_info(annual);\ncid  name          type  notnull  dflt_value  pk\n---  ------------  ----  -------  ----------  --\n0                  TEXT  0                    0 \n1    BUYER_COUNTY  TEXT  0                    0 \n2    BUYER_STATE   TEXT  0                    0 \n3    year          TEXT  0                    0 \n4    count         TEXT  0                    0 \n5    DOSAGE_UNIT   TEXT  0                    0 \n6    countyfips    TEXT  0                    0\nsqlite> pragma table_info(land)\ncid  name         type  notnull  dflt_value  pk\n---  -----------  ----  -------  ----------  --\n0                 TEXT  0                    0 \n1    Areaname     TEXT  0                    0 \n2    STCOU        TEXT  0                    0 \n3    LND010190F   TEXT  0                    0 \n4    LND010190D   TEXT  0                    0 \n5    LND010190N1  TEXT  0                    0\n(I truncated this latter output at 5.)"
  },
  {
    "objectID": "data_sqlite.html#working-with-data",
    "href": "data_sqlite.html#working-with-data",
    "title": "11  SQL via sqlite",
    "section": "11.2 Working with data",
    "text": "11.2 Working with data\nLet’s print out a few columns of the population data.\nsqlite> select BUYER_COUNTY, BUYER_STATE, STATE, COUNTY, year, population from population limit 5;\nBUYER_COUNTY  BUYER_STATE  STATE  COUNTY  year  population\n------------  -----------  -----  ------  ----  ----------\nAUTAUGA       AL           1      1       2006  51328     \nBALDWIN       AL           1      3       2006  168121    \nBARBOUR       AL           1      5       2006  27861     \nBIBB          AL           1      7       2006  22099     \nBLOUNT        AL           1      9       2006  55485   \nThe limit 5 prints out five rows. Let’s perform some of the tasks in the write up. For example, they want to print out some of the missing data in the annual dataset.\nsqlite> select * from annual where countyfips = \"NA\" limit 10;\n     BUYER_COUNTY  BUYER_STATE  year  count  DOSAGE_UNIT  countyfips\n---  ------------  -----------  ----  -----  -----------  ----------\n188  ADJUNTAS      PR           2006  147    102800       NA        \n189  ADJUNTAS      PR           2007  153    104800       NA        \n190  ADJUNTAS      PR           2008  153    45400        NA        \n191  ADJUNTAS      PR           2009  184    54200        NA        \n192  ADJUNTAS      PR           2010  190    56200        NA        \n193  ADJUNTAS      PR           2011  186    65530        NA        \n194  ADJUNTAS      PR           2012  138    57330        NA        \n195  ADJUNTAS      PR           2013  138    65820        NA        \n196  ADJUNTAS      PR           2014  90     59490        NA        \n197  AGUADA        PR           2006  160    49200        NA   \nHere, we used the condition “NA” to test for missingness, since the CSV files have the string NA values for missing data. Places other than Puerto Rico (PR)? Lets check some\nsqlite> select * from annual where countyfips = \"NA\" and BUYER_STATE != \"PR\" limit 10;\n       BUYER_COUNTY  BUYER_STATE  year  count  DOSAGE_UNIT  countyfips\n-----  ------------  -----------  ----  -----  -----------  ----------\n10072  GUAM          GU           2006  319    265348       NA        \n10073  GUAM          GU           2007  330    275600       NA        \n10074  GUAM          GU           2008  313    286900       NA        \n10075  GUAM          GU           2009  390    355300       NA        \n10076  GUAM          GU           2010  510    413800       NA        \n10077  GUAM          GU           2011  559    475600       NA        \n10078  GUAM          GU           2012  616    564800       NA        \n10079  GUAM          GU           2013  728    623200       NA        \n10080  GUAM          GU           2014  712    558960       NA        \n17430  MONTGOMERY    AR           2006  469    175390       NA     \nInspect the missing data further on your own. It looks like its the unincorporated territories and a handful of Arkansas values missing countyfips (Federal Information Processing Standard). Specifically, Montgomery county AR is missing FIPs codes. Since we want to look US states in specific, excluding territories, we will just set the Montgomery county ones to the correct value 05097 and ignore the other missing values.\nsqlite> update annual set countyfips = 05097 where BUYER_STATE = \"AR\" and BUYER_COUNTY = \"MONTGOMERY\"\nsqlite> select * from annual where BUYER_STATE = \"AR\" and BUYER_COUNTY = \"MONTGOMERY\"\n\n       BUYER_COUNTY  BUYER_STATE  year  count  DOSAGE_UNIT  countyfips\n-----  ------------  -----------  ----  -----  -----------  ----------\n17430  MONTGOMERY    AR           2006  469    175390       5097      \n17431  MONTGOMERY    AR           2007  597    241270       5097      \n17432  MONTGOMERY    AR           2008  561    251760       5097      \n17433  MONTGOMERY    AR           2009  554    244160       5097      \nNow lets delete rows from the annual table that have missing county data. Check on these counties before and verify that the’ve been deleted afterwards. Also, we want to grab just three columns from the land table, so let’s create a new one called land_area. Also, the column there is called STCOU, which we want to rename to coutyfips. (I’m going to stop printing out the results of every step, so make sure you’re checking your work as you go.)\nsqlite> delete from annual where BUYER_COUNTY = \"NA\"\nsqlite> create table land_area as select Areaname, STCOU, LND110210D from land;\nsqlite> alter table land_area rename column STCOU to countyfips;\nNext we want to start joining the tables, so let’s left join our table and print out the counts to make sure we accounted correctly.\nsqlite> create table county_info as select * from population left join land_area using(countyfips);\nsqlite> select count(*) from land;\n3198\nsqlite> select count(*) from land_area;\n3198\nsqlite> select count(*) from county_info;\n28265\nsqlite> select count(*) from population;"
  },
  {
    "objectID": "data_sqlite.html#notes",
    "href": "data_sqlite.html#notes",
    "title": "11  SQL via sqlite",
    "section": "11.3 Notes",
    "text": "11.3 Notes\nAt this point, hopefully you have enough of a background to finish doing the example from Open Case Studies. I have to say, that working with SQL is pleasant, but I prefer python as a home base. In addition, after working with the data, I want to use plotting and analysis tools. In the next chapter, we’ll look at using python as a base language to interact with an sqlite database."
  },
  {
    "objectID": "data_sqlite.html#sqlite-in-python",
    "href": "data_sqlite.html#sqlite-in-python",
    "title": "11  SQL via sqlite",
    "section": "11.4 sqlite in python",
    "text": "11.4 sqlite in python\nAn sqlite3 library ships with python. In this tutorial, we’ll discuss how to utilize this library and read sqlite tables into pandas. With this, you can generalize to other python APIs to other databases.\nFirst, let’s continue on with our work from the previous notebook. A nice little tutorial can be found here.\n\nimport sqlite3 as sq3\nimport pandas as pd\n\ncon = sq3.connect(\"sql/opioid.db\")\n# cursor() creates an object that can execute functions in the sqlite cursor\n\nsql = con.cursor()\n\nfor row in sql.execute(\"select * from county_info limit 5;\"):\n    print(row)\n\n    \n# you have to close the connection\ncon.close\n\n('1', 'AUTAUGA', 'AL', '01001', '1', '1', 'Autauga', 'Autauga County, Alabama', 'B01003_001', '2006', '51328', 'Autauga, AL', '594.44')\n('2', 'BALDWIN', 'AL', '01003', '1', '3', 'Baldwin', 'Baldwin County, Alabama', 'B01003_001', '2006', '168121', 'Baldwin, AL', '1589.78')\n('3', 'BARBOUR', 'AL', '01005', '1', '5', 'Barbour', 'Barbour County, Alabama', 'B01003_001', '2006', '27861', 'Barbour, AL', '884.88')\n('4', 'BIBB', 'AL', '01007', '1', '7', 'Bibb', 'Bibb County, Alabama', 'B01003_001', '2006', '22099', 'Bibb, AL', '622.58')\n('5', 'BLOUNT', 'AL', '01009', '1', '9', 'Blount', 'Blount County, Alabama', 'B01003_001', '2006', '55485', 'Blount, AL', '644.78')\n\n\n<function Connection.close()>"
  },
  {
    "objectID": "data_sqlite.html#reading-into-pandas",
    "href": "data_sqlite.html#reading-into-pandas",
    "title": "11  SQL via sqlite",
    "section": "11.5 Reading into pandas",
    "text": "11.5 Reading into pandas\nLet’s read our sqlite database into pandas. At this point, we can then work on the dataset entirely in pandas. This is closest to how I work. I’m typically more comfortable working in R or python and so get my data out of database formats and into tidyverse or pandas formats as soon as I can.\n\ncon = sq3.connect(\"sql/opioid.db\")\n\ncounty_info = pd.read_sql_query(\"SELECT * from county_info\", con)\n\n# you have to close the connection\ncon.close\n\ncounty_info.head\n\n<bound method NDFrame.head of                BUYER_COUNTY BUYER_STATE countyfips STATE COUNTY  \\\n0          1        AUTAUGA          AL      01001     1      1   \n1          2        BALDWIN          AL      01003     1      3   \n2          3        BARBOUR          AL      01005     1      5   \n3          4           BIBB          AL      01007     1      7   \n4          5         BLOUNT          AL      01009     1      9   \n...      ...            ...         ...        ...   ...    ...   \n28260  28261       WASHAKIE          WY      56043    56     43   \n28261  28262         WESTON          WY      56045    56     45   \n28262  28263        SKAGWAY          AK      02230     2    230   \n28263  28264  HOONAH ANGOON          AK      02105     2    105   \n28264  28265     PETERSBURG          AK      02195     2    195   \n\n         county_name                               NAME    variable  year  \\\n0            Autauga            Autauga County, Alabama  B01003_001  2006   \n1            Baldwin            Baldwin County, Alabama  B01003_001  2006   \n2            Barbour            Barbour County, Alabama  B01003_001  2006   \n3               Bibb               Bibb County, Alabama  B01003_001  2006   \n4             Blount             Blount County, Alabama  B01003_001  2006   \n...              ...                                ...         ...   ...   \n28260       Washakie           Washakie County, Wyoming  B01003_001  2014   \n28261         Weston             Weston County, Wyoming  B01003_001  2014   \n28262        Skagway       Skagway Municipality, Alaska  B01003_001  2014   \n28263  Hoonah Angoon  Hoonah-Angoon Census Area, Alaska  B01003_001  2014   \n28264     Petersburg         Petersburg Borough, Alaska  B01003_001  2014   \n\n      population           Areaname LND110210D  \n0          51328        Autauga, AL     594.44  \n1         168121        Baldwin, AL    1589.78  \n2          27861        Barbour, AL     884.88  \n3          22099           Bibb, AL     622.58  \n4          55485         Blount, AL     644.78  \n...          ...                ...        ...  \n28260       8444       Washakie, WY    2238.55  \n28261       7135         Weston, WY    2398.09  \n28262        996        Skagway, AK     452.33  \n28263       2126  Hoonah-Angoon, AK    7524.92  \n28264       3212     Petersburg, AK    3281.98  \n\n[28265 rows x 13 columns]>"
  },
  {
    "objectID": "data_advanced_databases.html",
    "href": "data_advanced_databases.html",
    "title": "13  Big data storage",
    "section": "",
    "text": "You’ve probably already learned about some variation of databases either sql, nosql, spark, a cloud db, … We covered sqlite last chapter. Often, the backend of these databases can be quite complicated, while the front end requires SQL querries or something similar. We’ll look at a non-relational database format that is specifically useful for scientific computing called hdf5. HDF5 has implementations in many languages, but we’ll look at python. This is a hierarchical data format specifically useful for large array calculations.\nLet’s create a basic h5py file. First, let’s load our stuff.\nNow, let’s create an empty hdf5 file. Here’s the basic code; the option w is open for writing. There’s also w-, r, r+, a for write protected, read only, read/write, read/write and create. The first time I ran it I used:\nThen, subsequently\nNow let’s populate it with some data. The hdf5 file works almost like a directory where we can store hierarchical data. For example, suppose that we want sensors stored in a superstructure called sensors and want to fill in the data for sensor1 and sensor1.\nNow we can do normal np stuff on this sensor. However, hdf5 is only bringing in the part that we are using into memory. This allows us to work with very large files. Also, as we show here, you can name the data to a variable since that’s more convenient."
  },
  {
    "objectID": "data_advanced_databases.html#blockwise-basic-statistical-calculations",
    "href": "data_advanced_databases.html#blockwise-basic-statistical-calculations",
    "title": "13  Big data storage",
    "section": "13.1 Blockwise basic statistical calculations",
    "text": "13.1 Blockwise basic statistical calculations\nNow, consider taking the mean of both variables. Imagine that the time series is so long it’s not feasible to load into memory. So, we want to read it in blocks. You want your blocks to be as big as possible, since that’s fastest. In our case, of course, none of this is necessary.\nOur goal in this section is to do the following: calculate the empirical mean and variance for each sensor, center and scale each sensor, and write those changes to those variables, calculate the sample correlation then calculate the residual for sensor1 given sensor2. (I think typically you wouldn’t want to overwrite the original data; but, this is for pedagogical purposes.) We want our data organized so sensors are stored in a hierarchical “folder” called sensors and processed data is in a different folder.\nWe’re just simulating iid standard normals. So, we have a rough idea of the answers we should get, since the the data are theoretically mean 0, variance 1 and uncorrelated. After our calculations, they will have empirical mean 0 and variance 1 and the empirical correlation between the residual and sensor 2 will be 0.\nLet’s consider a block variation of the inner product. \\[\n<a, b> = \\sum_{i=0}^{n-1} a_i b_i = \\sum_{i=0}^{n/B} \\sum_{j=0}^{B-1} a_{j + i B} b_{j + i B}\n\\] (if \\(n\\) is divisible by \\(B\\). Otherwise you have to figure out what to do with the final block, which isn’t hard but makes the notation messier.) So, for example, the (sample) mean is then \\(<x, J>/n\\) where \\(J\\) is a vector of ones.\nLet’s calculate the mean using blockwise calculations.\n\nn = s1.shape[0]\nB = 32\n## mean center the blocks\nmean1 = 0\nmean2 = 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    mean1 += s1[block_indices].sum() / n \n    mean2 += s2[block_indices].sum() / n\n\n[mean1, mean2]\n\n[0.02317472126421261, 0.018259887652036223]\n\n\nLet’s now center our time series.\n\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    s1[block_indices] -= mean1  \n    s2[block_indices] -= mean2\n\nNow the (unbiased, sample) variance of centered vector \\(a\\) is simply \\(<a, a>/(n-1)\\).\n\nv1, v2 = 0, 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    v1 += np.sum(s1[block_indices] ** 2) / (n - 1)\n    v2 += np.sum(s2[block_indices] ** 2) / (n - 1)\n[v1, v2]\n\n[0.9303145564806921, 0.9518095709661069]\n\n\nNow let’s scale our vectors as\n\nsd1 = np.sqrt(v1)\nsd2 = np.sqrt(v2)\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    s1[block_indices] /= v1  \n    s2[block_indices] /= v2\n\nNow that our vectors are centered and scaled, the empirical correlation is simply \\(<a, b>/(n-1)\\). Let’s do that\n\ncor = 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    cor += np.sum(s1[block_indices] * s2[block_indices]) / (n-1) \ncor\n\n-0.009267019800591662\n\n\nFinally, we want to “regress out” s2 from s1. Since we normalized our series, the correlation is slope coefficient from linear regression (regardless of the outcome and dependent variable) and the intercept is zero (since we centered). Thus, the residual we want is \\(e_{12} = s_1 - \\rho s_2\\) where \\(\\rho\\) is the correlation.\n\nf['processed/resid_s1_s2'] = np.empty(n)\ne12 = f['processed/resid_s1_s2']\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    e12[block_indices] += s1[block_indices] - cor * s2[block_indices] \n\nNow we have our new processed data stored in a vector. To close our database simply do:\n\nf.close()\n\nNow our processed data is stored on disk.\n\nf = h5py.File('sensor.hdf5', 'r')\nf['processed/resid_s1_s2']\n\n<HDF5 dataset \"resid_s1_s2\": shape (1024,), type \"<f8\">\n\n\n\nf.close()"
  },
  {
    "objectID": "data_advanced_databases.html#homework",
    "href": "data_advanced_databases.html#homework",
    "title": "13  Big data storage",
    "section": "13.2 Homework",
    "text": "13.2 Homework\n\nPerform lots of regressions. Suppose that you have a setting where you would like to perform the operation \\[\n(X'X)^{-1} X' Y\n\\] where \\(X\\) is \\(n\\times p\\) and \\(Y\\) is \\(n\\times v\\). Consider the case where \\(Y\\) is very large (so \\(V\\) is large). Simulate some data where you perform this linear model in block calculations.\nWrite a block matrix multiplication program that takes in two matrices with agreeable dimensions stored as HDF5 and multiplies them in block sizes specified by the user."
  },
  {
    "objectID": "data_webscraping.html",
    "href": "data_webscraping.html",
    "title": "14  Webscraping",
    "section": "",
    "text": "We’ll need some packages to start, requests, beautifulsoup4 and selenium. Requesting elements from a static web page is very straightforward. Let’s take an example by trying to grab and plot the table of multiple Olympic medalists from Wikipedia then create a barplot of which sports have the most multiple medal winners.\nFirst we have to grab the data from the url, then pass it to beautifulsoup4, which parses the html, then pass it to pandas. First let’s import the packages we need.\nWe then need to read the web page into data.\nNow let’s read the page into bs4. Then we want to find the tables in the page. We add the class and wikitable information to specify which tables that we want. If you want to find classes, you can use a web tool, like selectorgadget or viewing the page source.\nNow we should take the html that we’ve saved, then read it into pandas. Fortunately, pandas has a read_html method. So, we convert our tables to strings then read it in. Since there’s multiple tables, we grab the first one.\nNow we’re in a position to build our plot. Let’s look at the count of 4 or more medal winers by sport and games."
  },
  {
    "objectID": "data_webscraping.html#selenium",
    "href": "data_webscraping.html#selenium",
    "title": "14  Webscraping",
    "section": "14.1 Selenium",
    "text": "14.1 Selenium\nIf the page has javacript, your basic web scraping may not work. In this case, you not only need to get and parse the page, but also to interact with the javascript. For this, enter Selenium. This is a python browser that allows you to automate web navigation. We’ll cover that in the next chapter."
  },
  {
    "objectID": "data_advanced_webscraping.html",
    "href": "data_advanced_webscraping.html",
    "title": "15  Advanced web scraping",
    "section": "",
    "text": "Before you start webscraping make sure to consider what you’re doing. Does your scraping violate TOS? Will it inconvenience the site, other users? Per Uncle Ben: WGPCGR.\nAlso, before you begin web scraping, look for a download data option or existing solution. Probably someone has run up against the same problem and worked it out. For example, we’re going to scrape some wikipedia tables, which there’s a million other solutions for, including a wikipedia api."
  },
  {
    "objectID": "data_advanced_webscraping.html#basic-web-scraping",
    "href": "data_advanced_webscraping.html#basic-web-scraping",
    "title": "15  Advanced web scraping",
    "section": "15.2 Basic web scraping",
    "text": "15.2 Basic web scraping\nWe covered this last chapter. However, let’s do an example of static page parsing just to get started. Consider scraping the table of top 10 heat waves from wikipedia. First, we open the url, then parse it using BeautifulSoup, then load it into a pandas dataframe.\n\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup as bs\nimport pandas as pd\nurl = \"https://en.wikipedia.org/wiki/List_of_natural_disasters_by_death_toll\"\nhtml = urlopen(url)\nparsed = bs(html, 'html.parser').findAll(\"table\")\npd.read_html(str(parsed))[11]\n\n\n\n\n\n  \n    \n      \n      Rank\n      Death toll\n      Event\n      Location\n      Date\n    \n  \n  \n    \n      0\n      1.0\n      1300\n      The Daulatpur–Saturia tornado\n      Manikganj, Bangladesh\n      1989\n    \n    \n      1\n      2.0\n      695\n      The Tri-State tornado outbreak\n      United States (Missouri–Illinois–Indiana)\n      1925\n    \n    \n      2\n      3.0\n      681\n      1973 Dhaka tornado\n      Bangladesh\n      1973\n    \n    \n      3\n      4.0\n      660\n      1969 East Pakistan tornado\n      East Pakistan (now Bangladesh)\n      1969\n    \n    \n      4\n      5.0\n      600\n      The Valletta, Malta tornado\n      Malta\n      1551 or 1556\n    \n    \n      5\n      6.0\n      500\n      The 1851 Sicily tornadoes\n      Sicily, Two Sicilies (now Italy)\n      1851\n    \n    \n      6\n      6.0\n      500\n      Narail-Magura tornado\n      Jessore, East Pakistan, Pakistan (now Bangladesh)\n      1964\n    \n    \n      7\n      6.0\n      500\n      Madaripur-Shibchar tornado\n      Bangladesh\n      1977\n    \n    \n      8\n      9.0\n      400\n      The 1984 Soviet Union tornado outbreak\n      Soviet Union (now Russia)\n      1984\n    \n    \n      9\n      10.0\n      317\n      The Great Natchez Tornado\n      United States (Mississippi–Louisiana)\n      1840\n    \n  \n\n\n\n\nThe workflow as as follows:\n\nWe used the developer console on the webpage to inspect the page and its properties.\nWe opened the url with urlopen\nWe parsed the webpage with BeautifulSoup then used the method findAll on that to search for every table\nPandas has a utility that converts a html tables into a dataframe. In this case it creates a list of tables, where the 12th one is the heatwaves. Note it needs the data to be converted to a string before proceeding.\n\nThis variation of web scraping couldn’t be easier. However, what if the content we’re interested in only exists after interacting with the page? Then we need a more sophisticated solution."
  },
  {
    "objectID": "data_advanced_webscraping.html#form-filling",
    "href": "data_advanced_webscraping.html#form-filling",
    "title": "15  Advanced web scraping",
    "section": "15.3 Form filling",
    "text": "15.3 Form filling\nWeb scraping can require posting to forms, such as logins. This can be done directly with python / R without elaborate programming, for example using the requests library. However, make sure you aren’t violating a web site’s TOS and also make sure you’re not posting your password to github as you commit scraping code. In general, don’t create a security hole for your account by web scraping it. Again, also check to make sure that the site doesn’t have an API with an authentication solution already before writing the code to post authentication. Many websites that want you to programmatically grab the data build an API."
  },
  {
    "objectID": "data_advanced_webscraping.html#programmatically-web-browsing",
    "href": "data_advanced_webscraping.html#programmatically-web-browsing",
    "title": "15  Advanced web scraping",
    "section": "15.4 Programmatically web browsing",
    "text": "15.4 Programmatically web browsing\nSome web scraping requires us to interact with the webpage. This requires a much more advanced solution where we programmatically use a web browser to interact with the page. I’m using selenium and chromedriver. To do this, I had to download chromedriver and set it so that it was in my unix PATH.\n\nfrom selenium import webdriver\ndriver = webdriver.Chrome()\ndriver.quit()\n\nIf all went well, a chrome window appeared then closed. That’s the browser we’re going to program. If you look closely at the browser before you close it, there’s a banner up to that says “Chrome is being controlled by automated test software.” Let’s go through the example on the selenium docs here. First let’s vist a few pages. We’ll go to my totally awesome web page that I meticulously maintain every day then duckduckgo. We’ll wait a few seconds in between. My site is created and hosted by google sites, which seems reasonable that they would store a cookie so that I can log in and edit my site (which I almost never do). Duckduckgo is a privacy browser, so let’s check to see if they create a cookie. (Hint, I noticed that selenium doesn’t like redirects, so use the actual page url.)\n\ndriver = webdriver.Chrome()\ndriver.get(\"https://sites.google.com/view/bcaffo/home\")\nprint(driver.get_cookies())\ndriver.implicitly_wait(5)\n## Let's get rid of all cookies before we visit duckduckgo\ndriver.delete_all_cookies()\ndriver.get(\"https://duckduckgo.com/\")\nprint(driver.get_cookies())\n\nFor me, at least, this prints out the cookie info for my google site then nothing for ddg. (I’m not evaluating the code in quarto since I don’t want to bring up the browser when I compile the document.)\nNow let’s find the page elements that we’d like to interact with. There’s a text box that we want to submit a search command into and a button that we’ll need to press. When I go to ddg and press CTRL-I I find that the search box is:\n<input id=\"search_form_input_homepage\" class=\"js-search-input search__input--adv\" type=\"text\" autocomplete=\"off\" name=\"q\" tabindex=\"1\" value=\"\" autocapitalize=\"off\" autocorrect=\"off\" placeholder=\"Search the web without being tracked\">\nNotice, the name=\"q\" html name for the search form. When I dig around and find the submit button, it’s code is:\n<input id=\"search_button_homepage\" class=\"search__button  js-search-button\" type=\"submit\" tabindex=\"2\" value=\"S\">\nNotice its id is search_button_homepage. Let’s find these elements.\n\nsearch_box = driver.find_element(by=By.NAME, value=\"q\")\nsearch_button = driver.find_element(by=By.ID, value=\"search_button_homepage\")\n\nNow let’s send the info and press submit\n\nsearch_box.send_keys(\"Selenium\")\nsearch_button.click()\ndriver.implicitly_wait(10)\ndriver.save_screenshot(\"assets/images/webscraping.png\")\npage_source = driver.page_source\ndriver.close()\n\nHere, we saved the page_source as a variable that then can be parsed with other html parses (like bs4). Play around with the methods associated with driver and navigate the web. You’ll see that selenium is pretty incredible. Here’s the screenshot that we took:\n\n\n\nScreenshot of webscraping"
  },
  {
    "objectID": "data_advanced_webscraping.html#homework",
    "href": "data_advanced_webscraping.html#homework",
    "title": "15  Advanced web scraping",
    "section": "15.5 Homework",
    "text": "15.5 Homework\n\nWrite a function that takes a search term, enters it into this link and returns the number of characters from the output.\nWrite a function that solves THE MAZE and returns your current location at its solution"
  },
  {
    "objectID": "intro_html.html",
    "href": "intro_html.html",
    "title": "4  HTML, CSS and javascript",
    "section": "",
    "text": "HTML is a markup language used by web browsers. HTML stands for hypetext markup language. Like all markup languages, it gives a text set of instructions that get interpreted into a nicer looking document. Other markup languages include XML, LaTeX, Org and markdown. (Yes, mark”down” is named as such since it’s a ultra-simple mark”up” language.)\nWe’ll need a little html knowledge since so much data science output is web-page oriented. Also, we’ll need to know a little about html to scrape web content. A web page typically has three elements: the html which gives the page structure and markup, css (cascading style sheets) for style and javascript for interactivity. We’ll cover a little html and javascript so that we can better understand certain data science products. However, you should take a web development course if you want in depth treatments.\nWe won’t spend much time talking about CSS. CSS gives a set of standards for the style of a web page. With CSS one can take the skeleton (HTML/JS) and dramatically change the style in the same way you could choose to play some sheet music in different ways. A quick tutorial on CSS can be found here.\nBack to HTML. An HTML document looks something like this. Take a file, insert the following code and give it the extension .html. Then, open it up in a browser.\n<!DOCTYPE html>\n<HTML>\n    <HEAD>\n        <TITLE> This is the web page title</TITLE>\n    </HEAD>\n    <BODY>\n        <H1>Heading 1</H1>\n        <H2>Heading 2</H2>\n        <P> Paragraph </P>\n        <CODE> CODE </CODE>\n    </BODY>\n</HTML>\nThe resulting document will look like the following\n\n\n   \n      \n      \n       Paragraph \n      CODE \n   \n\nAs you probably noticed, a bit of markup is something like <COMMAND>CONTENT</COMMAND>. The latter command has a forward slash. You should close your commands, even if your browser still renders the page like you like just because it makes for bad code not to. Also, someone else’s browser may not be as forgiving. Good code editors will help remind you to close your commands."
  },
  {
    "objectID": "intro_html.html#browser-stuff",
    "href": "intro_html.html#browser-stuff",
    "title": "4  HTML, CSS and javascript",
    "section": "4.2 Browser stuff",
    "text": "4.2 Browser stuff\nNote, since we’ll be working a lot with files, probably in one directory, you can use file:///PATH TO YOUR DIRECTORY to open up files (maybe even bookmark that directory). Also, CTRL-R is probably faster than clicking refresh and (in chrome at least) CTRL-I brings up developer tools (javascript console). When we have a web server running locallly, you usually go to localhost. For example, my jupyter lab server sends me to http://localhost:8888/lab/tree/. Here 8888 is a port, localhost refers to the server running on the lcoal computer and lab/tree is the relative path to the root of my jupyter lab server.\nBrowsers make choices in how they render HTML and CSS and implement javascript. So, unless you’re a web developer by trade, don’t get too exotic in your design choices. Also, a lot of HTML is auto generated. So, your mileage may vary by looking at page sources."
  },
  {
    "objectID": "intro_html.html#hosting",
    "href": "intro_html.html#hosting",
    "title": "4  HTML, CSS and javascript",
    "section": "4.3 Hosting",
    "text": "4.3 Hosting\nWhen you double click on your html file, it’s being hosted locally. So, no one else can see it. To have a web page on the internet it has to be hsoted on a server running web hosting software. Fortunately, github will actually allow us to host web pages. Basically, put an empty .nojekyll file in your repository (this tells it that it’s not a jekyll based web site and follow the instructions here. This will be really useful for us, since many of our datascience programs output web pages. For example, RMarkdown documents get translated into web documents. Similarly, jupyter-lab will output reveal.js (javascript/html) slide decks from our jupyter lab notebooks. Note that some of our programs will require servers that also run python or R in the back end, so github pages won’t suffice for that. There we need servers specifically set up to run those kinds of scripts."
  },
  {
    "objectID": "intro_html.html#javascript",
    "href": "intro_html.html#javascript",
    "title": "4  HTML, CSS and javascript",
    "section": "4.4 Javascript",
    "text": "4.4 Javascript\nJavascript is what makes webpages interactive. We’ll need a little javascript to understand how interactive web graphics work. Consider the following where we use javscript to change an HTML element in a web page\n<H2 id=\"textToChange\">Preference ?</H2>\n\n<button type=\"button\" onclick='document.getElementById(\"textToChange\").innerHTML = \"You prefer 1\"'>1</button>\n<button type=\"button\" onclick='document.getElementById(\"textToChange\").innerHTML = \"You prefer 2\"'>2</button>\nHere’s the result:\nPreference ?\nClick 1\nClick 2\n\n4.4.1 JSON\nJSON is a data format used in javascript, and adopted elsewhere. It’s a fairly straightforward data structure. In fact, you might have edited some JSON data by editing your Jupyter Lab ipython notebook properties. It goes \"name\":value where objects are enclosed in curly braces and arrays in brackets. You have to separate distinct object or values with quotes.\n{\n    \"name\" : \"Bowie\",\n    \"species\" : \"dog\",\n    \"age\" : 6,\n    \"likes\" : [\"biscuits\", \"naps\", \"walks\"],\n    \"dislikes\" : [\"the cat\", \"getting groomed\"]    \n}\nLet’s define a variable in our JS console. Open up your JS console and type:\npet = {\n    \"name\" : \"Bowie\",\n    \"species\" : \"dog\",\n    \"age\" : 6,\n    \"likes\" : [\"biscuits\", \"naps\", \"walks\"],\n    \"dislikes\" : [\"the cat\", \"getting groomed\"]\n};\nNow try doing things like pet.likes and hit enter."
  },
  {
    "objectID": "intro_html.html#example-of-using-a-javascript-library",
    "href": "intro_html.html#example-of-using-a-javascript-library",
    "title": "4  HTML, CSS and javascript",
    "section": "4.5 Example of using a javascript library",
    "text": "4.5 Example of using a javascript library\nLet’s use a javscript library to plot some data. Unless you need really fine control over the javascript elements, we usually do this by calling a python or R api to the JS library. But, it’s useful to do once to see how the library does things.\nHere is some data that we’ll use a lot. It is brain regions of interest (ROIs) by the percentage of the brain that they make up.\n               roi       comp\n0              CSF   7.370845\n1   Diencephalon_L   0.756288\n2   Diencephalon_R   0.763409\n3    Mesencephalon   0.864718\n4    Metencephalon  12.488275\n5   Myelencephalon   0.378464\n6  Telencephalon_L  42.030477\n7  Telencephalon_R  42.718368\nLet’s use Vegalite. This package creates the plot via a JSON object that contains all of the data and instructions.\n{\n    \"data\" : {\n        \"values\" : [\n{\"roi\" : \"CSF\"            , \"comp\" :  7.370845},\n{\"roi\" : \"Diencephalon_L\" , \"comp\" :  0.756288},\n{\"roi\" : \"Diencephalon_R\" , \"comp\" :  0.763409},\n{\"roi\" : \"Mesencephalon\"  , \"comp\" :  0.864718},\n{\"roi\" : \"Metencephalon\"  , \"comp\" : 12.488275},\n{\"roi\" : \"Myelencephalon\" , \"comp\" :  0.378464},\n{\"roi\" : \"Telencephalon_L\", \"comp\" : 42.030477},\n{\"roi\" : \"Telencephalon_R\", \"comp\" : 42.718368}\n        ]\n    },\n    },\n   \"mark\": \"bar\",\n   \"encoding\": {\n    \"y\": {\"field\": \"roi\" , \"type\": \"nominal\"},\n    \"x\": {\"field\": \"comp\", \"type\": \"quantitative\"}\n}\nThis needs to be embedded into html, plus the vega JS libraries loaded to execute. I have an example here. The output looks like this\n\n\n\nGraphic\n\n\nTypically, one creates these graphics in one’s home analysis language (like python or R). There are several libraries for doing as such. Some of the popular ones include: bookeh, vega, D3js, leaflet, but there are many more. There’s also connections to large private efforts including tableau, power bi, google charts and plotly."
  },
  {
    "objectID": "graphics_advanced_interactive.html",
    "href": "graphics_advanced_interactive.html",
    "title": "18  Advanced interactive graphics: D3",
    "section": "",
    "text": "In your other DS courses, you’ve learned how to create static graphics uses R, ggplot, matplotlib, seaborn … You’ve probably also learned how to create client side interactive graphics using libraries like plotly and maybe also learned client-server interactivity with shiny, dash …\nIn this section we’re going to dig deeper into client side graphics, which are almost always done via html, css, javascript and a javascript plotting library. We’re going to focus on d3.js, a well known javascript library for creating interactive data visulalizations.\nTools like d3 are mostly for creating professional data web graphics. So, most of our daily graphics use will just use python/R/julia/matlab … or plotting libraries like plotly. Usually, you want to prototype graphics outside of d3. Here, we’ll give you a smidge of using d3 to get you started if your goal is to become a graphics expert."
  },
  {
    "objectID": "graphics_advanced_interactive.html#introduction-to-d3",
    "href": "graphics_advanced_interactive.html#introduction-to-d3",
    "title": "18  Advanced interactive graphics: D3",
    "section": "18.1 Introduction to D3",
    "text": "18.1 Introduction to D3\nLet’s get started. I’m going to assume that you have a basic knowledge of html, css and a little bit of javascript. D3 works by manipulating html elements. Let’s select every paragraph element in a document.\n<!DOCTYPE html>\n<html lang=\"en\">\n\n<head>\n    <script src=\"https://d3js.org/d3.v5.min.js\"></script>\n</head>\n\n<body>\n    <p> Advanced </p>\n    <p> Data science </p> \n        <script>\n            let pselect = d3.selectAll(\"p\")\n            //let pselect = d3.select(\"p\").style(\"color\", \"green\");\n            //let pselect = d3.selectAll(\"p\").style(\"color\", \"green\");\n        </script>\n    </body>\n</html>\nGoing forward, we’ll omit most of the html commands.\n\nThe command <script src=\"https://d3js.org/d3.v5.min.js\"></script> loads d3 from a CDN. You could also download it locally if you’d like.\nThe script let pselect = d3.selectAll(\"p\").style(\"color\", \"green\"); creates a variable pselect that is all of the html paragraph elements\nTry doing this, loading the web page, then try uncommenting each other script line in turn and refreshing\nIn chrome do Ctrl-shift-i to get the developer console and inspect the variable pselect.\nNesting select or selectAll will select elements within the selected elements.\nYou can also select by id or class."
  },
  {
    "objectID": "graphics_advanced_interactive.html#a-simple-example",
    "href": "graphics_advanced_interactive.html#a-simple-example",
    "title": "18  Advanced interactive graphics: D3",
    "section": "18.2 A simple example",
    "text": "18.2 A simple example\nLet’s go through an example where we plot brain volumetric ROI data on the log scale using D3.\n<style>\n    .bar {\n        background: #f5b634;\n        border: 4px solid #0769ad;\n        height: 20px;\n    }\n</style>\n<body>\n        <script>\n            let roiData = [\n                {\"roi\": \"Telencephalon_L\", \"volume\" : 531111},\n                {\"roi\": \"Telencephalon_R\", \"volume\" : 543404},\n                {\"roi\": \"Diencephalon_L\",  \"volume\" : 9683  },\n                {\"roi\": \"Diencephalon_R\",  \"volume\" : 9678  },\n                {\"roi\": \"Mesencephalon\",   \"volume\" : 10268 },\n                {\"roi\": \"Metencephalon\",   \"volume\" : 159402},\n                {\"roi\": \"Myelencephalon\",  \"volume\" : 4973  },\n                {\"roi\": \"CSF\",             \"volume\" : 109776}\n            ];\n    \n            let divSelection = d3.select(\"body\") \n                    .selectAll(\"div\")\n                    .data(roiData)\n                    .enter()\n                    .append('div')\n                    .attr(\"class\", \"bar\")\n                    .style(\"width\", (d) => {return Math.log(d.volume) * 20 + \"px\"; })\n                    .text(d => d.roi)\n                    .on(\"mouseover\", function(){\n                        d3.select(this)\n                        .style(\"background-color\", \"orange\");\n                    })\n                    .on(\"mouseout\", function(){\n                        d3.select(this)\n                        .style(\"background-color\",\"#33A2FF\" )\n                    })        </script>\n    </body>\n\nThe data(roiDat) selects our dataset\nThe enter() and append('div') commands add div elements to the html document, one per data element.\nThe attr method considers our bar stylesheet style\nThe style method changes the style so that the bars have the width of our data. The notation (d) => {return d.volume * .001 + \"px\"} is a function that selects the ROI element of the data, multiplies it by .001 then converts it to text with px at the end.\nThe text method at the end appends the text to our plot\nThe on methods say what to do when one mouses over and off the bars. You can see now that they turn orange then back. Remove the mouseout .on call and see what happens.\n\nThe output looks like this. Hover over a bar to test. (Look at the file in d3/roi1.html)"
  },
  {
    "objectID": "graphics_advanced_interactive.html#working-through-a-realistic-example",
    "href": "graphics_advanced_interactive.html#working-through-a-realistic-example",
    "title": "18  Advanced interactive graphics: D3",
    "section": "18.3 Working through a realistic example",
    "text": "18.3 Working through a realistic example\nUnder assets/kirby_pivot.csv is a dataset with the kirby 21 data pivoted to have regions as columns. Let’s work through a d3 example of ploting right versus left asymmetry in the telencephalon (the largest area of the brain including the cortex and central white matter).\nHere’s the scatterplot that I’ve got so far. For HW, add text labels to the point, or a tooltip that gives point information when you hover over it.\n\nThe code for the plot is in d3/roi2.html. Let’s go over some of the main parts of the d3 code here. First, we set up the graphic\nconst h = 500\nconst w = 500\n\n// create the background\nlet svg = d3.select(\"body\")\n    .append(\"svg\")\n    .attr(\"width\" , h)\n    .attr(\"height\", w);\nNext we load in the data. First, we create a function that does a little row processing for us. Honestly, it’s probably better to just do this in python/R/julia … beforehand, but it’s worth showing here. We create variables for the log ratio between the right and left hemispheres and the log of the geometric mean. We’ll use this to create a Tukey mean/difference plot of the log of the volumes.\n//create the variables we're interested in\nlet rowConverter = function(d) {\n    return {\n        id : d.id,\n        //y is going to be the log difference R-L\n        logratio : Math.log(parseFloat(d.Telencephalon_R)) - Math.log(parseFloat(d.Telencephalon_L)),\n        //x is going to be the average log \n        loggm : (Math.log(parseFloat(d.Telencephalon_L)) + Math.log(parseFloat(d.Telencephalon_R))) * .5\n    };\n    }\n\n//the location where I'm pulling the csv from\nlet dataloc = \"https://raw.githubusercontent.com/smart-stats/advanced_ds4bio_book/main/qbook/assets/kirby_pivot.csv\"\n\n//read in the data and parse the rows \nkirby_pivot = d3.csv(dataloc, rowConverter)\nModern js uses something called ‘promises’, which alllows for asynchronous evaluation. When we read in our csv file, it gets created as a promise and not an array like we need. The result is that our plotting commands need to then be called as a method from the promise object. The reason for this is so that it only uses the data when the data is actually loaded (i.e. promise fulfilled.) So, the plotting commmands for us look like this.\nkirby_pivot.then(dat => {\n    PLOTTING COMMANDS\n})\nJust a reminder that the notation d => g(d) is JS shorthand for function(d) {return g(d);} and is used heavily in d3 coding. Now let’s fill in PLOTTING COMMANDS. First, let’s fill in some utility functions. We get the range of our x and y values to help set up our axes. d3 scales map our function values to a range we want. So let’s create scale maps for x, y and color and then also set up axes using those scales. We’ll also go ahead on plot our axes so they’re on the bottom.\nmaxx = d3.max(dat, d => d.loggm)\nminx = d3.min(dat, d => d.loggm)\nmaxy = d3.max(dat, d => d.logratio)\nminy = d3.min(dat, d => d.logratio)\n\n//fudge is the boundary otherwise points get chopped off\nlet fudge = 50\n\nlet yScale = d3.scaleLinear()\n    .domain([miny, maxy])\n    .range([h-fudge, fudge])\n\nlet pointScale = d3.scaleLinear()\n    .domain([miny, maxy])\n    .range([5, 10])\n\nlet colorScale = d3.scaleLinear()\n    .domain([miny, maxy])\n    .range([0, 1])\n\n\nlet xScale = d3.scaleLinear()\n    .domain([minx, maxx])\n    .range([w-fudge, fudge]);\n\n// define the axes\nlet xaxis = d3.axisBottom().scale(xScale)\nlet yaxis = d3.axisLeft().scale(yScale)\nsvg.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", \"translate(0,\" + (h - fudge) + \")\")\n    .call(xaxis)\n\nsvg.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", \"translate(\" + fudge + \",0)\")\n    .call(yaxis)\nNow let’s create the plot. We’re going to add circles at each location, which is attributes cx and cy. Notice we use our previous defined scales to give their locations. Also, we’ll set the color and size relative to the logratio. Finally, when we mouseover a point, let’s change the radius then change it back when we mouseoff.\nsvg.selectAll(\"circle\")\n    .data(dat)\n    .enter()\n    .append(\"circle\")\n    .attr(\"cy\", d => yScale(d.logratio))\n    .attr(\"cx\", d => xScale(d.loggm))\n    .attr(\"r\",  d => pointScale(d.logratio))\n    .attr(\"fill\", d => d3.interpolateWarm(colorScale(d.logratio)))\n    .attr(\"stroke\", \"black\")\n    .on(\"mouseover\", function() {\n        d3.select(this)\n            .attr(\"r\", 30)\n        })\n    .on(\"mouseout\", function() {\n        d3.select(this)\n        .attr(\"r\",  d => pointScale(d.logratio))\n    })\nObviously, this is a lot of work for a simple scatterplot. The difference is that here you have total control over plotting and interactivity elements."
  },
  {
    "objectID": "graphics_advanced_interactive.html#observable-and-observable-plot",
    "href": "graphics_advanced_interactive.html#observable-and-observable-plot",
    "title": "18  Advanced interactive graphics: D3",
    "section": "18.4 Observable and Observable Plot",
    "text": "18.4 Observable and Observable Plot\nObserverable is a notebook for working with d3. It’s quite neat since mixing javascript coding in a web notebook, which itself is written in javascript, makes for an interesting setup. Typically, one would do the data preprocessing in R, python, julia … then do the advanced graphing in d3. In addition to accepting d3 as inputs, observable has a slightly higher set of utility functions called observable plot. (Quarto, which this document is in, allows for observable cells.) So, let’s read in some ROI data and plot it in observable plot. Note this is the average of the Type I Level I ROIs. Notice this is much easier than using d3 directly.\n\ndata = FileAttachment(\"assets/kirby_avg.csv\").csv();\nPlot.plot({\nmarks: [Plot.barY(data, {x: \"roi\", y: \"volume\", fill : 'roi'})],\n    x: {tickRotate: 45},\n    color: {scheme: \"spectral\"},    \n    height: 400,\n    width: 400,\n    marginBottom: 100\n\n})"
  },
  {
    "objectID": "graphics_advanced_interactive.html#links",
    "href": "graphics_advanced_interactive.html#links",
    "title": "18  Advanced interactive graphics: D3",
    "section": "18.5 Links",
    "text": "18.5 Links\n\nObservable is not javascript\nd3 tutorial.\nd3 gallery"
  },
  {
    "objectID": "graphics_advanced_interactive.html#homework",
    "href": "graphics_advanced_interactive.html#homework",
    "title": "18  Advanced interactive graphics: D3",
    "section": "18.6 Homework",
    "text": "18.6 Homework\n\nCreate a D3 graphic web page that displays a scatterplot of your chosing. Show point information on hover.\nOn the same web page, create a D3 graphic web page that displays a stacked bar chart for the Kirby 21 data. Hover data should show subject information and increase the size of the bar. Here’s a plotly version to get a sense.\n\n\nimport pandas as pd\nimport plotly.express as px\nimport numpy as np\ndat = pd.read_csv(\"https://raw.githubusercontent.com/smart-stats/ds4bio_book/main/book/assetts/kirby21.csv\").drop(['Unnamed: 0'], axis = 1)\ndat = dat.assign(id_char = dat.id.astype(str))\nfig = px.bar(dat, x = \"id_char\", y = \"volume\", color = \"roi\")\nfig.show()\n\n\n                                                \n\n\n\nSubmit your webpages and all supporting code to your assignment repo\nHere’s a hint to the HW in d3/hwHint.html"
  },
  {
    "objectID": "graphics_interactive.html",
    "href": "graphics_interactive.html",
    "title": "17  Interactive graphics",
    "section": "",
    "text": "Javascript graphics allows one to put data oriented graphics into web documents (like this book), apps and other reproducible research documents. As mentioned, several well developed APIs have been developed to use Python, R … as the base language where graphics are output as javascript. Here, we’ll go through some examples using plotly, both because it’s a nice library of graphics functions, but also it’s what I know sort of well. However, if there’s another graphics platform you like, likely there’s a python and/or R API written for it."
  },
  {
    "objectID": "graphics_interactive.html#using-plotly",
    "href": "graphics_interactive.html#using-plotly",
    "title": "17  Interactive graphics",
    "section": "17.1 Using plotly",
    "text": "17.1 Using plotly\nConsider a dataset that has regional volumes for 20 subjects in a long dataset. I wrote some R code for reading in this dataset which you can follow along here.\n\nimport pandas as pd\nimport plotly.express as px\nimport numpy as np\n\n\ndat = pd.read_csv(\"assets/kirby21.csv\").drop(['Unnamed: 0'], axis = 1)\ndat.head()\n\n\n\n\n\n  \n    \n      \n      id\n      roi\n      volume\n    \n  \n  \n    \n      0\n      127\n      Telencephalon_L\n      531111\n    \n    \n      1\n      127\n      Telencephalon_R\n      543404\n    \n    \n      2\n      127\n      Diencephalon_L\n      9683\n    \n    \n      3\n      127\n      Diencephalon_R\n      9678\n    \n    \n      4\n      127\n      Mesencephalon\n      10268\n    \n  \n\n\n\n\nLet’s vew individual subjects. The id variable is a numeric variable, so let’s create a string version.\n\ndat = dat.assign(id_char = dat.id.astype(str))\nfig = px.bar(dat, x = \"id_char\", y = \"volume\", color = \"roi\")\nfig.show()\n\n\n                                                \n\n\nLet’s add an intra-cranial volume column by grouping by id, summing all volumes, then merging that data back into the main data. We’ll then add a composition variable, which is the regional volumes divided by the intra-cranial volume.\n\nicv = dat.groupby(['id']).volume.sum().reset_index().rename(columns = {'volume' : 'icv'})\ndat = pd.merge(dat, icv, on = 'id')\ndat = dat.assign(comp = dat.volume / dat.icv)\ndat.head()\n\n\n\n\n\n  \n    \n      \n      id\n      roi\n      volume\n      id_char\n      icv\n      comp\n    \n  \n  \n    \n      0\n      127\n      Telencephalon_L\n      531111\n      127\n      1378295\n      0.385339\n    \n    \n      1\n      127\n      Telencephalon_R\n      543404\n      127\n      1378295\n      0.394258\n    \n    \n      2\n      127\n      Diencephalon_L\n      9683\n      127\n      1378295\n      0.007025\n    \n    \n      3\n      127\n      Diencephalon_R\n      9678\n      127\n      1378295\n      0.007022\n    \n    \n      4\n      127\n      Mesencephalon\n      10268\n      127\n      1378295\n      0.007450\n    \n  \n\n\n\n\nLet’s now replot our compositional data (but now normalized to have height 1).\n\nfig = px.bar(dat, x = \"id_char\", y = \"comp\", color = \"roi\")\nfig.show()\n\n\n                                                \n\n\nLet’s look at the subject level means. Therefore, we have to average across id.\n\nroi_mean = dat.drop([\"id\", \"id_char\", \"icv\"], axis = 1).groupby([\"roi\"]).mean().reset_index()\nfig = px.bar(roi_mean, x = \"roi\", y = \"comp\")\nfig.show()\n\n\n                                                \n\n\nThere’s a hierarchy of regions in this dataset. Let’s visualize a subject’s type 1 level 5 data as it exists in the hierarchy. First, let’s load in the hierarchy information:\n\nurl = \"https://raw.githubusercontent.com/bcaffo/MRIcloudT1volumetrics/master/inst/extdata/multilevel_lookup_table.txt\"\nmultilevel_lookup = pd.read_csv(url, sep = \"\\t\").drop(['Level5'], axis = 1)\nmultilevel_lookup = multilevel_lookup.rename(columns = {\n    \"modify\"   : \"roi\", \n    \"modify.1\" : \"level4\",\n    \"modify.2\" : \"level3\", \n    \"modify.3\" : \"level2\",\n    \"modify.4\" : \"level1\"})\nmultilevel_lookup = multilevel_lookup[['roi', 'level4', 'level3', 'level2', 'level1']]\nmultilevel_lookup.head()\n\n\n\n\n\n  \n    \n      \n      roi\n      level4\n      level3\n      level2\n      level1\n    \n  \n  \n    \n      0\n      SFG_L\n      SFG_L\n      Frontal_L\n      CerebralCortex_L\n      Telencephalon_L\n    \n    \n      1\n      SFG_R\n      SFG_R\n      Frontal_R\n      CerebralCortex_R\n      Telencephalon_R\n    \n    \n      2\n      SFG_PFC_L\n      SFG_L\n      Frontal_L\n      CerebralCortex_L\n      Telencephalon_L\n    \n    \n      3\n      SFG_PFC_R\n      SFG_R\n      Frontal_R\n      CerebralCortex_R\n      Telencephalon_R\n    \n    \n      4\n      SFG_pole_L\n      SFG_L\n      Frontal_L\n      CerebralCortex_L\n      Telencephalon_L\n    \n  \n\n\n\n\nNow load in the subject data and merge it with the hierarchy data.\n\nid = 127\nsubjectData = pd.read_csv(\"assets/kirby21AllLevels.csv\")\nsubjectData = subjectData.loc[(subjectData.type == 1) & (subjectData.level == 5) & (subjectData.id == id)]\nsubjectData = subjectData[['roi', 'volume']]\n## Merge the subject data with the multilevel data\nsubjectData = pd.merge(subjectData, multilevel_lookup, on = \"roi\")\nsubjectData = subjectData.assign(icv = \"ICV\")\nsubjectData = subjectData.assign(comp = subjectData.volume / np.sum(subjectData.volume))\nsubjectData.head()\n\n\n\n\n\n  \n    \n      \n      roi\n      volume\n      level4\n      level3\n      level2\n      level1\n      icv\n      comp\n    \n  \n  \n    \n      0\n      SFG_L\n      12926\n      SFG_L\n      Frontal_L\n      CerebralCortex_L\n      Telencephalon_L\n      ICV\n      0.009350\n    \n    \n      1\n      SFG_R\n      10050\n      SFG_R\n      Frontal_R\n      CerebralCortex_R\n      Telencephalon_R\n      ICV\n      0.007270\n    \n    \n      2\n      SFG_PFC_L\n      12783\n      SFG_L\n      Frontal_L\n      CerebralCortex_L\n      Telencephalon_L\n      ICV\n      0.009247\n    \n    \n      3\n      SFG_PFC_R\n      11507\n      SFG_R\n      Frontal_R\n      CerebralCortex_R\n      Telencephalon_R\n      ICV\n      0.008324\n    \n    \n      4\n      SFG_pole_L\n      3078\n      SFG_L\n      Frontal_L\n      CerebralCortex_L\n      Telencephalon_L\n      ICV\n      0.002227\n    \n  \n\n\n\n\nHere’s a sunburst plot of a subject’s brain volumetrics.\n\nfig = px.sunburst(subjectData, path=['icv', 'level1', 'level2', 'level3', 'level4', 'roi'], \n                  values='comp', width=800, height=800)\nfig.show()\n\n\n                                                \n\n\nSimilarly, we can make a treemap.\n\nfig = px.treemap(subjectData, \n                 path = ['icv', 'level1', 'level2', 'level3', 'level4', 'roi'], \n                 values='comp',\n                 color='comp', \n                 color_continuous_scale = 'RdBu',\n                 color_continuous_midpoint = .005,\n                 width=800, height=800\n                )\nfig.show()"
  },
  {
    "objectID": "graphics_interactive.html#interactive-maps-using-folium-and-leaflet",
    "href": "graphics_interactive.html#interactive-maps-using-folium-and-leaflet",
    "title": "17  Interactive graphics",
    "section": "17.2 Interactive maps using folium and leaflet",
    "text": "17.2 Interactive maps using folium and leaflet\nA common form of interactive graphic is a map. There are several mapping libraries for python, including some in plotly. folium is another option that connects to the well known leaflet javascript library. Let’s create a quick plot of the Bloomberg School of Public Health Building, which is at latitude and longitude 39.298, -76.590. If you haven’t already, pip or conda install folium.\n\nimport folium \n\nm = folium.Map(location = [39.298, -76.590], zoom_start = 15)\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\nYou can then add elements to the map. For example, suppose we want a marker on the building saying “my office”. It’s just that easy! This is truly just the tip of the iceberg of using folium/leaflet.\n\nfolium.Marker([39.298, -76.590], popup = \"What it says when you click\",  tooltip = \"What it says when you hover\").add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "graphics_theory.html",
    "href": "graphics_theory.html",
    "title": "18  Advanced: theory of graphical display",
    "section": "",
    "text": "One of the main design arguments for the graphical display of information is data / ink maximization (tufte1990data?). This is the idea that idea that as much of the “ink” (non-background pixels) of the plot as possible should be displaying data.\nData/ink maximalization has been criticized empirically. For example, (inbar2007minimalism?) conducted a study with 87 undergraduates and found a clear preference for the non-maximized variations. Another line of argument discusses the “paradox of simplicity” (norman2007simplicity?), (eytam2017paradox?), whereby we have a strong aesthetic preference for simplicity, but also want flexibility and maximum utility.\n\n(bertin1983semiology?)\n\n\n\n\n(wickham2010graphical?)\n\n\n\n\n\n(cleveland1987research?)\n(cleveland1984many?)\n(cleveland1980calendar?)\n(carswell1992choosing?)\n(cleveland1986experiment?)\nMagical thinking (diaconis2006theories?)"
  },
  {
    "objectID": "graphics_theory.html#implementation",
    "href": "graphics_theory.html#implementation",
    "title": "18  Advanced: theory of graphical display",
    "section": "18.2 Implementation",
    "text": "18.2 Implementation\n\n18.2.1 Grammar of graphics\n\n(wilkinson2012grammar?)\n(wilkinson2013grammar?)\n(wickham2010layered?)\n\n\n\n18.2.2 Narative storytelling\nEdward and Jeffrey ((segel2010narrative?)) argue regarding the use of modern interactive tools in data narrative storytelling. They give seven canonical genres of narrative visulation."
  },
  {
    "objectID": "graphics_theory.html#graph-galleries-and-further-reading",
    "href": "graphics_theory.html#graph-galleries-and-further-reading",
    "title": "18  Advanced: theory of graphical display",
    "section": "18.3 Graph galleries and further reading",
    "text": "18.3 Graph galleries and further reading\n\n18.3.1 Further reading\n\nKarl Broman on How to display data badly\nKarl Broman Data Vizualization\nKarl Broman 10 worst plots\nKarl Broman Data visualization\n\n\n\n18.3.2 Graph galleries\n\nR graph gallery\nMatplotlib graph gallery\nPlotly\nD3 gallery\nVega gallery\nSeaborn\n\n\n\n18.3.3 Historically famous graphics\n\nhttps://medium.com/stotle-inc/the-greatest-graph-in-history-1155e0c25671Z\nhttps://plotlygraphs.medium.com/seven-modern-remakes-of-the-most-famous-graphs-ever-made-8ef30da1ab00\nhttps://www.datavis.ca/gallery/historical.php\nhttps://towardsdatascience.com/a-short-history-of-data-visualisation-de2f81ed0b23\nhttps://www.tableau.com/learn/articles/best-beautiful-data-visualization-examples\n\n\n\n18.3.4 Infographics in the media\n\nhttps://www.nytimes.com/spotlight/graphics\nJHU covid map"
  },
  {
    "objectID": "graphics_eda.html",
    "href": "graphics_eda.html",
    "title": "15  Exploratory data analysis example",
    "section": "",
    "text": "A picture is worth a 1,000 words\nOr saying how impactful intrer-ocular content is (i.e. when information hits you right between the eyes).\nI’m using Seaborn as the framework. There’s several plotting frameworks in python, but I find that seaborn has the nicest default plotting options. Also, it’s built on top of matplotlib, which is the main plotting library for DS for python.\nLet’s start with loading up some libraries.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()\n\nThe command sns.set sets the seaborn style. This sets the style for all matplotlib plots, even if not created in seaborn. I like the seaborn style, so I usually set it this way.\nFirst let’s download the data. Then we’ll read it in and drop some columns that aren’t needed for this analysis.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/kirby21.csv\")\ndf = df.drop(['Unnamed: 0', 'rawid', 'min', 'max', 'mean', 'std'], axis = 1)\ndf.head(4)\n\n\n\n\n\n  \n    \n      \n      roi\n      volume\n      type\n      level\n      id\n      icv\n      tbv\n    \n  \n  \n    \n      0\n      Telencephalon_L\n      531111\n      1\n      1\n      127\n      1378295\n      1268519\n    \n    \n      1\n      Telencephalon_R\n      543404\n      1\n      1\n      127\n      1378295\n      1268519\n    \n    \n      2\n      Diencephalon_L\n      9683\n      1\n      1\n      127\n      1378295\n      1268519\n    \n    \n      3\n      Diencephalon_R\n      9678\n      1\n      1\n      127\n      1378295\n      1268519\n    \n  \n\n\n\n\nLet’s look at the Type 1 Level 1 data and create a variable called comp which is brain composition, defined as the regional volumes over total brain volume. We’ll do this by selecting roi and comp then grouping by roi (region of interest) and taking the mean of the compostions.\n\n## Extract the Type 1 Level 1 data\nt1l1 = df.loc[(df['type'] == 1) & (df['level'] == 1)]\n\n\n## create a composition variable\nt1l1 = t1l1.assign(comp = t1l1['volume'] / t1l1['tbv'])\nt1l1 = t1l1.loc[t1l1['roi'] != 'CSF']\n\nLet’s get the mean of the composition variable across subjects by ROI. This is done by grouping by ROI then averaging over composition.\n\nsummary = t1l1[['roi', 'comp']].groupby('roi', as_index=False).mean()\nprint(summary)\n\n               roi      comp\n0   Diencephalon_L  0.007563\n1   Diencephalon_R  0.007634\n2    Mesencephalon  0.008647\n3    Metencephalon  0.124883\n4   Myelencephalon  0.003785\n5  Telencephalon_L  0.420305\n6  Telencephalon_R  0.427184\n\n\nOK, let’s try our first plot, a seaborn bar plot.\n\ng = sns.barplot(x='roi', y = 'comp', data = summary);\n## this is the matplotlib command for rotating \n## axis tick labels by 90 degrees.\nplt.xticks(rotation = 90);\n\n\n\n\nUnfortunately, seaborn doesn’t have a stakced bar chart. However, pandas does have one built in. To do this, however, we have to create a version of the data with ROIs as the columns. This can be done with a pivot statement. This converts our data from a “long” format to a “wide” format.\n\nt1l1pivot = t1l1.pivot(index = 'id', columns = 'roi', values = 'volume')\nt1l1pivot.head(4)\n\n\n\n\n\n  \n    \n      roi\n      Diencephalon_L\n      Diencephalon_R\n      Mesencephalon\n      Metencephalon\n      Myelencephalon\n      Telencephalon_L\n      Telencephalon_R\n    \n    \n      id\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      127\n      9683\n      9678\n      10268\n      159402\n      4973\n      531111\n      543404\n    \n    \n      142\n      9156\n      9071\n      10516\n      165803\n      4912\n      545603\n      552216\n    \n    \n      239\n      8937\n      9004\n      9070\n      124504\n      4023\n      483107\n      490805\n    \n    \n      346\n      8828\n      8933\n      9788\n      135090\n      4428\n      558849\n      568830\n    \n  \n\n\n\n\nNow that the data is in the right format, we can do our plot.\n\nt1l1pivot.plot(kind='bar', stacked=True, legend= False);\nplt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5));\n\n\n\n\nLet’s do some scatterplots. Let’s look at bilateral symmetry of the telencephalon. That is, let’s plot the right telencephalon versus the left telencephalon.\n\nsns.scatterplot(x = 'Telencephalon_L', y = 'Telencephalon_R', data = t1l1pivot);\nplt.xticks(rotation = 90);\n#plot an identity line from the data min to the data max\nx1 = min([t1l1pivot.Telencephalon_L.min(), t1l1pivot.Telencephalon_R.min()])\nx2 = max([t1l1pivot.Telencephalon_L.max(), t1l1pivot.Telencephalon_R.max()])\nplt.plot([x1, x2], [x1 , x2]);\n\n\n\n\nThis plot has the issue that there’s a lot of blank space. This is often addressed via a mean difference plot. This plot shows (X+Y) / 2 versus (X-y). This is basically just rotating the plot above by 45 degrees to get rid of all of the blank space around the diagonal line. Alternatively, you could plot (log(x) + log(y)) / 2 versus log(X) - log(Y). This plots the log of the geometric mean of the two observations versus the log of their ratio. Sometimes people use log base 2 or log base 10.\n\nt1l1pivot = t1l1pivot.assign(Tel_logmean = lambda x: (np.log(x.Telencephalon_L) * .5 +  np.log(x.Telencephalon_R)* .5))\nt1l1pivot = t1l1pivot.assign(Tel_logdiff = lambda x: (np.log(x.Telencephalon_R) -  np.log(x.Telencephalon_L)))\nsns.scatterplot(x = 'Tel_logmean', y = 'Tel_logdiff', data = t1l1pivot);\nplt.axhline(0, color='green');\nplt.xticks(rotation = 90);\n\n\n\n\nThus, apparently, the right side is always a little bigger than the left and the scale of the ratio is \\(e^{0.02}\\) while the scale of the geometric mean is \\(e^{13}\\). Note, \\(\\exp(x) \\approx 1 + x\\) for \\(x \\approx 0\\). So it’s about 2% larger. A note about right versus left in imaging. Often the labels get switched as there are different conventions (is it the right of the subject or the right of the viewer when looking straight at the subject?). Typically, it’s known that some of the areas of subject’s left hemisphere are larger and so it’s probably radiological (right of the viewer) convention here. Here’s a nicely done article about right versus left brain.\n(Also, in case you don’t believe me, here’s a plot of \\(e^x\\) versus \\(1+x\\) for values up to 0.1. This is the so-called Taylor expasion for \\(e^x\\) around 0. Notice the approximation gets worse, the curves diverge, as you get further away from 0.)\n\n## A sequence of numbers from 0 to .1 spaced by 0.001\nx = np.arange(0, .1, .001)\nex = np.exp(x)\n\nsns.lineplot(x = x, y = ex)\nplt.plot(x, x + 1)"
  },
  {
    "objectID": "graphics_images.html",
    "href": "graphics_images.html",
    "title": "19  Working with images",
    "section": "",
    "text": "Images broadly come in two types, vector and raster. Vector graphics are in formats like pdf, eps, svg and raster graphics are like jpeg, gif, png. Vector graphics store the image constructs and shapes. So, a vector graphics renderer can zoom in indefinitely on a shape and its edges will appear sharp. Vector fonts work this way. Raster graphics basically store a matrix and the pixels on the screen show the values of that matrix. Bitmapped fonts work this way. Of course, vector graphics have to be converted to raster to be actually displayed by the computer. Finally, some rater graphics formats have compression, which we won’t really discuss."
  },
  {
    "objectID": "graphics_images.html#working-with-raster-graphics",
    "href": "graphics_images.html#working-with-raster-graphics",
    "title": "19  Working with images",
    "section": "19.2 Working with raster graphics",
    "text": "19.2 Working with raster graphics\nRaster images are typically stored as an array. Grayscale images are matrices with the image intensity as the value and color pictures are stored as 3D arrays with the two main dimensions and color channels. A library for working with regular images in python is called PIL.\nThere are different raster specifications. RGB has 3 color channels, red, green and blue. CMYK has four: cyan, magenta, yellow and black. It’s interesting to note that the use of color channels existed before color cameras, when photographers would use different filters and additive and subtractive processes. The photograph below was created in 1877 by Louis Ducos du Hauron.\n\n\n\nColor image\n\n\nReading and working with images in python is quite easy because of the Python Image Library (PIL).\n\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nimg = Image.open(\"assets/images/Duhauron1877.jpg\")\n## You can see it with this, or img.show()\nimg\n\n\n\n\nPIL objects come with a ton of methods. For example, if we want to know whether we have an RGB or CMYK image, just print its mode.\n\nprint(img.mode)\n\nRGB\n\n\n\nr, g, b = img.split()\n\nplt.figure(figsize=(10,4));\nplt.subplot(1, 3, 1);\nplt.axis('off');\nplt.imshow(r);\n\nplt.subplot(1, 3, 2);\nplt.axis('off');\nplt.imshow(g);\n\nplt.subplot(1, 3, 3);\nplt.axis('off');\nplt.imshow(b);\n\n\n\n\nIf you’re tired of working with the image as a PIL object, it’s easy to convert to a np array.\n\nimg_array = np.array(img)\nimg_array.shape\n\n(1132, 1548, 3)\n\n\nBefore we leave PIL, it should be said that most image operations can be done in it. For example, cropping.\n\nbbox = [500, 630, 700, 760]\ncropped = img.crop(bbox)\ncropped\n\n\n\n\nWe can rotate the house and put it back\n\nrot = cropped.transpose(Image.Transpose.ROTATE_180)\nrot\n\n\n\n\n\n##Note this overwrites the image\nimg.paste(rot, bbox)\nimg"
  },
  {
    "objectID": "graphics_images.html#image-mathematics",
    "href": "graphics_images.html#image-mathematics",
    "title": "19  Working with images",
    "section": "19.3 Image mathematics",
    "text": "19.3 Image mathematics\n\n19.3.1 Convolutions\n\n19.3.1.1 1D transforms\nConvolutions are an important topic in mathematics, statistics, signal processing … Let’s discuss 1D convolutions first. A real valued convolution of two continuous signals, \\(X(t)\\) and \\(K(t)\\) is defined as \\(X* K\\)\n\\[\n(X* K)(t) = \\int_{-\\infty}^{\\infty} X(u) K(t-u) du\n= \\int_{-\\infty}^{\\infty} X(t-v) K(v) dv,\n\\]\nwhere the equality is determined by a simple change of variable argument. The discrete analog is\n\\[\n(X* K)(t) = \\sum_{u = -\\infty}^{\\infty} X(u) K(t-u)\n= \\sum_{v = -\\infty}^{\\infty} X(t-v) K(v)\n\\]\nThe convolution has many, many uses in data science and statistics. For example, the convolution of densities or mass functions is the respective density or mass function for the sum of random variables from those distributions. In applied data analysis, you can think of the convolution between \\(X\\) and \\(K\\) as smearing the function \\(K\\) over the function \\(X\\). Thus, it plays a key role in smoothing. Let’s try an example using the covid data and a box kernel. We take \\(K(t) = I\\{0 \\leq t < M\\} / M\\) (i.e. is 1 for times 0 to \\(M-1\\), then rescaled so it sums to 1). Assume that \\(N\\geq M\\) and that \\(X(t)\\) and \\(K(t)\\) are \\(0\\) and for \\(t < 0\\) or \\(t > N\\). Then, our convolution works out to be\n\\[\n(X* K)(t)\n= \\sum_{u = -\\infty}^{\\infty} X(u) K(t-u)\n= \\sum_{u = 0}^{N} X(u) K(t-u)\n= \\sum_{u = t}^{t + M - 1} X(u) K(t -u)\n= \\sum_{u = t}^{t + M - 1} X(u) / M\n\\]\nThat is, our convolution is a moving average of \\(X\\) where the convolution at point \\(t\\) is the average of the points between \\(t\\) and \\(t + M - 1\\). So, the convolution, as we’ve defined it, at point \\(t\\) is the moving average at point \\(t + (M-1)/2\\) (ie. it’s shifted by \\((M-1)/2\\)). Also, at the end (\\(t \\geq N - M + 1\\)), we’re averaging in the assumed zero values of the \\(X\\). This might be reasonable to do, or maybe not. The fact that we’re padding the end and not the beginning is just because of the range of index values we defined the kernel on. We’d have the same problem only on the other end if \\(K(t) = I(-M < t \\leq 0)/M\\). Of course, the computer will start summing things at index 0 regardless. However, it can shift the kernel relative to the signal arbitrarily by zero padding one end or the other or both. A reasonable strategy is to set it so that it averages in \\((M-1)/2\\) on both ends. Numpy allows you to only look at the range of \\(N - M\\) middle values where this isn’t an issue (argument mode = \"valid\").\nNote we could make the kernel weight points differently than just a box kernel. A popular choice is a Gaussian distribution.\nAlso, the convolution has \\(N+M-1\\) points. So, it has more time points than the original signal. Numpy has options to shift the convolution back into the same space as the original signal for you (i.e. has \\(N\\) points, mode = \"same\"). Or, you can just do it yourself if you do mode = \"full\", just shift by \\((M-1)/2\\). Similarly shift for mode = \"valid\" (but the convolution has fewer points in this case, so it won’t have corresponding points with \\(X\\) at the very beginning and end).\nHere’s an example using Italy’s daily covid case count data. We plot the data and the convolution smoothed data. In the bottom panels, we show the residuals to highlight the difference.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\ndat = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n## Get Italy, drop everyrthing except dates, convert to long (unstack converts to tuple)\nX = dat[dat['Country/Region'] == 'Italy'].drop([\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"], axis=1).unstack()\n## convert from tuple to array\nX = np.asarray(X)  \n## get case counts instead of cumulative counts\nX = X[1 : X.size] - X[0 : (X.size - 1)]\n## get the first non zero entry\nX =  X[np.min(np.where(X !=  0)) : X.size]\nplt.plot(X)\n\n\n\n\nNow let’s plot the convolutions with different options in np.convolve.\n\n## 41 day moving average\nN = len(X)\nM = 41\n\nfig, axes = plt.subplots(2, 3, figsize = [12.4, 12.4])\naxes[0,0].plot(X)\naxes[0,1].plot(X)\naxes[0,2].plot(X)\n\nK = np.ones(M) / M\n\n## Plot the convolution with the argument 'same'\n## this gives N (assumed greater than M) points\nXC = np.convolve(X, K, 'same')\naxes[0,0].plot(XC)\naxes[1,0].plot(X - XC)\n\n## Plot the convolution with the argument 'full'\n## which gives N+M-1 total pionts\nXC = np.convolve(X, K, 'full')\ntemp = np.pad(X, (M-1, 0), 'constant') \naxes[0,1].plot(XC)\naxes[1,1].plot(temp- XC)\n\n\n## Plot the convolution with the convolution shifted back by (M-1)/2\nXCshifted = XC[ (int((M - 1)/2)) : int(len(XC) - (M - 1)/2) ]\naxes[0, 2].plot(XCshifted)\naxes[1, 2].plot(X - XCshifted)\n## 41 day moving average\nN = len(X)\nM = 41\n\nfig, axes = plt.subplots(2, 3, figsize = [12.4, 12.4])\naxes[0,0].plot(X)\naxes[0,1].plot(X)\naxes[0,2].plot(X)\n\nK = np.ones(M) / M\n\n## Plot the convolution with the argument 'same'\n## this gives N (assumed greater than M) points\nXC = np.convolve(X, K, 'same')\naxes[0,0].plot(XC)\naxes[1,0].plot(X - XC)\n\n## Plot the convolution with the argument 'full'\n## which gives N+M-1 total pionts\nXC = np.convolve(X, K, 'full')\ntemp = np.pad(X, (M-1, 0), 'constant') \naxes[0,1].plot(XC)\naxes[1,1].plot(temp- XC)\n\n\n## Plot the convolution with the convolution shifted back by (M-1)/2\nXCshifted = XC[ (int((M - 1)/2)) : int(len(XC) - (M - 1)/2) ]\naxes[0, 2].plot(XCshifted)\naxes[1, 2].plot(X - XCshifted)\n\n\n\n\n\n\n\nLet’s show that the first point and end point of the convolution are the averages of \\((M-1)/2\\) points and and \\((M-1)/2+1\\) zeros at the beginning or end of the original signal just to show that our intuition is correct.\n\ntemp = np.convolve(X, K, 'same')\n[\n  # the first convolution point (temp[0]) and the average of the\n  # the first (M-1) / 2 X points and (M-1)/2 + 1 zeros\n  [temp[0],     X[0 : int(    (M - 1) / 2)].sum() / M],\n  # the last convolution point (temp[N-1]) and the average of the\n  # the last (M-1) / 2 X points and (M-1)/2 + 1 zeros\n  [temp[N - 1], X[int(N - (M - 1) / 2 - 1)  : N].sum() / M]\n \n]\n\n[[0.07317073170731708, 0.07317073170731707],\n [2163.0731707317073, 2163.0731707317073]]\n\n\nAlso, I averaged a lot (41 days) in order to make the shift very apparent. Let’s look at the performance for less wide of a kernel.\n\n## 21 day moving average\nM = 21\nK = np.ones(M) / M\n\nfig, axes = plt.subplots(1, 2, figsize = [12.4, 6.2])\nXC = np.convolve(X, K, 'same')\naxes[0].plot(X)\naxes[0].plot(XC)\naxes[1].plot(X - XC)\n\n\n\n\nIt should be stated that the convolution operation is multiplication in Fourier space. So, functions like np.convolve are performing FFTs in the background. However, if you’re going to do this yourself, make sure to keep track of indices and zero padding. (I.e. the bookkeeping.) Otherwise, the FFT wraps around and you get a little of the end averaged in with the beginning and vice versa. I work out getting the same answer as mode = “same” below.\n\nfig, axes = plt.subplots(1, 2, figsize = [12.4, 6.2])\n\n## Pad the X with zeros in the back, need at least M-1 \npad_width = (0, M - 1)\nXpadded = np.pad(X, pad_width, \"constant\")\n## Pad the kernel in the back with N-1, so both the kernel\n## and the X are of length, N+M-1\nKpadded = np.pad(K, (0, N - 1))\n\n## Note we take the real part b/c the complex part is all effectively \n## machine 0\nconvolution = np.fft.ifft(np.fft.fft(Xpadded) * np.fft.fft(Kpadded)).real\n\n## At this point the convolution is of length N + M - 1\n## To get it comparable with the original X, subtract (M-1)/2 indices\n## from each end\nconvolution = convolution[ int((M-1)/2) : int(N+(M-1)/2)]\n\n## Let's see how we did\naxes[0].plot(X)\naxes[0].plot(convolution)\n\n#Show they're the same by plotting the subtraction\naxes[1].plot(convolution - XC)\n\n\n\n\n\n\n19.3.1.2 2D transforms\nFor two dimensions, the convolution is similar\n\\[\n(X ** K)(i,j) = \\sum_{u=-\\infty}^{\\infty} \\sum_{v=-\\infty}^{\\infty}\nX(u, v)  K(i -u, k - v) = \\sum_{u=-\\infty}^{\\infty} \\sum_{v=-\\infty}^{\\infty}\nK(u, v)  X(i -u, k - v)  \n\\]\nOnce again, let’s think where \\(X\\) is of dimension \\((N_1, N_2)\\) and 0 outside of that range, and\n\\[\nK(u, v) = I(0 \\leq u < M_1, 0 \\leq v < M_2) / (M_1 M_2)\n\\]\n(i.e. \\(K\\) is a box on \\(M_1 \\leq N_1\\), \\(M_2 < N_2\\)). Then, applying the exact same argument as before, the convolution is:\n\\[\n(X ** K)(i,j) = \\sum_{u=i}^{M_1 + i - 1} \\sum_{v=j}^{M_2 + j - 1}\nX(u, v) / (M_1 M_2)\n\\]\nThat is, the convolution at point \\((i,j)\\) is the average of the neighboring points. Also, all of the same bookkeeping, zero padding and Fourier transform stuff apply (using the 2D FFT).\nFor regular kernels (box kernels, 2D Gaussians), convolution smooths the image, which has the efffect of making it blurrier. The kernel width determines how blurry the image will then be. This is typically done to denoise an image (to blur out the noise). Let’s try it on a cartoon image of Brian. We’ll just stick to a black and white image so that it’s 2D. A color image has 3 color channels, so is a 3D array. (However, you see the patten; you should be able to extend this to 3D with little problem.)\n\nimport PIL\nimport scipy.signal as sp\nimport urllib.request\n\n\nimgURL = \"https://github.com/smart-stats/ds4bio_book/raw/main/book/bcCartoon.png\"\nurllib.request.urlretrieve(imgURL, \"bcCartoon.png\")\nimg = np.asarray(PIL.Image.open(\"bcCartoon.png\").convert(\"L\"))\n\nplt.xticks([])\nplt.yticks([])\nplt.imshow(img, cmap='gray', vmin=0, vmax=255)\n\n<matplotlib.image.AxesImage at 0x7e54524b3d60>\n\n\n\n\n\nNow let’s take this image and convolve it with different kernels of different window lengths.\n\ndef kernel(i, j):\n  return np.ones((i, j)) / np.prod([i, j])\n\nplt.figure(figsize=[12.4, 12.4])\nimgC = sp.convolve2d(img, kernel(4, 4))\nplt.subplot(2, 2, 1)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"4x4\")\n\nimgC = sp.convolve2d(img, kernel(8, 8))\nplt.subplot(2, 2, 2)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"8x8\")\n\nimgC = sp.convolve2d(img, kernel(16, 16))\nplt.subplot(2, 2, 3)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"16x16\")\n\nboxsize = (5, 5)\nimgC = sp.convolve2d(img, kernel(32,32))\nplt.subplot(2, 2, 4)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"32x32\")\n\nText(0.5, 1.0, '32x32')\n\n\n\n\n\n\n\n19.3.1.3 Convolutional neural networks\nOf course, your kernel doesn’t have to be a box, or a truncated, discretized bivariate Gaussian density or even be non-negative. It’s helpful for smoothers to have non-negative kernels, since they’re just taking a generalized variation of a moving average that way. But, we want to use convolutions\nmore generally. Here, let’s take a kernel that is part of the image (left eye) and convolve it. I’ll make the kernel super peaked at eye features by extracting the eye and raising it to the 4th power.\nSo a relu activation function plus a bias term would then be able to highlight different thresheld variations of this convolution image. For example, here I add a bias term to the convolution then apply a leaky relu. You can see it just highlights the one area where the eye is. A leaky relu is\n\\[\nlrelu(x, c) = \\left\\{\n  \\begin{array}{ll}\n  x & \\text{if $x > 0$} \\\\\n  x * c & \\text{otherwise}\n  \\end{array}\n  \\right.\n\\]\nwhere \\(c\\) is usually set to a small value. If \\(c=0\\) the leaky relu is just the relu. I set \\(c\\) to be 0.05 so that we can see the background image.\n\nplt.figure(figsize=[12.4, 6.2])\n\nK = img[200 : 270,225 : 322]\nplt.subplot(1, 3, 1)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(K,  cmap='gray', vmin=0, vmax=255)\n## I normalized it this way so that the convolution\n## numbers wouldn't be so big\n## Also, I put it to the 4th power, so it exactly finds \n## the eye.\nK = K ** 4\nK = K / K.sum()\nK = K - K.mean()\n\nimgC = sp.convolve2d(img, K)\nplt.subplot(1, 3, 2)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC)\nplt.title(\"Convolution\")\n\ntemp = imgC.copy()\n## Add a bias term of -15\ntemp -= 15\n## Perform a leaky relu\ntemp[np.where(temp < 0)] = temp[np.where(temp < 0)] * .05\n\nplt.subplot(1, 3, 3)\nplt.imshow(temp)\nplt.xticks([])\nplt.yticks([])\nplt.title(\"LRELU of convolution + bias\")\n\nText(0.5, 1.0, 'LRELU of convolution + bias')\n\n\n\n\n\nBecause of how convolutions work, this will find this eye anywhere in the image. Here we just add another eye somewhere else and repeat the convolution.\n\nplt.figure(figsize=[12.4, 6.2])\n\n#put another eye in the image\nimgCopy = img.copy()\nimgCopy[60 : 130, 85 : 182] = img[200 : 270,225 : 322]\nplt.subplot(1, 2, 1)\nplt.imshow(imgCopy,  cmap='gray', vmin=0, vmax=255)\nplt.xticks([])\nplt.yticks([])\n\nimgC = sp.convolve2d(imgCopy, K)\n\nplt.subplot(1, 2, 2)\ntemp = imgC.copy()\n## Add a bias term of -15\ntemp -= 15\n## Perform a leaky relu\ntemp[np.where(temp < 0)] = temp[np.where(temp < 0)] * .05\n\nplt.subplot(1, 2, 2)\nplt.imshow(temp)\nplt.xticks([])\nplt.yticks([])\nplt.title(\"LRELU of convolution + bias\")\n\nText(0.5, 1.0, 'LRELU of convolution + bias')\n\n\n\n\n\nSo, we found a custom kernel that highlights this specific feature in images. Convnets layers learn the kernel. That is, CNNs learn the image that gets convolved with the previous layer to produce the next one. Here’s a really great pictorial guide by Sumit Saha.\nNow, let’s discuss some specific vocabulary used in CNNs.\n\nPadding zero padding just like we discussed for 1D transformations\nPooling pooling, often max pooling, is a dimension reduction technique, taking the max in little blocks.\nstride length instead of sliding the kernel by moving it one pixel at a time, move it more to increase computational efficiency and reduce the size of the output convolution."
  },
  {
    "objectID": "nns_intro.html",
    "href": "nns_intro.html",
    "title": "23  Neural networks, introduction",
    "section": "",
    "text": "Let’s start by relating neural networks to regression. Consider a simple case where we have two nodes, \\(1\\) and \\(X\\) pointing to an outcome \\(Y\\). What does this mean? Let’s first put some context around the problem. Imagine that we want to use a subject’s BMI \\(X\\) to predict their blood pressure, \\(Y\\). This diagram represents that.\n\n\n\n\n\nTo interpret this diagram as a neural network, consider the following rule:\n\n\n\n\n\n\nNote\n\n\n\nParent nodes that point to a child node are multiplied by weights then added together then operated on by an activation function to form the child node.\n\n\nIf the parent nodes point to the outcome, then the nodes are combined the operated on by a known function, called the activation function to form a prediction. So, in this case, this is saying that the intercept (node labeled \\(1\\))times a weight plus BMI (node labeled \\(X\\)) times a different weight get combined to form a prediction for SBP \\(Y\\). Or, in other words\n\\[\n\\hat Y = g(w_0 \\times 1 + w_1 \\times X)\n\\]\nwhere \\(g\\) is a function that we specify. So in this case, if \\(w_0 = 120\\), \\(w_1 = .1\\) and \\(g\\) is an idenity function, \\(g(a) = a\\), and a subject had a BMI of 30, then the prediction would be\n\\[\n\\hat Y = g(120 + .1 * 30) = 120.3\n\\]\nNote \\(g\\) is not shown in the diagram (though maybe you could with the shape of the child node) or something like that0. Also not shown in the daigram is:\n\nThe loss function, i.e. how to measure the different between \\(\\hat Y\\) and \\(Y\\).\nThe way the loss function combines subjects; we have multiple BMIs and SBPs\nHow we obtain the weights, \\(W_0\\) and \\(W_1\\); this is done by minmizing the loss function using an algorithm\n\nSo, imagine the case where \\(g\\) is an identity function, our loss function for different subjects is squared error and we combine different losses by adding them up. Then, our weights are obtained by minmizing\n\\[\n\\sum_{i=1}^N (Y_i - \\hat Y_i)^2\n\\]\nand so, presuming our optimization algorithm works well, it should be idential to linear regression.\nConsider a different setting. Imagine if our \\(Y\\) is 0 or 1 based on whether or not the subject is taking anti-hypertensive mediations. Further, let \\(g\\) be the sigmoid function, \\(g(a) = 1 / \\{1 + \\exp(-a)\\}\\). Our prediction is\n\\[\n\\hat Y = \\{1 + \\exp(-W_0 - W_1 X)\\}^{-1}\n\\]\nwhich is the logistic regression prediction with intercept \\(W_0\\) and slope \\(W_1\\). Consider a case where \\(W_0 = -4\\), \\(W_1 = .1\\) and \\(X=30\\), then our \\(\\hat Y = 1 / \\{1 + \\exp[-(-4 + .1\\times 30)\\}]\\approx .27\\). Thus, this model estimates a 27% probability that a subject with a BMI of 30 has hypertension.\nFurther, if we specify that the loss function is binary cross entropy\n\\[\n- \\sum_{i=1}^n \\{ Y_i \\log(\\hat Y_i) + (1 - Y_i) \\log(1 - \\hat Y_i)\\} / N\n\\]\nthen minmizing our loss function is identical to maximizing the likelihood for logistic regression.\n\n1 / (1 + np.exp(-(-4 + .1 * 30)))\n\n0.2689414213699951"
  },
  {
    "objectID": "nns_intro.html#more-layers",
    "href": "nns_intro.html#more-layers",
    "title": "23  Neural networks, introduction",
    "section": "23.2 More layers",
    "text": "23.2 More layers\nOf course, there’d be no point in using NNs for problems that we can just solve with generalized linear models. NNs get better when we add more layers, since then they can discover interactions and non-linearities. Consider the following model. Notice we quit explicitly adding the bias (intercept) term / node. In general assume the bias term is included unless otherwise specified.\n\n\n\n\n\nUsually, the nodes are added in so called layers. \\((X_1, X_2)\\) is the input layer, \\((H_{11}, H_{12})\\) is the first hidden layer, \\((H_{21}, H_{22})\\) is the second hidden layer and \\(Y\\) is the output layer. Imagine plugging an \\(X_1\\) and \\(X_2\\) into this network. It would feed forward through the network as\n\\[\n\\begin{align}\nH_{11} = & g_1(W_{011} + W_{111} X_1 + W_{211} X_2) \\\\\nH_{12} = & g_1(W_{012} + W_{112} X_1 + W_{212} X_2) \\\\\nH_{21} = & g_2(W_{021} + W_{121} H_{11} + W_{221} H_{12}) \\\\\nH_{22} = & g_2(W_{022} + W_{122} H_{12} + W_{222} H_{12}) \\\\\n\\hat Y = & g_3(W_{031} + W_{131} H_{21} + W_{231} H_{22})\n\\end{align}\n\\]\nwhere \\(g_k\\) are specified activation functions. Typically, we would have a different activation function for the output layer than the others, and the other would have the same activation function. So, for example, if \\(Y\\) was binary, like hypertension diagnosis, then \\(g_1=g_2\\) and \\(g_3\\) would be a sigmoid."
  },
  {
    "objectID": "nns_intro.html#activation-functions",
    "href": "nns_intro.html#activation-functions",
    "title": "23  Neural networks, introduction",
    "section": "23.3 Activation functions",
    "text": "23.3 Activation functions\nThe output activation function tends to be based on the structure of the outcome. For example, a binary outcome would likely have a sigmoidal, or other function from \\(\\mathbb{R}\\) to \\([0, 1]\\) so as to model a probability. Historically, the internal activation functions were binary thresholds. This was owning to the fact that neural networks were models of (biological) neurons and the threshold was a model of an action potential being propigated. However, modern neural networks have less of a direct connection to their biological motivation and other activation functions tend to be used. The most popular right now is the rectified linear unit (RELU) function. This is simply:\n\\[\nRELU(a) = \\left\\{\n\\begin{array}{ll}\na & \\text{if $a>0$} \\\\\n0 & \\text{otherwise}\n\\end{array}\n\\right.\n= a \\times I(a > 0)\n\\]\nPlotted, this is:\n\nplt.plot( [-1, 0, 1], [0, 0, 1], linewidth = 4);\n\n\n\n\nIf a bias term is included, then the fact that the RELU is centered at zero isn’t important, since the intercept term effectively shifts the function around. These kinds of splin terms are incredibly flexible. Just to show you an example, let’s fit the sine function using a collection of shifted RELUs. This is just\n\\[\nY = \\sin(X) + \\epsilon\n\\]\nbeing fit with\n\\[\n\\sum_{i=1}^N \\left\\{ Y_i - W_{021} - \\sum_{j=1}^{d} W_{j21} g(W_{1j1} X_i- W_{0j1}) \\right\\}^2\n\\]\nwhere the \\(W_{kj}\\) are the weights for layer \\(k\\). Below, we’re just setting \\(W_{1j1} = 1\\) and specifying the \\(W_{0j1}\\) at a sequence of values.\n\n## Generate some data, a sine function on 0,4*pi\nn = 1000\nx = np.linspace(0, 4 * np.pi, n)\ny = np.sin(x) + .2 * np.random.normal(size = n)\n\n## Generate the spline regressors\ndf = 30\nknots = np.linspace(x.min(), x.max(), df)\nxmat = np.zeros((n, df))\nfor i in range(0, df): xmat[:,i] = (x - knots[i]) * (x > knots[i])\n\n## Fit them\nfrom sklearn.linear_model import LinearRegression\nyhat = LinearRegression().fit(xmat, y).predict(xmat)\n\n## Plot them versus the data\nplt.plot(x, y);\nplt.plot(x, yhat);\n\n\n\n\nThis corresponds to a network like depicted below if there were \\(d=3\\) hidden nodes, there was a relu activation function at the first layer, then a identity activation function for the output layer and the weights for the first layer are specified.\n\n\n\n\n\nWe can actually fit this function way better using splines and a little bit more care. However, this helps show how even one layer of RELU activated nodes can start to fit complex shapes."
  },
  {
    "objectID": "nns_intro.html#optimization",
    "href": "nns_intro.html#optimization",
    "title": "23  Neural networks, introduction",
    "section": "23.4 Optimization",
    "text": "23.4 Optimization\nOne of the last bits of the puzzle we have to figure out is how to obtain the weights. A good strategy would be to minimize the loss function. However, it’s hard to minmize. If we had a derivative, we could try the following. Let \\(L(W)\\) be the loss function for weights \\(W\\). Note, we’re omitting the fact that this is a function of the data (predictors and outcome) as well, since that’s a set of fixed numbers. Consider updating parameters as\n\\[\nW^{(new)} = W^{(old}) - e * L'(W^{(old)})\n\\]\nWhat does this do? It moves the parameters by a small amount, \\(e\\), called the learning rate, in the direction the opposite of the gradient. Think of a one dimensional convex function. If the derivative at a point is positive, then that point is larger than where the minimum is. Similarily, if the derivative is negative, it’s smaller. So, the idea is to head a small amount in the opposite direction of the derivative. How much? How about along the line of the derivative? That’s all gradient descent does, just in more than one dimension.\nHow do we get the gradient? Consider the following. If \\(X\\) is our vector of predictors and \\(Y\\) is our vector of outputs, a neural network with 3 layers, can be thought of as, where \\(L_k\\) is layer \\(K\\) and \\(W_k\\) are the weights for that layer:\n\\[\nL_3(L_2(L_1(X, W_1), W_2) W_3)\n\\]\nOr a series of function compositions. Recall from calculus, if we want the derivative of composed functions we have a really simple rule called the chain rule:\n\\[\n\\frac{d}{dx}f(g(x)) = f'(g(x)) g'(x)\n\\]\nI.e. if \\(h=f(u)\\) and \\(u = g(x)\\) then \\(\\frac{dh}{dx} = \\frac{dh}{du}\\frac{du}{dx}\\). Thus, characterized this way, the chain rule formally acts like fractions (though this is a symbolic equivalence having entirely different underlying meanings).\nIf we use the chain rule on our composed loss functions, we wind up bookkeeping backwards through our neural network. That is why it’s called backwards propagation (backprop).\nSo, our algorithm goes something like this. Given, \\(W^{(new)}\\), network, \\(\\phi(X, W)\\), which depends on the predictors and the weights and loss, \\(L(Y, \\hat Y)\\), which depends on the observed and predicted outputs.\n\nSet \\(W^{(old)}=W^{(new)}\\)\nCalculate \\(\\hat Y = \\phi(X, W^{(old)})\\) and loss \\(L(Y, \\hat Y)\\).\nUse back propagation to get to get a numerical approximation to \\(\\frac{d}{dW} L\\{Y, \\phi(X, W)\\} |_{W=W^{(old)}} = L'(W^{(old)})\\)\nUpdate \\(W^{(new)} = W^{(old)} - e L'(W^{(old)})\\)\nGo to step 0."
  },
  {
    "objectID": "nns_basic_regression.html",
    "href": "nns_basic_regression.html",
    "title": "24  Basic regression as a NN",
    "section": "",
    "text": "sns.scatterplot(x = dat['T2'], y = dat['PD'])\n\n<AxesSubplot:xlabel='T2', ylabel='PD'>\n\n\n\n\n\n\nfit = smf.ols('PD ~ T2', data = dat).fit()\nfit.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:           PD          R-squared:             0.661\n\n\n  Model:                   OLS         Adj. R-squared:        0.657\n\n\n  Method:             Least Squares    F-statistic:           190.9\n\n\n  Date:             Sun, 26 Feb 2023   Prob (F-statistic): 9.77e-25\n\n\n  Time:                 17:45:21       Log-Likelihood:      -57.347\n\n\n  No. Observations:         100        AIC:                   118.7\n\n\n  Df Residuals:              98        BIC:                   123.9\n\n\n  Df Model:                   1                                    \n\n\n  Covariance Type:      nonrobust                                  \n\n\n\n\n               coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept     0.3138     0.052     6.010  0.000     0.210     0.417\n\n\n  T2            0.7832     0.057    13.815  0.000     0.671     0.896\n\n\n\n\n  Omnibus:        1.171   Durbin-Watson:         1.501\n\n\n  Prob(Omnibus):  0.557   Jarque-Bera (JB):      0.972\n\n\n  Skew:           0.241   Prob(JB):              0.615\n\n\n  Kurtosis:       2.995   Cond. No.               1.89\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# The in sample predictions\nyhat = fit.predict(dat['T2'])\n\n# Make sure that it's adding the intercept\n#test = 0.3138 + dat['T2'] * 0.7832\n#sns.scatterplot(yhat,test)\n\n## A plot of the in sample predicted values\n## versus the actual outcomes\nsns.scatterplot(x = yhat, y = dat['PD'])\nplt.plot([-1, 3], [-1, 3], linewidth=2)\n\n\n\n\n\nn = dat.shape[0]\n\n## Get the y and x from \nxtraining = torch.from_numpy(dat['T2'].values)\nytraining = torch.from_numpy(dat['PD'].values)\n\n## PT wants floats\nxtraining = xtraining.float()\nytraining = ytraining.float()\n\n## Dimension is 1xn not nx1\n## squeeze the second dimension\nxtraining = xtraining.unsqueeze(1)\nytraining = ytraining.unsqueeze(1)\n\n## Show that everything is the right size\n[xtraining.shape, \n ytraining.shape,\n [n, 1]\n ]\n\n[torch.Size([100, 1]), torch.Size([100, 1]), [100, 1]]\n\n\n\n## Show that linear regression is a pytorch \nmodel = torch.nn.Sequential(\n    torch.nn.Linear(1, 1)\n)\n\n## MSE is the loss function\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\n## Set the optimizer\n## There are lots of choices\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n\n## Loop over iterations\nfor t in range(10000):\n\n    ## Forward propagation\n  y_pred = model(xtraining)\n    \n  ## the loss for this interation\n  loss = loss_fn(y_pred, ytraining)\n\n  #print(t, loss.item() / n)\n\n  ## Zero out the gradients before adding them up \n  optimizer.zero_grad()\n  \n  ## Backprop\n  loss.backward()\n  \n  ## Optimization step\n  optimizer.step()\n\n\nytest = model(xtraining).detach().numpy().reshape(-1)\nsns.scatterplot(x = ytest, y = yhat)\nplt.plot([-1, 3], [-1, 3], linewidth=2)\n\n\n\n\n\nfor param in model.parameters():\n  print(param.data)\n\ntensor([[0.7831]])\ntensor([0.3138])"
  },
  {
    "objectID": "nns_logistic_regression.html",
    "href": "nns_logistic_regression.html",
    "title": "25  Logistic regression as a NN",
    "section": "",
    "text": "import pandas as pd\nimport torch\nimport statsmodels.formula.api as smf\nimport statsmodels as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n## Read in the data and display a few rows\ndat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv\")\ndat.head(4)\n\n## Create a binary outcome variable (people will use gold lesions in HW)\nm = np.median(dat.T2)\ndat = dat.assign(y = (dat.T2 > m) * 1 )\n## Create a normalized regression variable\ndat = dat.assign(x = (dat.PD - np.mean(dat.PD)) / np.std(dat.PD))\ndat.head()\n\n\n\n\n\n  \n    \n      \n      FLAIR\n      PD\n      T1\n      T2\n      FLAIR_10\n      PD_10\n      T1_10\n      T2_10\n      FLAIR_20\n      PD_20\n      T1_20\n      T2_20\n      GOLD_Lesions\n      y\n      x\n    \n  \n  \n    \n      0\n      1.143692\n      1.586219\n      -0.799859\n      1.634467\n      0.437568\n      0.823800\n      -0.002059\n      0.573663\n      0.279832\n      0.548341\n      0.219136\n      0.298662\n      0\n      1\n      1.181648\n    \n    \n      1\n      1.652552\n      1.766672\n      -1.250992\n      0.921230\n      0.663037\n      0.880250\n      -0.422060\n      0.542597\n      0.422182\n      0.549711\n      0.061573\n      0.280972\n      0\n      1\n      1.426453\n    \n    \n      2\n      1.036099\n      0.262042\n      -0.858565\n      -0.058211\n      -0.044280\n      -0.308569\n      0.014766\n      -0.256075\n      -0.136532\n      -0.350905\n      0.020673\n      -0.259914\n      0\n      0\n      -0.614749\n    \n    \n      3\n      1.037692\n      0.011104\n      -1.228796\n      -0.470222\n      -0.013971\n      -0.000498\n      -0.395575\n      -0.221900\n      0.000807\n      -0.003085\n      -0.193249\n      -0.139284\n      0\n      0\n      -0.955175\n    \n    \n      4\n      1.580589\n      1.730152\n      -0.860949\n      1.245609\n      0.617957\n      0.866352\n      -0.099919\n      0.384261\n      0.391133\n      0.608826\n      0.071648\n      0.340601\n      0\n      1\n      1.376909\n    \n  \n\n\n\n\n\nfit = smf.logit('y ~ x', data = dat).fit()\nfit.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.427855\n         Iterations 7\n\n\n\n\nLogit Regression Results\n\n  Dep. Variable:           y          No. Observations:       100  \n\n\n  Model:                 Logit        Df Residuals:            98  \n\n\n  Method:                 MLE         Df Model:                 1  \n\n\n  Date:            Sun, 26 Feb 2023   Pseudo R-squ.:       0.3827  \n\n\n  Time:                17:46:11       Log-Likelihood:      -42.785 \n\n\n  converged:             True         LL-Null:             -69.315 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        3.238e-13\n\n\n\n\n               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept     0.0367     0.269     0.136  0.892    -0.491     0.565\n\n\n  x             2.2226     0.436     5.095  0.000     1.368     3.078\n\n\n\n\n\n# The in sample predictions\nyhat = 1 / (1 + np.exp(-fit.fittedvalues))\n\n\nn = dat.shape[0]\n\n## Get the y and x from \nxtraining = torch.from_numpy(dat['x'].values)\nytraining = torch.from_numpy(dat['y'].values)\n\n## PT wants floats\nxtraining = xtraining.float()\nytraining = ytraining.float()\n\n## Dimension is 1xn not nx1\n## squeeze the second dimension\nxtraining = xtraining.unsqueeze(1)\nytraining = ytraining.unsqueeze(1)\n\n## Show that everything is the right size\n[xtraining.shape, \n ytraining.shape,\n [n, 1]\n ]\n\n[torch.Size([100, 1]), torch.Size([100, 1]), [100, 1]]\n\n\n\n## Doing it more now the pytorch docs recommend\n## Example taken from \n## https://medium.com/biaslyai/pytorch-linear-and-logistic-regression-models-5c5f0da2cb9\n\n## They recommend creating a class that defines\n## the model\nclass LogisticRegression(torch.nn.Module):\n     def __init__(self):\n        super(LogisticRegression, self).__init__()\n        self.linear = torch.nn.Linear(1, 1, bias = True)\n     def forward(self, x):\n        y_pred = torch.sigmoid(self.linear(x))\n        return y_pred\n\n## Then the model is simply  \nmodel = LogisticRegression()\n\n## MSE is the loss function\nloss_fn = torch.nn.BCELoss()  \n\n## Set the optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n\n## Loop over iterations\nfor t in range(100000):\n\n  ## Forward propagation\n  y_pred = model(xtraining)\n\n  ## the loss for this interation\n  loss = loss_fn(y_pred, ytraining)\n\n  #print(t, loss.item() / n)\n\n  ## Zero out the gradients before adding them up \n  optimizer.zero_grad()\n  \n  ## Backprop\n  loss.backward()\n  \n  ## Optimization step\n  optimizer.step()\n\n\nytest = model(xtraining)\nytest = ytest.detach().numpy().reshape(-1)\nplt.plot(yhat, ytest,  \".\")\nplt.plot([0, 1], [0, 1], linewidth=2)\n\n\n\n\n\nfor param in model.parameters():  \n  print(param.data)\n\ntensor([[1.3181]])\ntensor([0.0870])"
  }
]