[
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This is a book for the Data Science for Bio/Biostat/Public Health/medical classes."
  },
  {
    "objectID": "intro_markdown.html",
    "href": "intro_markdown.html",
    "title": "1  Markdown",
    "section": "",
    "text": "Before getting started, you need to learn some markdown. Markdown is a markup language (like HTML) that is absurdly easy. Every data scientist needs to know markdown. Fortunately, you’re five minutes away from knowing it. A markdown file is a text file that needs to be rendered to look nice. If you want an example, this page was written in markdown. To try it, go to google colab or a jupyter lab, create a markdown cell and start editing. Also, github will markup a markdown page so that is a good place to experiment.\nAs mentioned, markdown is markup language. So, you write in plain text and then it needs to be rendered into a pretty document or page. For example, all of these notes were written in markdown, but then converted to HTML. There are different flavors of markdown. So, syntax can change a bit. I’m using the one that works in quarto.\nHere’s how you section in markdown.\n# Top level heading\n## Second level heading\n### Third level heading\nand so on. It should look something like this, though the style can change depending on how it is being rendered.\n Top level heading \n Second level heading \n Third level heading \nYou can can bold text and italicize text like this:\n**bold** text and *italicize*\nIf you want an ordered list, like this:\n\nItem one.\nItem two.\nItem three.\n\nthen you can just do:\n1. Item one.\n2. Item two.\n3. Item three.\nNote, if you move around the order of the list in the markdown code, the markup engine will just reorder them starting at whatever the first number is. An unordered lists can be done with asterisks or plus signs or dashes. (I tend to use asterisks.) Also, putting brackets with an x makes for a check mark.\n* [ ] Pick up broccoli\n* [ ] Pick up oat milk\n* [x] Pick up golden berries\n* [x] Pick up tea\n\nPick up broccoli\nPick up oat milk\nPick up golden berries\nPick up tea\n\nInline code, like lambda x: x ** 2 can be written with backticks like this:\n`lambda x: x ** 2`\nBlock code is written in between three backticks.\n```\nlike this\n```\nLinks can be done like this:\n[Markdown cheat sheet](https://www.markdownguide.org/cheat-sheet/).\nwhich renders like this: Markdown cheat sheet. (Also, that’s a real link to a nice MD cheat sheet.) Images can be done like this\n![Image alt text](assets/images/book_graphic.png)\nIf your converter can use mathjax, or some other LaTeX math rendering library, you can insert LaTeX equations. For example,\n\\[\n\\int_{-\\infty}^\\infty (2\\pi\\sigma^2)^{-1/2} \\exp\\left\\{-\\frac{1}{2\\sigma^2} (x - \\mu)^2\\right\\} dx = 1\n\\]\ncan be written as\n$$\n\\int_{-\\infty}^\\infty (2\\pi\\sigma^2)^{-1/2} \\exp\\left\\{-\\frac{1}{2\\sigma^2} (x - \\mu)^2\\right\\} dx = 1\n$$\nI noticed the github markdown renderer doesn’t load mathjax, but most of the data science things do, like jupyter-lab, colab and quarto.\nThat’s plenty of markdown to start. Try it out. You’ll find that you pick it up really fast."
  },
  {
    "objectID": "intro_unix.html",
    "href": "intro_unix.html",
    "title": "2  Unix",
    "section": "",
    "text": "Some basic unix commands will go a long way, especially for when you’re working on a remote server. On windows, you can actually install a unix environment on so-called services for unix. So, on windows, you have three options for working with the command line, i) install a linux subsystem and use that, ii) use the DOS command prompt or iii) use powershell. The commands here would only work for option i. However, when I work on a windows system, I tend to just use options ii or iii. Here, we’ll assume you’re working on a linux or unix system, or windows services for linux and you’ll have to read up elsewhere if you want to learn windows proper terminal commands.\nTo get a unix terminal, you have several options. Since we’re promoting jupyter and jupyterlab, just open up the terminal on there. (Again, assuming you’re working on a unix/linux system.)\nThe first thing you should try is figuring out where you’re at. Do this by typing\nprompt&gt; pwd\nThis will show you where you are in the directory structure. If you want to see the contents of the directory try these\nprompt&gt; ls\nprompt&gt; ls -al\nprompt&gt; ls -alh\nAdding the flags -a lists everything, including directories with a weird character in front. The l gives the long format, which gives more information and the h changes the filesize lists to a more human readable format. I also like the option --color. What you get with-alh` is as follows.\ntotal 36K\ndrwxrwxrwx+ 7 codespace root      4.0K Feb 14 14:24 .\ndrwxr-xrwx+ 5 codespace root      4.0K Oct 19 15:21 ..\ndrwxrwxrwx+ 6 codespace codespace 4.0K Feb 14 14:31 book\ndrwxrwxrwx+ 8 codespace root      4.0K Feb 15 21:34 .git\n-rw-rw-rw-  1 codespace codespace  171 Feb 14 14:24 .gitignore\n-rw-rw-rw-  1 codespace codespace    0 Feb 14 14:23 .nojekyll\n-rw-rw-rw-  1 codespace codespace  444 Feb 14 14:24 README.md\ndrwxrwxrwx+ 3 codespace codespace 4.0K Feb 14 14:24 slides\ndrwxrwxrwx+ 7 codespace codespace 4.0K Oct 19 15:21 .venv\ndrwxrwxrwx+ 2 codespace codespace 4.0K Oct 19 15:23 .vscode\nThe drwxrwxrwx+1 looking columns give permissions d=directory, r=read, w=write and x=execute, the groups are owner (you), group, everyone. So a file that is -rw------- can be read and written to by the owner, but cannot be executed by anyone and no one else can read or write to it (except the superuser, who gets to do everything).\nTo change a directory, try the following\nprompt&gt; cd DIRECTORY\nwhere DIRECTORY is the name of the directory that you want to change into. You can hit TAB to autocomplete names. The command\nprompt&gt; mv PATH_TO_INPUT_FILE PATH_TO_OUTPUT_FILE\nmoves the file. This is also how you rename a file, since you could just do mv FILENAME1 FILENAME2 and change the name.\nThe unix command for removing things is rm. So\nrm FILENAME\ndeletes the file. Note linux really deletes things, so do this with some care. You can’t remove directories this way, instead you could do rmdir DIRECTORY, but the directory has to be empty. If you want to use rm to remove a directory and its contents, you can do rm -rf DIRECTORY. However, use this with care.\nFinally, I find it very useful to use wget to grab files from the internet. So, for example,\nwget https://URL.../FILENAME\nwill grab the file from that link. Super useful.\nThat’s enough unix to get you started. You’ll find as you use the terminal more and more, you’ll like it better and better. Eventually, you’ll find GUIs kind of frustrating."
  },
  {
    "objectID": "intro_html.html#html",
    "href": "intro_html.html#html",
    "title": "4  HTML, CSS and javascript",
    "section": "4.1 HTML",
    "text": "4.1 HTML\nHTML is a markup language used by web browsers. HTML stands for hypetext markup language. Like all markup languages, it gives a text set of instructions that get interpreted into a nicer looking document. Other markup languages include XML, LaTeX, Org and markdown. (Yes, mark”down” is named as such since it’s a ultra-simple mark”up” language.)\nWe’ll need a little html knowledge since so much data science output is web-page oriented. Also, we’ll need to know a little about html to scrape web content. A web page typically has three elements: the html which gives the page structure and markup, css (cascading style sheets) for style and javascript for interactivity. We’ll cover a little html and javascript so that we can better understand certain data science products. However, you should take a web development course if you want in depth treatments.\nWe won’t spend much time talking about CSS. CSS gives a set of standards for the style of a web page. With CSS one can take the skeleton (HTML/JS) and dramatically change the style in the same way you could choose to play some sheet music in different ways. A quick tutorial on CSS can be found here.\nBack to HTML. An HTML document looks something like this. Take a file, insert the following code and give it the extension .html. Then, open it up in a browser.\n&lt;!DOCTYPE html&gt;\n&lt;HTML&gt;\n    &lt;HEAD&gt;\n        &lt;TITLE&gt; This is the web page title&lt;/TITLE&gt;\n    &lt;/HEAD&gt;\n    &lt;BODY&gt;\n        &lt;H1&gt;Heading 1&lt;/H1&gt;\n        &lt;H2&gt;Heading 2&lt;/H2&gt;\n        &lt;P&gt; Paragraph &lt;/P&gt;\n        &lt;CODE&gt; CODE &lt;/CODE&gt;\n    &lt;/BODY&gt;\n&lt;/HTML&gt;\nThe resulting document will look like the following\n\n\n   \n      Heading 1\n      Heading 2\n       Paragraph \n      CODE \n   \n\nAs you probably noticed, a bit of markup is something like &lt;COMMAND&gt;CONTENT&lt;/COMMAND&gt;. The latter command has a forward slash. You should close your commands, even if your browser still renders the page like you like just because it makes for bad code not to. Also, someone else’s browser may not be as forgiving. Good code editors will help remind you to close your commands."
  },
  {
    "objectID": "intro_html.html#browser-stuff",
    "href": "intro_html.html#browser-stuff",
    "title": "4  HTML, CSS and javascript",
    "section": "4.2 Browser stuff",
    "text": "4.2 Browser stuff\nNote, since we’ll be working a lot with files, probably in one directory, you can use file:///PATH TO YOUR DIRECTORY to open up files (maybe even bookmark that directory). Also, CTRL-R is probably faster than clicking refresh and (in chrome at least) CTRL-I brings up developer tools (javascript console). When we have a web server running locallly, you usually go to localhost. For example, my jupyter lab server sends me to http://localhost:8888/lab/tree/. Here 8888 is a port, localhost refers to the server running on the lcoal computer and lab/tree is the relative path to the root of my jupyter lab server.\nBrowsers make choices in how they render HTML and CSS and implement javascript. So, unless you’re a web developer by trade, don’t get too exotic in your design choices. Also, a lot of HTML is auto generated. So, your mileage may vary by looking at page sources."
  },
  {
    "objectID": "intro_html.html#hosting",
    "href": "intro_html.html#hosting",
    "title": "4  HTML, CSS and javascript",
    "section": "4.3 Hosting",
    "text": "4.3 Hosting\nWhen you double click on your html file, it’s being hosted locally. So, no one else can see it. To have a web page on the internet it has to be hsoted on a server running web hosting software. Fortunately, github will actually allow us to host web pages. Basically, put an empty .nojekyll file in your repository (this tells it that it’s not a jekyll based web site and follow the instructions here. This will be really useful for us, since many of our datascience programs output web pages. For example, RMarkdown documents get translated into web documents. Similarly, jupyter-lab will output reveal.js (javascript/html) slide decks from our jupyter lab notebooks. Note that some of our programs will require servers that also run python or R in the back end, so github pages won’t suffice for that. There we need servers specifically set up to run those kinds of scripts."
  },
  {
    "objectID": "intro_html.html#javascript",
    "href": "intro_html.html#javascript",
    "title": "4  HTML, CSS and javascript",
    "section": "4.4 Javascript",
    "text": "4.4 Javascript\nJavascript is what makes webpages interactive. We’ll need a little javascript to understand how interactive web graphics work. Consider the following where we use javscript to change an HTML element in a web page\n&lt;H2 id=\"textToChange\"&gt;Preference ?&lt;/H2&gt;\n\n&lt;button type=\"button\" onclick='document.getElementById(\"textToChange\").innerHTML = \"You prefer 1\"'&gt;1&lt;/button&gt;\n&lt;button type=\"button\" onclick='document.getElementById(\"textToChange\").innerHTML = \"You prefer 2\"'&gt;2&lt;/button&gt;\nHere’s the result:\nPreference ?\nClick 1\nClick 2\n\n4.4.1 JSON\nJSON is a data format used in javascript, and adopted elsewhere. It’s a fairly straightforward data structure. In fact, you might have edited some JSON data by editing your Jupyter Lab ipython notebook properties. It goes \"name\":value where objects are enclosed in curly braces and arrays in brackets. You have to separate distinct object or values with quotes.\n{\n    \"name\" : \"Bowie\",\n    \"species\" : \"dog\",\n    \"age\" : 6,\n    \"likes\" : [\"biscuits\", \"naps\", \"walks\"],\n    \"dislikes\" : [\"the cat\", \"getting groomed\"]    \n}\nLet’s define a variable in our JS console. Open up your JS console and type:\npet = {\n    \"name\" : \"Bowie\",\n    \"species\" : \"dog\",\n    \"age\" : 6,\n    \"likes\" : [\"biscuits\", \"naps\", \"walks\"],\n    \"dislikes\" : [\"the cat\", \"getting groomed\"]\n};\nNow try doing things like pet.likes and hit enter."
  },
  {
    "objectID": "intro_html.html#example-of-using-a-javascript-library",
    "href": "intro_html.html#example-of-using-a-javascript-library",
    "title": "4  HTML, CSS and javascript",
    "section": "4.5 Example of using a javascript library",
    "text": "4.5 Example of using a javascript library\nLet’s use a javscript library to plot some data. Unless you need really fine control over the javascript elements, we usually do this by calling a python or R api to the JS library. But, it’s useful to do once to see how the library does things.\nHere is some data that we’ll use a lot. It is brain regions of interest (ROIs) by the percentage of the brain that they make up.\n               roi       comp\n0              CSF   7.370845\n1   Diencephalon_L   0.756288\n2   Diencephalon_R   0.763409\n3    Mesencephalon   0.864718\n4    Metencephalon  12.488275\n5   Myelencephalon   0.378464\n6  Telencephalon_L  42.030477\n7  Telencephalon_R  42.718368\nLet’s use Vegalite. This package creates the plot via a JSON object that contains all of the data and instructions.\n{\n    \"data\" : {\n        \"values\" : [\n{\"roi\" : \"CSF\"            , \"comp\" :  7.370845},\n{\"roi\" : \"Diencephalon_L\" , \"comp\" :  0.756288},\n{\"roi\" : \"Diencephalon_R\" , \"comp\" :  0.763409},\n{\"roi\" : \"Mesencephalon\"  , \"comp\" :  0.864718},\n{\"roi\" : \"Metencephalon\"  , \"comp\" : 12.488275},\n{\"roi\" : \"Myelencephalon\" , \"comp\" :  0.378464},\n{\"roi\" : \"Telencephalon_L\", \"comp\" : 42.030477},\n{\"roi\" : \"Telencephalon_R\", \"comp\" : 42.718368}\n        ]\n    },\n    },\n   \"mark\": \"bar\",\n   \"encoding\": {\n    \"y\": {\"field\": \"roi\" , \"type\": \"nominal\"},\n    \"x\": {\"field\": \"comp\", \"type\": \"quantitative\"}\n}\nThis needs to be embedded into html, plus the vega JS libraries loaded to execute. I have an example here. The output looks like this\n\n\n\nGraphic\n\n\nTypically, one creates these graphics in one’s home analysis language (like python or R). There are several libraries for doing as such. Some of the popular ones include: bookeh, vega, D3js, leaflet, but there are many more. There’s also connections to large private efforts including tableau, power bi, google charts and plotly."
  },
  {
    "objectID": "intro_git.html#the-least-you-need-to-know",
    "href": "intro_git.html#the-least-you-need-to-know",
    "title": "3  Git, github",
    "section": "3.1 The least you need to know",
    "text": "3.1 The least you need to know\nI’m not going to fully recreate git / github tutorials here; here’s one I recommend by Sean Kross. Instead, I’m going to go through a typical git / github workflow.\n\nInitialization I almost always initialize my git repository on github with a readme.md file.\nClone I typically clone the repository to my local computer using the command line or a local git gui that works with github, like this one. Note that you only have to clone the repo once. After it’s cloned you have a full local copy of the repository.\nadd new files to track and stage them after I’ve worked with them.\ncommit the changes to the local repository with a meaningful commit message.\npush the changes to the repository.\nIf there’s changes on the remote repository not represented in my local repository, I pull those changes to my local repo.\n\nFor larger projects, you’re likely working with multiple people, some of whom you’ve given access to your remote repository and some of whom you have not. The ones who can’t directly push to the remote repo might have their own version of the code and their own version on github. If they think you should incorporate those changes, they might issue a pull request to you. You can then opt to pull their changes into your repo on github, then pull them from github to your local repo. One of the reasons why services like github and bitbucket are so popular is that they make this coordination fairly easy, along with having nice project messaging and management tools.\nIn our class, we use github classroom. For github classroom, you’ll get a link to a repo to put your submission files into. When you push to the remote repository, you’ll have submitted. But, up to the due date you can make changes an push again."
  },
  {
    "objectID": "intro_git.html#a-little-more-detail",
    "href": "intro_git.html#a-little-more-detail",
    "title": "3  Git, github",
    "section": "3.2 A little more detail",
    "text": "3.2 A little more detail\n\n3.2.1 Getting started\nGit is a version control system invented by Linus Torvalds, the invetor of the linux operating system. A github repository is a directory that the software git is using to keep track of versions. Install git, it’s already installed on most cloud services and many operaiting systems. You can initialize a repository in a directory with the command git init in that directory. Honestly, I almost never start a git repo this way, because I’ve connected my repo to a cloud server. But, more on that later. After you’ve initialized it, your git repo is a locally version controlled system.\n\n\n3.2.2 Adding files\nOK, now you want to add some of your files to your repo. You have a directory You can add files with the command\ngit add FILENAME\nThe file FILENAME is now staged to be part of the repository, but isn’t added yet! Files that you haven’t added won’t be tracked. This is good, since there’s many files, llike log files, that we don’t want to track. You only need to add the files once, then they are being tracked. Try it out with a readme markdown file in your repository.\n\n\n3.2.3 Checking status\nOne of the most useful git commands is\ngit status\nThis gives the status of which files are being tracked, which are not and which have changes.\n\n\n3.2.4 Commiting changes\nYou have to actually commit your changes to the repository to have them represented. Committing changes is a local operation, subsequently, we’ll show you how to coordinate with a remote repository. Here’s the command I use to commit most often\ngit commit -a -m \"COMMIT MESSAGE\"\nHere the flag -a means commit all files staged for commits. You can also commit individual files or a collection of specific files. The -m \"COMMIT MESSAGE\" is the message associated with your commit. Make your commit messages more informative. If you don’t add -m github will bring up an editor for you to enter your commit message.\n\n\n3.2.5 Remote repositories\nGit becomes much more useful when it’s coordinated with a remote repository. Github is a server for hosting remote git repositories, there are several others, bitbucket is an example. Github also includes a great deal of functionality for working with the remote repository through the website. There’s many git and github guis that you could use if you want. I always liked to github one and magit in emacs.\n\n3.2.5.1 Connecting your remote and local repos\nYou need to connect your local repo to the remote repo. First create a version of the repo on github (actually then it gives you instructions). Then you can either\n\nClone the repo from the server git clone REMOTE SERVER\nConnect the server to your local repository\n\nFor 2. you use a command like:\ngit remote add origin git@github.com:USERNAME/REPONAME.git\nPersonally, I find it easier just to create the repo on github and clone it. You can clone over ssh or https, if you don’t know what ssh is use https and then switch over to ssh as you learn how to use it.\n\n\n3.2.5.2 Pushing changes\n\n\n\n\n\nYou push from the repo you’re working on to another (remote repo). So, for me, this typically means pushing from my local computer repo to github or to a scientific computing cluster.\ngit push origin main\nThe above command could be read as “push my new committed changes from my origin to the main branch on my remote repo”. Older repos had the default branch master instead of main. But, github switched in 2020 or so. So, some older documentation may not have this switch.\n\n\n3.2.5.3 Pulling changes\n\n\n\n\n\nThere might be changes that you want represented locally that were changed on the remote repository; for example, you might edit a readme file through github’s web interface. The command for this is really easy, it’s just pull\ngit pull\nThe pain occurs if you have made local changes that are in conflict with the remote changes. I’d write about merge conflicts, but I can’t, I just can’t (shudder). [Here’s some docs on managing merge conflicts] (https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/addressing-merge-conflicts/resolving-a-merge-conflict-using-the-command-line).\n\n\n3.2.5.4 Cloning\n\n\n\n\n\nTo clone a remote repository is to simply create a local copy where you can pull changes from the remote. If a repository is public and you own it, then you can clone it and push changes back to the remote. If you don’t, you can clone and pull changes, but then can’t push back to the remote.\nTry with the book repo, clone it so you have access to the files. In the figure below the owner can pull and push to the repo. In the picture below, consider that you clone GH1 off of github to you. You can pull updates from GH1 but can’t push changes to GH1. The owner of GH1 of course can pull and push.\nAs an example, close this book (via https) with:\ngit clone https://github.com/smart-stats/ds4bio_book.git\n\n\n3.2.5.5 Forking\n\n\n\n\n\nA common workflow for github is to “fork” the repository. This differs from cloning in the sense that your fork is an entirely separate new copy. In the picture above, GH2 is a fork of GH1. It can pull upstream changes from GH1, but can’t write to it, since you don’t own that repo. You can push and pull from Local2 which is connected to GH2.\nForking on github is easy. Just click the “fork” button in the upper right corner.\n\n\n3.2.5.6 Pull requests\n\n\n\n\n\nImagine a case where you forked the book repo and have some great additions (maybe a section on merge conflicts?). You think that these changes you think should be represented for other students. You can’t push changes to my repo; the very idea would be perposterous! How do you get your great changes represented in my repo?\nYou issue a pull request in the sense of requesting GH1 to pull your changes from GH2. Then, I would get notified of the pull request and would decide whether or not to merge them into my repo. This system has many benefits, and particularly, is great for operating on open source projects.\nI like to do pull requests directly on github through the web interface."
  },
  {
    "objectID": "intro_git.html#branching",
    "href": "intro_git.html#branching",
    "title": "3  Git, github",
    "section": "3.3 Branching",
    "text": "3.3 Branching\nYou often want multiple versions of a repository. For example, you might want a development version, a working version and a stable version. This is what branches are for in git. To create and switch to a branch called dev, use:\ngit checkout -b dev\nThis creates a new branch called dev and switches to that branch. If you already have dev, use checkout without -b. Suppose you want to merge your change from dev into your default branch (main)\ngit checkout main\ngit merge dev\nStart very basic with branches, then when you get the hang of it you can use it like a pro. Here’s more documentation."
  },
  {
    "objectID": "intro_git.html#clients",
    "href": "intro_git.html#clients",
    "title": "3  Git, github",
    "section": "3.4 Clients",
    "text": "3.4 Clients\nYou can avoid the command line stuff with a git client, of which there are many. It’s still good to know the command line options, for remote work if nothing else. I’ve tried several git clients and my favorite is the github client software and magit in emacs."
  },
  {
    "objectID": "intro_git.html#setting-up-ssh",
    "href": "intro_git.html#setting-up-ssh",
    "title": "3  Git, github",
    "section": "3.5 Setting up ssh",
    "text": "3.5 Setting up ssh\nIf you want to connect to a remote repository to github on a unix or linux system, it’s probably easiest to set up using so called secure shell. You can also set it up a variety of ways on Windows, but the rules are a little less universal. If you find this very hard to follow, just use a git and github gui (like github desktop, which is what I like to recommend).\nSecure shell works as follows. It creates a public key (think like a lock, everyone can see it) and a private key (like the key for the lock, only you have it). When you connect to github, it has your public key and you use the private key to “unlock” it. First, you need a .ssh directory from your personal directory. If you don’t have that, try ssh username@server to a server that you know and it will automatically create one.\nwe’ll go through this tutorial.\nThen, cd into your .ssh director and type the following and follow the instructions\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\nThis generates your public/prviate key pair. Then you need to take the public key file, and upload it to github under settings. This is described here.\nIn some cases I’ve also had to add this to a file config in my .ssh directory:\nHost github.com\n     ForwardX11 no\n     HostName github.com\n     User git\n     IdentityFile ~/.ssh/YOUR_PRIVATE_KEY_FILENAME\nOnce this is setup, then you should be able to push and pull from github from the command line without passwords."
  },
  {
    "objectID": "intro_git.html#github-pages",
    "href": "intro_git.html#github-pages",
    "title": "3  Git, github",
    "section": "3.6 Github pages",
    "text": "3.6 Github pages\nIt’s possible to host web pages on github. By hosting I mean serving the web page on the internet so that others can view it. There’s a couple of steps to doing this. First, you need an html file to serve. If you look in the chapter on html, there’s the code for a basic wepage. Save a web page as say “index.html”. You can double check that it works by simply double clicking on the file on your local computer. It should display as a web page in your browser, but it’s only viewable by you. Notice the addres begins with file:///.... To serve the file so that others can view it, we need it to exist on a server.\nGithub will serve the file for us, but first we need a repository. Create a public repository; I typically do this on github. Then you need to add an empty file called .nojekyll. Don’t forget the period at the beginning. This prevents github from thinking you’re using the jekyll framework, which is the default. Since we’re serving a basic webpage, we don’t need this framework. I create this file on github just by clicking Add File then Create new file.\nNext we need to tell github that this repo serves web pages. Click on settings then pages on the left. Then, under source choose the branch that you want to serve the pages from. Add your html file, commit and push to github. Give it a few minutes, then your file will be served from\nhttps:///GITHUBUSERNAME.github.io/REPONAME/FILENAME.html\nwhere here GITHUBUSERNAME is either your github username or the organization. REPONAME is the name of your repository and FILENAME.html is your html file, with the path if it’s in subdirectories. As an example, here’s a version I did https://bcaffo.github.io/testHtmlRepo/index.html."
  },
  {
    "objectID": "python.html#notebooks",
    "href": "python.html#notebooks",
    "title": "Python",
    "section": "Notebooks",
    "text": "Notebooks\nNotebooks are going to be especially useful for us, as they’re a great way to do data analyses. With notebooks, you can merge richer documentation together with analysis code. You can take this to the extreme, and have solutions that create reproducible final documents. This book is an example, where the entire thing is written in jupyter-book. We’ll discuss this idea a little more when we discuss reproducible research. Alternatively, you can use your notebook as a working document that\nMost notebook solutions have text blocks and code blocks. The text is marked up in a markup language called “Markdown”, which we discussed eariler.\nIf you’re very new to notebooks in python, I would suggest starting with colab. The colab documentation is useful."
  },
  {
    "objectID": "python.html#weaved-text-formats",
    "href": "python.html#weaved-text-formats",
    "title": "Python",
    "section": "Weaved text formats",
    "text": "Weaved text formats\nI wrote this book in a format called quarto (see https://quarto.org/ ). This is a slightly different approach than jupyter notebooks and are perhaps better at producing final document-style output. Other approaches similar to quarto documents include R markdown, sweave and org mode."
  },
  {
    "objectID": "python_basic.html#data-structures",
    "href": "python_basic.html#data-structures",
    "title": "5  Python basics",
    "section": "5.1 Data structures",
    "text": "5.1 Data structures\nPython has some more advanced data structures that build on its primitive types.\n\nLists: ordered collections of objects\nSets: like lists but only have unique elements\nTuples: like lists, but not mutable, i.e. need to create a new one to modify\nDictionaries: named elements that you can reference by their name rather than position\n\nFirst, let’s look at some list operations.\n\ndat = [1, 4, 8, 10] # define a list\nprint(dat[0])       # reference an element\nprint(dat[2 : 4])   # reference elements\nprint(dat[2 : ]) \nprint(dat[:2])\ndat2 = [dat, dat]        # creating a list of lists\nprint(dat2)\nprint(dat2[1][2])        # referencing an item in a nested list\ndat3 = [dat2, \"string1\"] # mixed types\nprint(dat3)\ndat4 = dat + dat         # list concatenation\nprint(dat4)\n\n1\n[8, 10]\n[8, 10]\n[1, 4]\n[[1, 4, 8, 10], [1, 4, 8, 10]]\n8\n[[[1, 4, 8, 10], [1, 4, 8, 10]], 'string1']\n[1, 4, 8, 10, 1, 4, 8, 10]\n\n\nNow, let’s look at dictionaries.\n\ndict = {\"a\" : 1, \"b\" : 2} # Create a dictionary of two elements named a and b taking values 1 and 2 respectively\nprint(dict)\nprint(dict['a'])          # reference the element named a\n\n{'a': 1, 'b': 2}\n1\n\n\nSets and tuples are similar to lists, however with some important distinctions. Sets, contain only unique elements and tuples are immutable lists.\n\nset1 = {\"a\", \"b\", \"c\"}\nset2 = {\"a\", 1, True}\nset3 = {\"a\", \"b\", \"c\", \"c\"}\nprint(set1)\nprint(set2)\nprint(set3)\n\n{'b', 'a', 'c'}\n{1, 'a'}\n{'b', 'a', 'c'}\n\n\nHere’s an example to illustrate a tuple.\n\nlist1 = [\"a\", \"b\", \"c\"]\ntuple1 = (\"a\", \"b\", \"c\")\nlist1[0] = \"aa\" #Works just fine\nlist1\n#tuple1[0] = \"aa\" #doesn't work\n\n['aa', 'b', 'c']\n\n\n\n5.1.1 Mutable in immutable entities\nWhen working with objects in python, mutable and immutable elements act differently. Lists are mutable. So, below, the element y gets appended along with x.\n\nx = [10]\ny = x\nx.append(20)\n## Notice y has the appended element\nprint(y)\n## let's try again, as of now x = [10, 20] \nx[0] = x[0] + 11\n## Now x = [21, 20], but did y change?\nprint(y)\n\n[10, 20]\n[21, 20]\n\n\nThings like numbers and strings are immutable. Notice that changing y does not change x.\n\nx = 10\ny = x\nx = x + 10\nprint((x, y))\n\n(20, 10)"
  },
  {
    "objectID": "python_programming.html",
    "href": "python_programming.html",
    "title": "6  Python programming",
    "section": "",
    "text": "Try the following requests input from a user using the command input then it tests the value of the input using an if statement. A couple of things to take note of. First, python uses the white space instead of inclosing the statement in braces or parentheses. Secondly, don’t forget the colons after programming statements like if, else, for, def and so on. Otherwise, if/else statements work just like you’d expect.\n\n# do this if you'd like to prompt for an input\n# x = input(\"are you mean (y/n)? &gt; \")\n# Let's just assume the user input 'n'\nx = 'n'\nif x == 'y': \n print(\"Slytherine!\")\nelse:\n print(\"Gryffindor\")\n\nGryffindor\n\n\nJust to further describe white space useage in python, consider testing whether statementA is True. Below, statementB is executed as part of the if statement whereas statementC is outside of it because it’s not indented. This is often considered an eye rolling aspect of the language, but I think it’s nice in the sense that it bakes good code identation practices into the language.\n## Some more about white space\nif statementA:\n  statementB   # Executed if statementA is True\nstatementC     # Executed regardless since it's not indented\nThe generic structure of if statements in python are\nif statement1 :\n ...\nelif statement2 :\n ...\nelse \n ...\nHere’s an example (note this is just equal to the statement (a &lt; 0) - (a &gt; 0)\n\na = 5\n\nif a &lt; 0 :\n  a = -1\nelif a &gt; 0 :\n  a = 1\nelse :\n  a = 0\n\nprint(a)\n\n1\n\n\nfor and while loops can be used for iteration. Here’s some examples\n\nfor i in range(4) :\n print(i)\n\n0\n1\n2\n3\n\n\n\nx = 4\nwhile x &gt; 0 :\n x = x - 1\n print(x)\n\n3\n2\n1\n0\n\n\nNote for loops can iterate over list-like structures.\n\nfor w in 'word':\n print(w)\n\nw\no\nr\nd\n\n\nThe range function is useful for creating a structure to loop over. It creates a data type that can be iterated over, but isn’t itself a list. So if you want a list out of it, you have to convert it.\n\na = range(3)\nprint(a)\nprint(list(a))\n\nrange(0, 3)\n[0, 1, 2]"
  },
  {
    "objectID": "python_functions.html",
    "href": "python_functions.html",
    "title": "7  Functions",
    "section": "",
    "text": "Writing functions are an important aspect of programming. Writing functions helps automate redundant tasks and create more reusable code. Defining functions in python is easy. Let’s write a function that raises a number to a power. (This is unnecessary of course.) Don’t forget the colon.\n\ndef pow(x, n = 2):\n  return x ** n\n\nprint(pow(5, 3))\n\n125\n\n\nNote our function has a mandatory arugment, x, and an optional arugment, n, that takes the default value 2. Consider this example to think about how python evaluates function arguments. These are all the same.\n\nprint(pow(3, 2))\nprint(pow(x = 3, n = 2))\nprint(pow(n = 2, x = 3))\n#pow(n = 2, 3) this returns an error, the second position is n, but it's a named argument too\n\n9\n9\n9\n\n\nYou can look here, https://docs.python.org/3/tutorial/controlflow.html, to study the rules. It doesn’t make a lot of sense to get to cute with your function calling arguments. I try to obey both the order and the naming. I argue that this is the way to go since usually functions are written with some sensible ordering of arguments and naming removes all doubt. Python has a special variable for variable length arguments. Here’s an example.\n\ndef concat(*args, sep=\"/\"):\n return sep.join(args)  \n\nprint(concat(\"a\", \"b\", \"c\"))\nprint(concat(\"a\", \"b\", \"c\", sep = \":\"))\n\na/b/c\na:b:c\n\n\nLambda can be used to create short, unnamed functions. This has a lot of uses that we’ll see later.\n\nf = lambda x: x ** 2\nprint(f(5))\n\n25\n\n\nHere’s an example useage where we use lambda to make specific “raise to the power” functions.\n\ndef makepow(n):\n return lambda x: x ** n\n\nsquare = makepow(2)\nprint(square(3))\ncube = makepow(3)\nprint(cube(2))\n\n9\n8"
  },
  {
    "objectID": "data_cleaning_example.html",
    "href": "data_cleaning_example.html",
    "title": "10  Data cleaning, an example",
    "section": "",
    "text": "11 Working with the data\nLet’s get rid of the column rawid and the unnamed column since they’re kind of useless for today’s lecture. Also let’s work with only the volume.\ndf = df.drop(['Unnamed: 0', 'rawid', 'min', 'max', 'mean', 'std'], axis = 1)\nNow let’s create a column called icv for intra-cranial volume. ICV is defined as the summ of the Type I Level 1 structures and cerebrospinal fluid. For the rest of this lecture, we’re just going to look at this type and level. Here df['type'] refers to the pandas column type. You can also reference it with df.type. However, the latter has problems when the variable name has spaces or periods …\nt1l1 = df.loc[(df['type'] == 1) & (df['level'] == 1)]\n## Create a new column based on ICV\nt1l1 = t1l1.assign(icv = sum(t1l1['volume']))\nt1l1\n\n\n\n\n\n\n\n\nroi\nvolume\ntype\nlevel\nicv\n\n\n\n\n0\nTelencephalon_L\n531111\n1\n1\n1378295\n\n\n1\nTelencephalon_R\n543404\n1\n1\n1378295\n\n\n2\nDiencephalon_L\n9683\n1\n1\n1378295\n\n\n3\nDiencephalon_R\n9678\n1\n1\n1378295\n\n\n4\nMesencephalon\n10268\n1\n1\n1378295\n\n\n5\nMetencephalon\n159402\n1\n1\n1378295\n\n\n6\nMyelencephalon\n4973\n1\n1\n1378295\n\n\n7\nCSF\n109776\n1\n1\n1378295\nNow the TBV is defined as the sum of the volume for all rows except CSF.\nt1l1 = t1l1.assign(tbv = sum(t1l1['volume'][(t1l1.roi != 'CSF')]))\nt1l1\n\n\n\n\n\n\n\n\nroi\nvolume\ntype\nlevel\nicv\ntbv\n\n\n\n\n0\nTelencephalon_L\n531111\n1\n1\n1378295\n1268519\n\n\n1\nTelencephalon_R\n543404\n1\n1\n1378295\n1268519\n\n\n2\nDiencephalon_L\n9683\n1\n1\n1378295\n1268519\n\n\n3\nDiencephalon_R\n9678\n1\n1\n1378295\n1268519\n\n\n4\nMesencephalon\n10268\n1\n1\n1378295\n1268519\n\n\n5\nMetencephalon\n159402\n1\n1\n1378295\n1268519\n\n\n6\nMyelencephalon\n4973\n1\n1\n1378295\n1268519\n\n\n7\nCSF\n109776\n1\n1\n1378295\n1268519\nLet’s look at brain composition.\nt1l1 = t1l1.assign(comp = t1l1['volume'] / t1l1['tbv'])\nt1l1\n\n\n\n\n\n\n\n\nroi\nvolume\ntype\nlevel\nicv\ntbv\ncomp\n\n\n\n\n0\nTelencephalon_L\n531111\n1\n1\n1378295\n1268519\n0.418686\n\n\n1\nTelencephalon_R\n543404\n1\n1\n1378295\n1268519\n0.428377\n\n\n2\nDiencephalon_L\n9683\n1\n1\n1378295\n1268519\n0.007633\n\n\n3\nDiencephalon_R\n9678\n1\n1\n1378295\n1268519\n0.007629\n\n\n4\nMesencephalon\n10268\n1\n1\n1378295\n1268519\n0.008094\n\n\n5\nMetencephalon\n159402\n1\n1\n1378295\n1268519\n0.125660\n\n\n6\nMyelencephalon\n4973\n1\n1\n1378295\n1268519\n0.003920\n\n\n7\nCSF\n109776\n1\n1\n1378295\n1268519\n0.086539\nPandas has built in methods for plotting. Later on, we’ll try different plotting packages.\nt1l1.plot.bar(x='roi',y='comp');\nPutting the semicolon after the statement omits a message.\nIn colab, you have to install packages it doesn’t have everytime you reconnect the runtime. I’ve commented this out here, since plotly is already installed locally for me. To install in colab, use a ! in front of the unix command. In this case we’re using the python package management system pip to install plotly, an interactive graphing envinronment.\nHere the 4.0.0 is a version of plotly. If you want the latest stable release version, just omit this.\nIn a jupyter cell, you can execute a command line command by prefacing the statement with an exclamation point.\nWe can create an interactive plot with plotly. This is a professionally developed package that makes interactive plotting very easy. Also, it renders nicely within colab or jupyter notebooks. For plotly graphics, I would suggest assigning the graph to a variable then calling that variable to show the plot. This way you can modify the plot later if you’d like.\nimport plotly.express as px\nmyplot = px.bar(t1l1, x='roi', y='volume')\nmyplot.show()"
  },
  {
    "objectID": "data_cleaning_example.html#imports-and-files",
    "href": "data_cleaning_example.html#imports-and-files",
    "title": "10  Data cleaning, an example",
    "section": "10.1 Imports and files",
    "text": "10.1 Imports and files\nThe first thing we’ll try is loading some data and plotting it. To do this, we’ll need some packages. Let’s load up pandas, a package for data management, matplotlib for plotting and numpy for numerical manipulations. The python command for this is import.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\n\nwe say as in order to not have to type out the entire module name to access its methods."
  },
  {
    "objectID": "data_cleaning_example.html#reading-data-in-with-pandas",
    "href": "data_cleaning_example.html#reading-data-in-with-pandas",
    "title": "10  Data cleaning, an example",
    "section": "10.2 Reading data in with pandas",
    "text": "10.2 Reading data in with pandas\nLet’s now read in an MRICloud dataset using pandas. We want to use the function read_csv within pandas. Notice we imported pandas as pd so the command is pd.read_csv. Also, pandas can accept URLs, so we just put the link to the file in the argument. The data we want to read in is in a github repo I created.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/kirby127a_3_1_ax_283Labels_M2_corrected_stats.csv\")\n\nYou can see the variables created with locals. However, this shows you everything and you usually have to text process it a little.\nLet’s look at the first 4 rows of our dataframe. The object dataset is a pandas object with associated methods. One is head which allows one to see the first few rows of data.\n\ndf.head(4)\n\n\n\n\n\n\n\n\nUnnamed: 0\nrawid\nroi\nvolume\nmin\nmax\nmean\nstd\ntype\nlevel\n\n\n\n\n0\n1\nkirby127a_3_1_ax.img\nTelencephalon_L\n531111\n0\n374\n128.3013\n51.8593\n1\n1\n\n\n1\n2\nkirby127a_3_1_ax.img\nTelencephalon_R\n543404\n0\n300\n135.0683\n53.6471\n1\n1\n\n\n2\n3\nkirby127a_3_1_ax.img\nDiencephalon_L\n9683\n15\n295\n193.5488\n32.2733\n1\n1\n\n\n3\n4\nkirby127a_3_1_ax.img\nDiencephalon_R\n9678\n10\n335\n193.7051\n32.7869\n1\n1"
  },
  {
    "objectID": "data_sqlite.html#a-more-reaslistic-example",
    "href": "data_sqlite.html#a-more-reaslistic-example",
    "title": "11  SQL via sqlite",
    "section": "11.1 A more reaslistic example",
    "text": "11.1 A more reaslistic example\nLet’s create and work with a more realistic example. Consider the data Opiods in the US at Open Case Studies https://github.com/opencasestudies/ocs-bp-opioid-rural-urban as described here. Read over their writeup, as we’re mostly going to be showing how to duplicate a lot of their steps in sqlite.\nFirst, you need to download the data, which you could do by right clicking and saving the file or with a command:\nwget https://raw.githubusercontent.com/opencasestudies/ocs-bp-opioid-rural-urban/master/data/simpler_import/county_pop_arcos.csv\nwget https://raw.githubusercontent.com/opencasestudies/ocs-bp-opioid-rural-urban/master/data/simpler_import/land_area.csv\nwget https://raw.githubusercontent.com/opencasestudies/ocs-bp-opioid-rural-urban/master/data/simpler_import/county_annual.csv\nNext, let’s import them into sqlite\ncommand prompt&gt; sqlite3 opioid.db\nsqlite&gt; .mode csv\nsqlite&gt; .import county_pop_arcos.csv population\nsqlite&gt; .import county_annual.csv annual\nsqlite&gt; .import land_area.csv land\nsqlite&gt; .tables\nannual      land        population\nWhat variables do the tables include? The pragma command is unique to sqlite and contains a bunch of helper functions.\nsqlite&gt; pragma table_info(population);\ncid  name          type  notnull  dflt_value  pk\n---  ------------  ----  -------  ----------  --\n0                  TEXT  0                    0 \n1    BUYER_COUNTY  TEXT  0                    0 \n2    BUYER_STATE   TEXT  0                    0 \n3    countyfips    TEXT  0                    0 \n4    STATE         TEXT  0                    0 \n5    COUNTY        TEXT  0                    0 \n6    county_name   TEXT  0                    0 \n7    NAME          TEXT  0                    0 \n8    variable      TEXT  0                    0 \n9    year          TEXT  0                    0 \n10   population    TEXT  0                    0 \nsqlite&gt; pragma table_info(annual);\ncid  name          type  notnull  dflt_value  pk\n---  ------------  ----  -------  ----------  --\n0                  TEXT  0                    0 \n1    BUYER_COUNTY  TEXT  0                    0 \n2    BUYER_STATE   TEXT  0                    0 \n3    year          TEXT  0                    0 \n4    count         TEXT  0                    0 \n5    DOSAGE_UNIT   TEXT  0                    0 \n6    countyfips    TEXT  0                    0\nsqlite&gt; pragma table_info(land)\ncid  name         type  notnull  dflt_value  pk\n---  -----------  ----  -------  ----------  --\n0                 TEXT  0                    0 \n1    Areaname     TEXT  0                    0 \n2    STCOU        TEXT  0                    0 \n3    LND010190F   TEXT  0                    0 \n4    LND010190D   TEXT  0                    0 \n5    LND010190N1  TEXT  0                    0\n(I truncated this latter output at 5.)"
  },
  {
    "objectID": "data_sqlite.html#working-with-data",
    "href": "data_sqlite.html#working-with-data",
    "title": "11  SQL via sqlite",
    "section": "11.2 Working with data",
    "text": "11.2 Working with data\nLet’s print out a few columns of the population data.\nsqlite&gt; select BUYER_COUNTY, BUYER_STATE, STATE, COUNTY, year, population from population limit 5;\nBUYER_COUNTY  BUYER_STATE  STATE  COUNTY  year  population\n------------  -----------  -----  ------  ----  ----------\nAUTAUGA       AL           1      1       2006  51328     \nBALDWIN       AL           1      3       2006  168121    \nBARBOUR       AL           1      5       2006  27861     \nBIBB          AL           1      7       2006  22099     \nBLOUNT        AL           1      9       2006  55485   \nThe limit 5 prints out five rows. Let’s perform some of the tasks in the write up. For example, they want to print out some of the missing data in the annual dataset.\nsqlite&gt; select * from annual where countyfips = \"NA\" limit 10;\n     BUYER_COUNTY  BUYER_STATE  year  count  DOSAGE_UNIT  countyfips\n---  ------------  -----------  ----  -----  -----------  ----------\n188  ADJUNTAS      PR           2006  147    102800       NA        \n189  ADJUNTAS      PR           2007  153    104800       NA        \n190  ADJUNTAS      PR           2008  153    45400        NA        \n191  ADJUNTAS      PR           2009  184    54200        NA        \n192  ADJUNTAS      PR           2010  190    56200        NA        \n193  ADJUNTAS      PR           2011  186    65530        NA        \n194  ADJUNTAS      PR           2012  138    57330        NA        \n195  ADJUNTAS      PR           2013  138    65820        NA        \n196  ADJUNTAS      PR           2014  90     59490        NA        \n197  AGUADA        PR           2006  160    49200        NA   \nHere, we used the condition “NA” to test for missingness, since the CSV files have the string NA values for missing data. Places other than Puerto Rico (PR)? Lets check some\nsqlite&gt; select * from annual where countyfips = \"NA\" and BUYER_STATE != \"PR\" limit 10;\n       BUYER_COUNTY  BUYER_STATE  year  count  DOSAGE_UNIT  countyfips\n-----  ------------  -----------  ----  -----  -----------  ----------\n10072  GUAM          GU           2006  319    265348       NA        \n10073  GUAM          GU           2007  330    275600       NA        \n10074  GUAM          GU           2008  313    286900       NA        \n10075  GUAM          GU           2009  390    355300       NA        \n10076  GUAM          GU           2010  510    413800       NA        \n10077  GUAM          GU           2011  559    475600       NA        \n10078  GUAM          GU           2012  616    564800       NA        \n10079  GUAM          GU           2013  728    623200       NA        \n10080  GUAM          GU           2014  712    558960       NA        \n17430  MONTGOMERY    AR           2006  469    175390       NA     \nInspect the missing data further on your own. It looks like its the unincorporated territories and a handful of Arkansas values missing countyfips (Federal Information Processing Standard). Specifically, Montgomery county AR is missing FIPs codes. Since we want to look US states in specific, excluding territories, we will just set the Montgomery county ones to the correct value 05097 and ignore the other missing values.\nsqlite&gt; update annual set countyfips = 05097 where BUYER_STATE = \"AR\" and BUYER_COUNTY = \"MONTGOMERY\"\nsqlite&gt; select * from annual where BUYER_STATE = \"AR\" and BUYER_COUNTY = \"MONTGOMERY\"\n\n       BUYER_COUNTY  BUYER_STATE  year  count  DOSAGE_UNIT  countyfips\n-----  ------------  -----------  ----  -----  -----------  ----------\n17430  MONTGOMERY    AR           2006  469    175390       5097      \n17431  MONTGOMERY    AR           2007  597    241270       5097      \n17432  MONTGOMERY    AR           2008  561    251760       5097      \n17433  MONTGOMERY    AR           2009  554    244160       5097      \nNow lets delete rows from the annual table that have missing county data. Check on these counties before and verify that the’ve been deleted afterwards. Also, we want to grab just three columns from the land table, so let’s create a new one called land_area. Also, the column there is called STCOU, which we want to rename to coutyfips. (I’m going to stop printing out the results of every step, so make sure you’re checking your work as you go.)\nsqlite&gt; delete from annual where BUYER_COUNTY = \"NA\"\nsqlite&gt; create table land_area as select Areaname, STCOU, LND110210D from land;\nsqlite&gt; alter table land_area rename column STCOU to countyfips;\nNext we want to start joining the tables, so let’s left join our table and print out the counts to make sure we accounted correctly.\nsqlite&gt; create table county_info as select * from population left join land_area using(countyfips);\nsqlite&gt; select count(*) from land;\n3198\nsqlite&gt; select count(*) from land_area;\n3198\nsqlite&gt; select count(*) from county_info;\n28265\nsqlite&gt; select count(*) from population;"
  },
  {
    "objectID": "data_sqlite.html#notes",
    "href": "data_sqlite.html#notes",
    "title": "11  SQL via sqlite",
    "section": "11.3 Notes",
    "text": "11.3 Notes\nAt this point, hopefully you have enough of a background to finish doing the example from Open Case Studies. I have to say, that working with SQL is pleasant, but I prefer python as a home base. In addition, after working with the data, I want to use plotting and analysis tools. In the next chapter, we’ll look at using python as a base language to interact with an sqlite database."
  },
  {
    "objectID": "data_sqlite.html#sqlite-in-python",
    "href": "data_sqlite.html#sqlite-in-python",
    "title": "11  SQL via sqlite",
    "section": "11.4 sqlite in python",
    "text": "11.4 sqlite in python\nAn sqlite3 library ships with python. In this tutorial, we’ll discuss how to utilize this library and read sqlite tables into pandas. With this, you can generalize to other python APIs to other databases.\nFirst, let’s continue on with our work from the previous notebook. A nice little tutorial can be found here.\n\nimport sqlite3 as sq3\nimport pandas as pd\n\ncon = sq3.connect(\"sql/opioid.db\")\n# cursor() creates an object that can execute functions in the sqlite cursor\n\nsql = con.cursor()\n\nfor row in sql.execute(\"select * from county_info limit 5;\"):\n    print(row)\n\n    \n# you have to close the connection\ncon.close\n\n('1', 'AUTAUGA', 'AL', '01001', '1', '1', 'Autauga', 'Autauga County, Alabama', 'B01003_001', '2006', '51328', 'Autauga, AL', '594.44')\n('2', 'BALDWIN', 'AL', '01003', '1', '3', 'Baldwin', 'Baldwin County, Alabama', 'B01003_001', '2006', '168121', 'Baldwin, AL', '1589.78')\n('3', 'BARBOUR', 'AL', '01005', '1', '5', 'Barbour', 'Barbour County, Alabama', 'B01003_001', '2006', '27861', 'Barbour, AL', '884.88')\n('4', 'BIBB', 'AL', '01007', '1', '7', 'Bibb', 'Bibb County, Alabama', 'B01003_001', '2006', '22099', 'Bibb, AL', '622.58')\n('5', 'BLOUNT', 'AL', '01009', '1', '9', 'Blount', 'Blount County, Alabama', 'B01003_001', '2006', '55485', 'Blount, AL', '644.78')\n\n\n&lt;function Connection.close()&gt;"
  },
  {
    "objectID": "data_sqlite.html#reading-into-pandas",
    "href": "data_sqlite.html#reading-into-pandas",
    "title": "11  SQL via sqlite",
    "section": "11.5 Reading into pandas",
    "text": "11.5 Reading into pandas\nLet’s read our sqlite database into pandas. At this point, we can then work on the dataset entirely in pandas. This is closest to how I work. I’m typically more comfortable working in R or python and so get my data out of database formats and into tidyverse or pandas formats as soon as I can.\n\ncon = sq3.connect(\"sql/opioid.db\")\n\ncounty_info = pd.read_sql_query(\"SELECT * from county_info\", con)\n\n# you have to close the connection\ncon.close\n\ncounty_info.head\n\n&lt;bound method NDFrame.head of                BUYER_COUNTY BUYER_STATE countyfips STATE COUNTY  \\\n0          1        AUTAUGA          AL      01001     1      1   \n1          2        BALDWIN          AL      01003     1      3   \n2          3        BARBOUR          AL      01005     1      5   \n3          4           BIBB          AL      01007     1      7   \n4          5         BLOUNT          AL      01009     1      9   \n...      ...            ...         ...        ...   ...    ...   \n28260  28261       WASHAKIE          WY      56043    56     43   \n28261  28262         WESTON          WY      56045    56     45   \n28262  28263        SKAGWAY          AK      02230     2    230   \n28263  28264  HOONAH ANGOON          AK      02105     2    105   \n28264  28265     PETERSBURG          AK      02195     2    195   \n\n         county_name                               NAME    variable  year  \\\n0            Autauga            Autauga County, Alabama  B01003_001  2006   \n1            Baldwin            Baldwin County, Alabama  B01003_001  2006   \n2            Barbour            Barbour County, Alabama  B01003_001  2006   \n3               Bibb               Bibb County, Alabama  B01003_001  2006   \n4             Blount             Blount County, Alabama  B01003_001  2006   \n...              ...                                ...         ...   ...   \n28260       Washakie           Washakie County, Wyoming  B01003_001  2014   \n28261         Weston             Weston County, Wyoming  B01003_001  2014   \n28262        Skagway       Skagway Municipality, Alaska  B01003_001  2014   \n28263  Hoonah Angoon  Hoonah-Angoon Census Area, Alaska  B01003_001  2014   \n28264     Petersburg         Petersburg Borough, Alaska  B01003_001  2014   \n\n      population           Areaname LND110210D  \n0          51328        Autauga, AL     594.44  \n1         168121        Baldwin, AL    1589.78  \n2          27861        Barbour, AL     884.88  \n3          22099           Bibb, AL     622.58  \n4          55485         Blount, AL     644.78  \n...          ...                ...        ...  \n28260       8444       Washakie, WY    2238.55  \n28261       7135         Weston, WY    2398.09  \n28262        996        Skagway, AK     452.33  \n28263       2126  Hoonah-Angoon, AK    7524.92  \n28264       3212     Petersburg, AK    3281.98  \n\n[28265 rows x 13 columns]&gt;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data science and AI for the biological and medical sciences using python",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "data_advanced_databases.html#blockwise-basic-statistical-calculations",
    "href": "data_advanced_databases.html#blockwise-basic-statistical-calculations",
    "title": "12  Big data storage",
    "section": "12.1 Blockwise basic statistical calculations",
    "text": "12.1 Blockwise basic statistical calculations\nNow, consider taking the mean of both variables. Imagine that the time series is so long it’s not feasible to load into memory. So, we want to read it in blocks. You want your blocks to be as big as possible, since that’s fastest. In our case, of course, none of this is necessary.\nOur goal in this section is to do the following: calculate the empirical mean and variance for each sensor, center and scale each sensor, and write those changes to those variables, calculate the sample correlation then calculate the residual for sensor1 given sensor2. (I think typically you wouldn’t want to overwrite the original data; but, this is for pedagogical purposes.) We want our data organized so sensors are stored in a hierarchical “folder” called sensors and processed data is in a different folder.\nWe’re just simulating iid standard normals. So, we have a rough idea of the answers we should get, since the the data are theoretically mean 0, variance 1 and uncorrelated. After our calculations, they will have empirical mean 0 and variance 1 and the empirical correlation between the residual and sensor 2 will be 0.\nLet’s consider a block variation of the inner product. \\[\n&lt;a, b&gt; = \\sum_{i=0}^{n-1} a_i b_i = \\sum_{i=0}^{n/B} \\sum_{j=0}^{B-1} a_{j + i B} b_{j + i B}\n\\] (if \\(n\\) is divisible by \\(B\\). Otherwise you have to figure out what to do with the final block, which isn’t hard but makes the notation messier.) So, for example, the (sample) mean is then \\(&lt;x, J&gt;/n\\) where \\(J\\) is a vector of ones.\nLet’s calculate the mean using blockwise calculations.\n\nn = s1.shape[0]\nB = 32\n## mean center the blocks\nmean1 = 0\nmean2 = 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    mean1 += s1[block_indices].sum() / n \n    mean2 += s2[block_indices].sum() / n\n\n[mean1, mean2]\n\n[-0.022212967172872494, -0.00825148352928065]\n\n\nLet’s now center our time series.\n\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    s1[block_indices] -= mean1  \n    s2[block_indices] -= mean2\n\nNow the (unbiased, sample) variance of centered vector \\(a\\) is simply \\(&lt;a, a&gt;/(n-1)\\).\n\nv1, v2 = 0, 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    v1 += np.sum(s1[block_indices] ** 2) / (n - 1)\n    v2 += np.sum(s2[block_indices] ** 2) / (n - 1)\n[v1, v2]\n\n[0.958662426534898, 1.0333468113266224]\n\n\nNow let’s scale our vectors as\n\nsd1 = np.sqrt(v1)\nsd2 = np.sqrt(v2)\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    s1[block_indices] /= v1  \n    s2[block_indices] /= v2\n\nNow that our vectors are centered and scaled, the empirical correlation is simply \\(&lt;a, b&gt;/(n-1)\\). Let’s do that\n\ncor = 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    cor += np.sum(s1[block_indices] * s2[block_indices]) / (n-1) \ncor\n\n0.014622420886487464\n\n\nFinally, we want to “regress out” s2 from s1. Since we normalized our series, the correlation is slope coefficient from linear regression (regardless of the outcome and dependent variable) and the intercept is zero (since we centered). Thus, the residual we want is \\(e_{12} = s_1 - \\rho s_2\\) where \\(\\rho\\) is the correlation.\n\nf['processed/resid_s1_s2'] = np.empty(n)\ne12 = f['processed/resid_s1_s2']\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    e12[block_indices] += s1[block_indices] - cor * s2[block_indices] \n\nNow we have our new processed data stored in a vector. To close our database simply do:\n\nf.close()\n\nNow our processed data is stored on disk.\n\nf = h5py.File('sensor.hdf5', 'r')\nf['processed/resid_s1_s2']\n\n&lt;HDF5 dataset \"resid_s1_s2\": shape (1024,), type \"&lt;f8\"&gt;\n\n\n\nf.close()"
  },
  {
    "objectID": "data_advanced_databases.html#homework",
    "href": "data_advanced_databases.html#homework",
    "title": "12  Big data storage",
    "section": "12.2 Homework",
    "text": "12.2 Homework\n\nPerform lots of regressions. Suppose that you have a setting where you would like to perform the operation \\[\n(X'X)^{-1} X' Y\n\\] where \\(X\\) is \\(n\\times p\\) and \\(Y\\) is \\(n\\times v\\). Consider the case where \\(Y\\) is very large (so \\(V\\) is large). Simulate some data where you perform this linear model in block calculations.\nWrite a block matrix multiplication program that takes in two matrices with agreeable dimensions stored as HDF5 and multiplies them in block sizes specified by the user."
  },
  {
    "objectID": "statistics_ml.html#linear-regression",
    "href": "statistics_ml.html#linear-regression",
    "title": "23  Maximum Likelihood",
    "section": "23.1 Linear regression",
    "text": "23.1 Linear regression\nYou might be surprised to find out that linear regression can also be cast as a likelihood problem. Consider an instance where we assume that the \\(Y_i\\) are Gaussian with a mean equal to \\(\\beta_0 + \\beta_1 x_i\\) and variance \\(\\sigma^2\\). What that means is that the probability that \\(Y_i\\) lies betweens the points \\(A\\) and \\(B\\) is governed by the equation\n\\[\nP(Y_i \\in [A, B) ~|~ x_i) = \\int_A^B \\exp\\left\\{ -(y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2 \\right\\} dy_i\n\\]\nLetting \\(A=-\\infty\\) and taking the derivative with respect to \\(B\\), we obtain the density function, sort of the probability on an infintessimally small interval:\n\\[\n\\exp\\left\\{ -(y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2 \\right\\}\n\\]\nUses the density evaluated at the observed data, the joint likelihood assuming independence is:\n\\[\n\\prod_{i=1}^n \\exp\\left\\{ -(y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2 \\right\\}\n= \\exp\\left\\{ -\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2 \\right\\}\n\\]\nSince it’s more convenient to deal with logs we get that the joint log likelihood is\n\\[\n- \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 / 2\\sigma^2\n\\]\nSince minimizing the negative is the same as maximizing this, and the constants of proportionality are irrelevant for maximizing for \\(\\beta_1\\) and \\(\\beta_0\\), we get that maximum likelihood for these parameters minimizes\n\\[\n\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2\n\\]\nwhich is the same thing we minimized to obtain our least squares regression estimates."
  },
  {
    "objectID": "statistics_causal.html#associational-models-versus-causal-models",
    "href": "statistics_causal.html#associational-models-versus-causal-models",
    "title": "24  Causal DAGs",
    "section": "24.1 Associational models versus causal models",
    "text": "24.1 Associational models versus causal models\nCausal models differ from associational models in that they codify causal directions not just associations. In our program, you might have learned of the use of propensity scores, counterfactuals or randomization to study causality. There, typically the goal is to make causal statements with as few assumptions as possible or at least understanding the assumptions. Typically, the object of study is the estimation of an effect avergated over covarites.\nCausal graphs take a different approach, even if they wind up at the same place. Here, the goal is to postulate hyptothetical causal relationships and use those hypothetical relationships to estimate causal effects.\n\n24.1.1 Graphs and graphical models\nA graph, \\(G\\) is a collection of nodes, say \\(V=\\{1,\\ldots, p\\}\\) and a set edges between the nodes, i.e. a set of elements \\((i,j)\\). The graph is directed if \\((i,j)\\) is considered different then \\((j,i)\\).\nNode \\(i\\) is a parent of node \\(j\\) if \\((i,j) \\in E\\) and \\((j,i)\\notin E\\). Similarly, node \\(i\\) is a child of node \\(j\\) if \\((j,i) \\in E\\) and \\((i,j)\\notin E\\). A node is a descendant of another if it is a child, or a child of a child and so on.\n\n\n24.1.2 DAG and SCMs\nDAGs define a unique factorization (set of independence relationships) with compatible probability models. I find it useful to think of causal DAGs in the terms of structural causal models (SCMs). Such models demonstrate an example of a generative models that statisfy the DAG and the have clear connections with the probabability models. An SCM over a collection of variables, \\(X=(X_1, \\ldots, X_p)\\), postulates a set of functional relationships \\[\nX_j = f(P_j, \\epsilon_j)\n\\] where \\(P_j\\) are the antecedent causes of \\(X_j\\), called the parents of \\(X_j\\), and \\(\\epsilon_j\\) is an accumulation of variables treated as mutally independent. This defines a directed graph, \\(G\\) say, where a graph is collection of vertices corresponding to our variables, \\(V=\\{1,\\ldots, p\\}\\), corresponding to the \\(X_i\\), and edges, \\(E\\), which is a set of ordered pairs of nodes.\n\n\n\n\n\nIn this case, \\(P_1 = \\{\\}\\), \\(P_2 = \\{1\\}\\) and \\(P_3 = \\{1,2\\}\\). DAGs in general define the independence assumptions associated with compatible probability models. SCMs are such an example that clearly define compatible probability models. Of course, given a large enough cross-sectional sample, we can estimate the joint distribution of \\(P(X_1,\\ldots, X_p)\\) and all of its conditionals. Disregarding statistical variation, which can be accounted for using traditional inferential methods, these conditionals should agree with the independence relationships from the DAG, if the DAG is correct. This yields a fruitful way to consider probability models. For example one could use DAGs as a heuristic and see how the observed data agrees with the independence relationships in compatible probability models implied by the DAG.\nBy itself, this does not create any causal claims. However, the following strategy could. Postulate a causal model, like the SCM, consider the independence relationships implied by the SCM, compares those indepnence relationships with those seen in the observed data. This gives us a method to falsify causal models using the data.\nOne specific way in which we use the assumptions is to investigate how the graph changes when we fix a node at a specific value, like an intevention, thus breaking its association with its parents. This operation is conceptual, but at times we can relate probabilities associated with interventions that were not realized. Consider an instrance where where \\(X_1\\) is a collection of confounders, \\(X_2\\) is an exposure and \\(X_3\\) is an outcome. Ideally, we’d like to know \\[\nP(X_3 ~|~ do(X_2) = x_2)\n\\] That is, the impact on the response if we were to set the exposure to \\(e_0\\).\n\n\n24.1.3 Blocking and d-separation\nBefore we talk about interventions, let’s consider discussing compatibility of the hypothetical directed graph and our observed data. Return to our previous diagram.\n\n\\(X1\\) is a confounder betweend \\(X2\\) and \\(X3\\)\n\\(X2\\) is a mediator between \\(X1\\) and \\(X3\\)\n\\(X3\\) is a collider between \\(X1\\) and \\(X2\\)\n\nConsider an example. \\(X1\\) is having a BMI &gt; 35, \\(X2\\) is sleep disordered breathing and \\(X3\\) is hypertension.\n\n\n\n\n\nHere if we’re studying whether SDB causes HTN, BMI35 confounds the relationship as a possible common cause of both. We would need to adjust for BMI35 to make sure the association between SDB and HTN isn’t just due to this common cause.\nIf we were studying whether BMI35 causes HTN, we might be interested in how much of that effect is mediated (indirectly) through SDB and how much is directly from BMI35.\nIf we are studying the relationship between BMI35 and SDB directly, adjusting for HTN may cause an association. Consider the (fictitious) case where there is a large number of people who have SDB who are not obese, yet all have hypertension, for whatever the reason. Then, among the HTN, there could be a negative association between BMI35 and SDB, because of the large collection of patients would who have SDB and are not obese and the same for obese and not hyptertensive. That is, adjusting for HTN created an association. This is an example of Berkson’s paradox. This is a somewhat contrived example, but hopefully you get the point. The wikipedia article has a funny example where they consider \\(X_1\\) is whether or not the hamburger is good at a fast food restaurant, \\(X_2\\) is whether or not the fries are good and \\(X_3\\) is whether or not people eat there. Since few people would eat at a place where both the hamburger and fries are bad, conditioning on \\(X_3\\) can create a negative association.\nThe main point here is that adjusting for colliders may open up a pathway between the nodes.\nA path between two nodes \\(n_1\\) and \\(n_k\\) is a sequence of nodes, \\(v_1, v_2,\\ldots v_{k}\\), where \\(v_{i}\\) and \\(v_{i+1}\\) are connected. The path is directed if \\(v_{i}\\) points to \\(v_{i+1}\\) for \\(i=1,\\ldots,k\\). A graph is a Directed Acyclic Graph (DAG) if all edges are directed and there are no two nodes \\(v_i\\) and \\(v_j\\) with a directed path in both directions.\nA path between \\(v_1\\) and \\(v_k\\), \\(v_1,\\ldots, v_k\\), is blocked by a set of nodes, \\(S\\), if for some \\(v_j\\) in \\(S\\)\n\n\\(v_j\\in S\\) and \\(v_k\\) is a mediator or confounder between \\(v_{j-1}\\) and \\(v_{j+1}\\) in either direction or\n\\(v_j\\notin S\\) and all of the descendants of \\(v_j \\notin S\\) and \\(v_{j}\\) is a collider between \\(v_{j-1}\\) and \\(v_{j+1}\\).\n\nIn other words, a path is blocked if a mediator or confounder is included in \\(S\\) or a collider and all of its descendants is excluded from \\(S\\).\nFor 1. this is equivalent to saying one of\n\n\\(v_{j-1}\\rightarrow v_{j} \\rightarrow v_{j+1}\\)\n\\(v_{j-1}\\leftarrow v_{j} \\leftarrow v_{j+1}\\)\n\\(v_{j-1}\\leftarrow v_{j} \\rightarrow v_{j+1}\\)\n\nholds. For 2. recall a collider is \\(v_{j-1}\\rightarrow v_{j} \\leftarrow v_{j+1}\\).\nThis could be translated into the following statistical statement. Conditioning on a mediator or confounder or not conditioning on a collider blocks a path, conditioning on a collider opens a path.\nWe say that two nodes or groups of nodes are d-separated by a set of nodes, \\(S\\), if every path between nodes in the two groups is blocked by \\(S\\). d-separation is useful because it gives us conditional independence relationships in the sense that if \\(X_i\\) is d-separated with \\(X_j\\) given \\(S\\) then \\(X_i \\perp X_j ~|~ S\\) on all probability distribution compatible with the graph.\nConsider the following graph.\n\n\n\n\n\n\\(X_1\\) and \\(X_5\\) are conditionally indepndent given \\(X_2\\) and \\(X_3\\). Why? Conditioning on \\(X_2\\) blocks the paths \\(X_1 \\leftarrow X_2 \\rightarrow X_3 \\leftarrow X_5\\) even despite the part \\(X_2 \\rightarrow X_3 \\leftarrow X_5\\) is opened by conditioning on the collider, \\(X_3\\). Furthermore, conditioning on \\(X_2\\) or \\(X_3\\) blocks the path \\(X_1 \\leftarrow X_2 \\rightarrow X_3 \\rightarrow X_4 \\rightarrow X_6 \\leftarrow X_5\\). Finally, conditioning on \\(X_3\\) blocks the path \\(X_1 \\leftarrow X_3 \\leftarrow X_5\\).\nAnother interesting one is that \\(X_2\\) and \\(X_5\\) are marginally independent. This is because not conditioining on \\(X_3\\) blocks the path \\(X_2 \\rightarrow X_3 \\leftarrow X_5\\) and not conditioning on \\(X_6\\) blocks the path \\(X_2 \\rightarrow X_3 \\rightarrow X_4 \\rightarrow X_6 \\leftarrow X_5\\).\nHere’s the complete set of conditional independence relationships.\n\n\\(X_1\\) and \\(X_4\\) are d-separated by \\(\\{X_3\\}\\)\n\\(X_1\\) and \\(X_5\\) are d-separated by \\(\\{X_2, X_3\\}\\)\n\\(X_1\\) and \\(X_6\\) are d-separated by \\(\\{X_4, X_5\\}\\), \\(\\{X_3, X_5\\}\\), \\(\\{X_2, X_3\\}\\)\n\\(X_2\\) and \\(X_4\\) are d-separated by \\(\\{X_3\\}\\)\n\\(X_2\\) and \\(X_5\\) are d-separated by \\(\\{\\}\\) (the null set, i.e. they’re marginally independent).\n\\(X_2\\) and \\(X_6\\) are d-separated by \\(\\{X_4, X_5\\}\\), \\(\\{X_3, X_5\\}\\)\n\\(X_3\\) and \\(X_6\\) are d-separated by \\(\\{X_4, X_5\\}\\)\n\\(X_4\\) and \\(X_5\\) are d-separated by \\(X_3\\).\n\nThese all imply the independence relationships, such as \\(X_1 \\perp X_4 ~|~ X_3\\)."
  },
  {
    "objectID": "statistics_causal.html#do-calculus-and-backdoor-criterion",
    "href": "statistics_causal.html#do-calculus-and-backdoor-criterion",
    "title": "24  Causal DAGs",
    "section": "24.2 Do calculus and backdoor criterion",
    "text": "24.2 Do calculus and backdoor criterion\nRecall that specifying a causal graph implies the independence relationships of a probability distribution under assumptions such as the SCM. Sometimes, we’re interested in the causal relationship between an exposure, \\(X\\) and an outcome, \\(Y\\). Consider a theoretical intervention obtained by setting \\(X = a\\), which we write as \\(do(X) = a\\). We want to estimate \\(P(Y ~|~ do(X) = a)\\).\nA set \\(Z\\) satisfies the back door criterion with respect to nodes \\(X\\) and \\(Y\\) if\n\nNo descendant of \\(X\\) is in \\(Z\\).\n\\(Z\\) blocks every path \\(X\\) and \\(Y\\) that contains an arrow pointing to \\(X\\).\n\nThe back door criteria is similar to d-separation. However, we only focus on arrows pointing into \\(X\\) and don’t allow for descendants of \\(X\\).\nThe magic of the back door adjustment comes from the relationship, the adjustment formula:\n\\[\nP(Y ~|~ do(X) = x) = \\sum_{z\\in S} P(y ~ | x, z) p(z)\n\\]\nwhere \\(S\\) satisfies the back door criterion. If the \\(z\\) are all observed variables, then the RHS of this equation is estimable. Note the interesting statement that not all variables need to be observed, just \\(y\\), \\(x\\) and \\(z\\).\nSo, in our previous example, adjusting for \\(S = \\{X_2, X_3\\}\\) allows us to estimate the causal effect of \\(X\\) on \\(Y\\), even if \\(X_4\\) and \\(X_5\\) are not measured.\nIt’s important to emphasize, that every aspect of the adjustment formula is theoretically estimable if \\(Y\\), \\(X\\) and the nodes in \\(S\\) are observed.\nConsider the following graph.\n\n\n\n\n\nHere are the minimal backdoor adjustment variables between \\(X\\) and \\(Y\\):\n\n\\(S = \\{X_2, X_3\\}\\)\n\\(S = \\{X_3, X_5\\}\\)\n\\(S = \\{X_4, X_5\\}\\)\n\nHere are some invalid backdoor sets of variables.\n\n\\(S\\) equal to any single node.\n\n\\(S=\\{X_3\\}\\) does not block the path \\(X\\leftarrow X_2 \\rightarrow X_3 \\leftarrow X_5 \\rightarrow Y\\).\n\\(S=\\{X_4\\}\\) or \\(S=\\{X_2\\}\\) does not block the path \\(X \\leftarrow X_3 \\leftarrow X_5 \\rightarrow Y\\).\n\\(S=\\{X_5\\}\\) does not block the path \\(X \\leftarrow X_3 \\leftarrow X_4 \\rightarrow Y\\).\n\n\\(S=\\{X_3, X_4\\}\\) does not block the path \\(X \\leftarrow X_2 \\rightarrow X_3 \\leftarrow X_5 \\rightarrow Y\\).\n\\(S=\\{X_2, X_4\\}\\) does not block the path \\(X\\leftarrow X_3 \\leftarrow X_5 \\rightarrow Y\\).\n\n\n24.2.1 Example graphs\nIn all the following, we’re interested in the causal effect of \\(X\\) and \\(Y\\). \\(Z\\) variables are observed and \\(U\\) variables are unobserved. Every variable is binary to make the discussion easier.\n\n24.2.1.1 Randomization\nIf \\(X\\) is randomized and everyone takes the treatment assigned to them (left plot) then \\(X\\) has no parents other than the randomization mechanism,\\(R\\). We’re omitting any descendants of \\(X\\) since we don’t have to worray about them. Regardless of the complexity of the relationship between the collection of observed, unobserved, known and unknown variables, \\(Z, U\\), and \\(Y\\) we can estimate the causal effect simply without conditioning on anything.\nIn contrast, if some people ignore their randomized treatment status and elect to choose a different treatment one may have opened a backdoor path (right plot). For example, if the treatment can’t be blinded and those randomized to the control with the worst baseline symptoms elect to obtain the treatment elsewhere.\n\n\n\n\n\n\n\n24.2.1.2 Simple confounding\nThe diagram below shows classic confounding. Conditioning in \\(Z\\) allows for the estimation of the causal effect.\n\n\n\n\n\nNow the estimate of the adjusted effect (under our assumptions) is\n\\[\nP(Y ~|~ do(X) = x) = P(Y ~|~ X=x, z = 0)P(z = 0) + P(Y ~|~ X=x, Z=1)P(Z=1)\n\\]\nIn the following two examples, the unmeasured confounder \\(U\\) can be controlled for by conditioning on \\(Z\\) and exactly the same estimate can be used as in the simple confounding model.\n\n\n(-0.3, 1.3)\n\n\n\n\n\n\n\n24.2.1.3 Mediation\nIn mediation, all or part of the effect of \\(X\\) on \\(Y\\) flows through yet another variable \\(Z\\).\n\n\n\n\n\nThe backdoor criteria does not apply here, since \\(Z\\) is a descendant of \\(X\\). To answer the question “What is the causal effect of \\(X\\) on \\(Y\\)?” one need not adjust. However, mediation is typically studied in a different way. Instead, one asks questions such as “How much of the effect of \\(X\\) on \\(Y\\) flows or doesn’t flow through \\(Z\\)?”. To answer this question, one usually conditions on \\(Z\\) for a different goal than the backdoor adjustment is accomplishing.\n(cinelli2021crash?) shows an interesting example of mediation where one would want to adjust for \\(Z\\) (left plot below).\n\n\n(-0.3, 1.3)\n\n\n\n\n\nIn this case, \\(M\\) still mediates the relationship between \\(X\\) and \\(Y\\). However, \\(Z\\) is in a backdoor path to \\(M\\). So, some of the variation in \\(M\\) that impacts \\(Y\\) could be due to \\(Z\\) rather than \\(X\\). The right plot is similar and makes the point more explicit. \\(Z\\) confounds the relationship between \\(X\\) and \\(Y\\) through \\(M\\). Without adjusting for \\(Z\\), the path \\(X\\leftarrow Z \\rightarrow M \\rightarrow Y\\) remains unblocked.\n\n\n24.2.1.4 Bad controls\nThe following are all unhelpful for conditioning on \\(Z\\) using the backdoor criteria.\nUpper left. Adjusting for colliders is the standard bad control. Below adjusting for \\(Z\\) open ups a backdoor path that was closed. From a common sense perspective, why would you want to adjust for a consequence of \\(X\\) and \\(Y\\) when exploring their relationship?\nIn the upper right diagram below, \\(Z\\) is a so-called instrumental variable. A good example is \\(Z\\) being the randomization indicator and \\(X\\) being the treatment the person actually took. It is important in this example to emphasize that use of the instrumental variable is often a very fruitful method of analysis. However, it’s not a useful backdoor adjustment and conditioning on \\(Z\\) simply removes most of the relevant variation in \\(X\\). If one wants to use \\(Z\\) as an instrumental variable in this setting, then specific methods taylored to instrumental variable use need to be employed.\nIn the lower left plot, \\(Z\\) is a descendant of \\(X\\). Conditioning on \\(Z\\) removes relevant pathway information regarding the relationship between \\(X\\) and \\(Y\\)&gt;\nThe lower right plot is similar. Conditioning on \\(Z\\) removes variation in \\(M\\) which hinders our ability to study the relationship between \\(X\\) and \\(Y\\) through \\(M\\).\n\n\n\n\n\n\n\n24.2.1.5 Conditioning may help\nIn the upper left plot, adjusting for \\(Z\\) may reduce variability in \\(Y\\) to help focus on the relationship between \\(X\\) and \\(Y\\).\nIn the upper left plot, adjusting for \\(Z\\) may reduce variation in the mediator unrelated to the relationship between \\(X\\) and \\(Y\\).\n\n\n(-0.3, 1.3)"
  },
  {
    "objectID": "statistics_causal.html#exercises",
    "href": "statistics_causal.html#exercises",
    "title": "24  Causal DAGs",
    "section": "24.3 Exercises",
    "text": "24.3 Exercises\n\n24.3.1 Graphs\nConsider the following graph where we want to answer the question: what is \\(P(Y ~|~ do(X) = x)\\) where every variable is binary.\n\n\n(-0.3, 1.3)\n\n\n\n\n\n\nWhat are the minimal set of adjustment variables for the backdoor criteria?\nIs \\(X \\perp Y ~|~ Z_1, Z_2\\)?\nIs \\(X \\perp Y ~|~ Z_2, Z_3\\)?\nGiven a cross sectional sample, if \\(Z_3\\) is unobserved, give a formula for the estimation of \\(P(Y ~|~ do(X) = x)\\) that only requires observed variables.\n\n\n\n\n\n\n\nWhat are the minimal set of adjustment variables for the backdoor criteria?\nGiven a cross sectional sample, give a formula for the estimation of \\(P(Y ~|~ do(X) = x)\\) that only requires observed variables.\n\n\n\n24.3.2 Data exercise\nThe wikipedia page on Simpson’s paradox gives this data concerning two treatments of kidney stones, the percentage of succcessful procedures and the size of the stone. Note, among both large stones and small stones A is better than B. However, among all B is preferable to A.\n\n\n\nSize\nTreatment\nSuccess\nN\nProp\n\n\n\n\nLarge\nA\n192\n263\n73%\n\n\n\nB\n55\n80\n69%\n\n\nSmall\nA\n81\n87\n93%\n\n\n\nB\n234\n270\n87%\n\n\nBoth\nA\n273\n350\n78%\n\n\n\nB\n289\n350\n83%\n\n\n\nEstimate the treatment effect difference: \\[\nP(Success ~|~ Do(Treatment) = B)\n- P(Success ~|~ Do(Treatment) = A)\n\\] under the following graphical models where \\(X\\) is treatment, \\(Y\\) is success and \\(Z\\) is stone size:\n\n\n\n\n\nComment on how reasonable each of these models are given the setting. Here’s a reference: (julious1994confounding?).\nGive any other DAGs, perhaps including unmeasured variables, that you think are relevant."
  },
  {
    "objectID": "statistics_causal.html#reading",
    "href": "statistics_causal.html#reading",
    "title": "24  Causal DAGs",
    "section": "24.4 Reading",
    "text": "24.4 Reading\n\nThe definitive causal reference is (pearl2009causality?).\nI got a lot of this stuff from (peters2017elements?), which you can read here\nAlso read (hardt2021patterns?), which you can read here\nA crash course in good and bad controls, or here\ndagitty"
  },
  {
    "objectID": "supervised_binary_classification.html",
    "href": "supervised_binary_classification.html",
    "title": "25  Introduction to binary classification",
    "section": "",
    "text": "26 Classification\nLet’s try creating the simplest possible classifier, a threshold. So here we want to pick the value of the threshold so that lower values are classified GOLD_Lesion == 0 (i.e. no lesion) and higher values are GOLD_Lesion == 1 (lesion at this voxel). We want to do this on labeled voxels so that we can pick a meaningful threshold on voxels without a gold standard labeling. That is, for new patients we want to automatically label their images one voxel at a time with a simple thresholding rule. We’re going to use our training data where we know the truth to develop the threshold.\nNote the idea behind doing this is only useful if the new images without the gold standard are in the same units as the old one, which is not usually the case for MRIs. The technique for trying to make images comparable is called normalization.\nLet’s first just try each of the datapoints itself as a threshold and pick which one does best. However, I’m going to break the data into a training and testing set. The reason for this is that I want to make sure that I don’t overfit. That is, we’re going to test our algorithm on a dataset that wasn’t used to train the algorithm.\nx = dat.FLAIR\ny = dat.GOLD_Lesions\nn = len(x)\ntrainFraction = .75\n\n## Build a training and testing set\n## Prob of being in the train set is trainFraction\nsample = np.random.uniform(size = n) &lt; trainFraction\n\n## Get the training and testing sets\nxtrain = x[ sample]\nytrain = y[ sample]\nxtest =  x[~sample]\nytest =  y[~sample]\n## Starting values, just set it to \n## 0 so that it improves on the first\n## try\nbestAccuracySoFar = 0\n\nfor t in np.sort(xtrain):\n  ## Strictly greater than the threshold is\n  ## our algorithm\n  predictions = (xtrain &gt; t)\n  accuracy = np.mean(ytrain == predictions)\n  if (accuracy &gt; bestAccuracySoFar):\n    bestThresholdSoFar = t \n    bestAccuracySoFar = accuracy \n\nthreshold = bestThresholdSoFar\nNow let’s test our our “algorithm”, on the test set. We’ll look at the test set accuracy, but also how it breaks up into the sensisitivity and specificity."
  },
  {
    "objectID": "supervised_binary_classification.html#definitions",
    "href": "supervised_binary_classification.html#definitions",
    "title": "25  Introduction to binary classification",
    "section": "26.1 Definitions",
    "text": "26.1 Definitions\ntest set accuracy = proportion of correct classifications on the test data\ntest set sensitivity = proportion declared diseased among those that are actually diseased. (In this case lesion = disease)\ntest set specificity = proportion declared not diseased among those that are actually not diseased.\nTo interpret the sensitivity and specificity, imagine setting the threshold nearly to zero. Then we’ll declare almost every voxel a lesion and we’ll have nearly 100% sensitivity and nearly 0% specificity. If we declare a voxel as a lesion it’s not that interesting. If we declare a voxel as not lesions, then it’s probably not a lesion.\nIf we set the threshold really high, then we’ll have nearly 0% sensitivity and 100% specificity. If we say a voxel is not lesioned, it’s not that informative, since we declare nearly everything not a lesion. But if we declare a voxel a lesion, it usually is.\nSo, if you have a high sensitivity, it’s good for ruling diseases out. If you have a high specificity it’s good for ruling diseases in. If you have a high both? Then you have a very good test.\n\n## Let's test it out on the test set\ntestPredictions = (xtest &gt; threshold)\n\n## The test set accuracy\ntestAccuracy = np.mean(testPredictions == ytest)\n\n## Let's see how it specifically does on the\n## set of instances where ytest == 0 and ytest == 1\n## The % it gets correct on ytest == 0 is called\n## the specificity and the percent correct when \n## ytest == 1 is called the sensitivity.\nsub0 = ytest == 0\nsub1 = ytest == 1\n\ntestSpec = np.mean(ytest[sub0] == testPredictions[sub0])\ntestSens = np.mean(ytest[sub1] == testPredictions[sub1])\n\npd.DataFrame({\n 'Threshold': threshold,\n 'Accuracy': testAccuracy, \n 'Specificity': testSpec, \n 'Sensitivity': testSens}, index = [0])\n\n\n\n\n\n\n\n\nThreshold\nAccuracy\nSpecificity\nSensitivity\n\n\n\n\n0\n1.907889\n0.586207\n0.9375\n0.153846\n\n\n\n\n\n\n\n\nsns.kdeplot(x0, shade = True, label = 'Gold Std = 0')\nsns.kdeplot(x1, shade = True, label = 'Gold Std = 1')\nplt.axvline(x=threshold)\n            \nplt.show()\n\n/tmp/ipykernel_73405/2742163068.py:1: FutureWarning:\n\n\n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n\n/tmp/ipykernel_73405/2742163068.py:2: FutureWarning:\n\n\n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n\n\n\n\n\n\nOK, so out plot has better sensitivity than specificity and a test set accuracy of around 68%. The lower specificity is because there’s a lower percentage of blue below the line than orange above the line. Recall, we’re saying above the threshold is a lesion and orange is the distribution for voxels with lesions.\nSo, for this algorithm, the high sensitivity says that all else being equal, if you declare a voxel as not being a lesion, it probably isn’t. In other words, if you’re out in the lower part of the orange distribution, there’s a lot of blue there.\nHowever, all else isn’t equal. Most voxels aren’t lesions. This factors into our discussion in a way that we’ll discuss later.\n\nfpr, tpr, thresholds = roc_curve(ytest, xtest)\nroc_auc = auc(fpr, tpr)\n\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "supervised_regression_origin.html",
    "href": "supervised_regression_origin.html",
    "title": "26  Regression through the origin",
    "section": "",
    "text": "In this notebook, we investigate a simple poblem where we’d like to use one scaled regressor to predict another. That is, let \\(Y_1, \\ldots Y_n\\) be a collection of variables we’d like to predict and \\(X_1, \\ldots, X_n\\) be predictors. Consider minimizing\n\\[\nl = \\sum_i ( Y_i - \\beta X_i)^2 = || Y - \\beta X||^2.\n\\]\nTaking a derivative of \\(l\\) with respect to \\(\\beta\\) yields\n\\[\nl' = - \\sum_i 2 (Y_i - \\beta X_i) X_i.\n\\]\nIf we set this equal to zero and solve for beta we obtain the classic solution:\n\\[\n\\hat \\beta = \\frac{\\sum_i Y_i X_i}{\\sum_i X_i^2} = \\frac{&lt;Y, X&gt;}{||X||^2}.\n\\]\nNote further, if we take a second derivative we get\n\\[\nl'' = \\sum_i 2 x_i^2  \n\\]\nwhich is strictly positive unless all of the \\(x_i\\) are zero (a case of zero variation in the predictor where regresssion is uninteresting). Regression through the origin is a very useful version of regression, but it’s quite limited in its application. Rarely do we want to fit a line that is forced to go through the origin, or stated equivalently, rarely do we want a prediction algorithm for \\(Y\\) that is simply a scale change of \\(X\\). Typically, we at least also want an intercept. In the example that follows, we’ll address this by centering the data so that the origin is the mean of the \\(Y\\) and the mean of the \\(X\\). As it turns out, this is the same as fitting the intercept, but we’ll do that more formally in the next section.\nFirst let’s load the necessary packages.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nNow let’s download and read in the data.\n\ndat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv\")\ndat.head() \n\n\n\n\n\n\n\n\nFLAIR\nPD\nT1\nT2\nFLAIR_10\nPD_10\nT1_10\nT2_10\nFLAIR_20\nPD_20\nT1_20\nT2_20\nGOLD_Lesions\n\n\n\n\n0\n1.143692\n1.586219\n-0.799859\n1.634467\n0.437568\n0.823800\n-0.002059\n0.573663\n0.279832\n0.548341\n0.219136\n0.298662\n0\n\n\n1\n1.652552\n1.766672\n-1.250992\n0.921230\n0.663037\n0.880250\n-0.422060\n0.542597\n0.422182\n0.549711\n0.061573\n0.280972\n0\n\n\n2\n1.036099\n0.262042\n-0.858565\n-0.058211\n-0.044280\n-0.308569\n0.014766\n-0.256075\n-0.136532\n-0.350905\n0.020673\n-0.259914\n0\n\n\n3\n1.037692\n0.011104\n-1.228796\n-0.470222\n-0.013971\n-0.000498\n-0.395575\n-0.221900\n0.000807\n-0.003085\n-0.193249\n-0.139284\n0\n\n\n4\n1.580589\n1.730152\n-0.860949\n1.245609\n0.617957\n0.866352\n-0.099919\n0.384261\n0.391133\n0.608826\n0.071648\n0.340601\n0\n\n\n\n\n\n\n\nIt’s almost always a good idea to plot the data before fitting the model.\n\nx = dat.T2\ny = dat.PD\nplt.plot(x, y, 'o')\n\n\n\n\nNow, let’s center the data as we mentioned so that it seems more reasonable to have the line go through the origin. Notice here, the middle of the data, both \\(Y\\) and \\(X\\), is right at (0, 0).\n\nx = x - np.mean(x)\ny = y - np.mean(y)\nplt.plot(x, y, 'o')\n\n\n\n\nHere’s our slope estimate according to our formula.\n\nb = sum(y * x) / sum(x ** 2 )\nb\n\n0.7831514763655999\n\n\nLet’s plot it so to see how it did. It looks good. Now let’s see if we can do a line that doesn’t necessarily have to go through the origin.\n\nplt.plot(x, y, 'o')\nt = np.array([-1.5, 2.5])\nplt.plot(t, t * b)"
  },
  {
    "objectID": "supervised_regression.html#some-definitions",
    "href": "supervised_regression.html#some-definitions",
    "title": "27  Prediction with regression",
    "section": "27.1 Some definitions",
    "text": "27.1 Some definitions\n\nThe covariance is defined as \\(Cov(X,Y) = \\sum_{i=1}^n (Y_i - \\bar Y) (X_i - \\bar X) / (N-1)\\)\nThe standard deviation of \\(X\\) is \\(SD_X\\), \\(\\sqrt{Cov(X, X)}\\)\nThe Pearson correlation is defined as \\(\\frac{Cov(X, Y)}{SD_X \\times SD_Y}\\)\n\nThe Pearson correlation measures the degree of linear association between two variables where neither is thought of as an outcome or predictor. It is a unit free quantity. If you just say “correlation” without further context, it’s understood to mean the Pearson correlation. The covariance measures the same thing, though it has the units of the units X times the units of Y. The sample standard deviation of X has the units of X and measures the spread, or variability, of X. The variance, \\(Cov(X, X)\\), is simply the square of the standard deviation and has units of X squared.\n\nx = dat['T2']\ny = dat['PD']\ntrainFraction = 0.75\n\n## Hold out data\nsample = np.random.uniform(size = 100) &lt; trainFraction\nxtrain = x[ sample]\nytrain = y[ sample]\nxtest =  x[~sample]\nytest =  y[~sample]\n\n## get the slope on the training data\nbeta1 = st.pearsonr(xtrain, ytrain)[0] * np.std(ytrain) / np.std(xtrain)\nbeta0 = np.mean(ytrain) - np.mean(xtrain) * beta1\nprint([beta0, beta1])\n \nsns.scatterplot(x = xtrain, y = ytrain)\n## add a line\nsns.lineplot(x=xtrain, y=beta0 + beta1 * xtrain)\n\n[0.34150907800816416, 0.7106244491562108]\n\n\n&lt;AxesSubplot: xlabel='T2', ylabel='PD'&gt;\n\n\n\n\n\n\nprint(st.linregress(x = xtrain, y = ytrain))\nsns.regplot(x=xtrain, y=ytrain)\n\nLinregressResult(slope=0.7106244491562107, intercept=0.3415090780081642, rvalue=0.7673572294603325, pvalue=3.9776845801438783e-16, stderr=0.0685675884784028, intercept_stderr=0.05960251507468416)\n\n\n&lt;AxesSubplot: xlabel='T2', ylabel='PD'&gt;\n\n\n\n\n\nLet’s now calculate our predictions on the test set. Recall, the test set was not used to come up with estimates of \\(\\beta_0\\) and \\(\\beta_1\\). We’ll show the training MSE and the testing MSE as well as a plot of the test set actual Ys versus the predicted ones.\n\nyhat_test = beta0 + beta1 * xtest\nyhat_train = beta0 + beta1 * xtrain\n\n## claculate the MSE in the training and test sets\nprint([ np.mean( (ytrain - yhat_train) ** 2), \n        np.mean( (ytest -  yhat_test) ** 2 ) ])\n\n \nsns.scatterplot(x = yhat_test, y = ytest)\nplt.xlabel('Predicted value from xtest T2 values')\nplt.ylabel('Actual PD value from ytest')\n\n[0.18167283828576666, 0.20706632063441943]\n\n\nText(0, 0.5, 'Actual PD value from ytest')"
  },
  {
    "objectID": "supervised_logistic.html#classification-with-one-continuous-variable",
    "href": "supervised_logistic.html#classification-with-one-continuous-variable",
    "title": "28  Logistic regression",
    "section": "28.1 Classification with one continuous variable",
    "text": "28.1 Classification with one continuous variable\nSuppose now that we want to predict the gold standard from the FLAIR values. Fitting a line seems weird, since the outcome can only be 0 or 1. A line would allow for arbitrarily small or large predictions. Similiarly, forcing the prediction to be exactly 0 or 1 leads to difficult optimization problems. A clever solution is to instead model\n\\[\nP(Y_i = 1 ~|~ X_i)\n\\]\nwhere \\(Y_i\\) is the gold standard value (0 or 1 for no lesion or lesion at that voxel, respectively) and \\(X_i\\) is the FLAIR value for voxel \\(i\\). This solves the problem somewhat nicely, but it still leaves some issues unresolved. For example, what does probability even mean in this context? And also probabilities are between 0 and 1, that’s better than exactly 0 or 1, but still would create problems.\nFor the probability, it’s generally a good idea to think about what you’re modeling as random in the context. In this case, we’re thinking of our voxels as a random sample of FLAIR and gold standard voxel values from some population. This is a meaningful benchmark even if it’s not true. We’ll find that often in statistics we model data as if it comes from a probability distribution when we know it didn’t. We simply know that the probability distribution is a useful model for thinking about the problem.\nAs for getting the probabilities from \\([0,1]\\) to \\((-\\infty, \\infty)\\), we need a function, preferably a monotonic one. The generally agreed upon choice is the logit (natural log of the odds) function. The logit function of a probability is defined as\n\\[\n\\eta = \\mathrm{logit}(p) = \\log\\{p / (1 - p)\\}\n\\]\nwhere \\(p\\) is the probability and \\(O = p/(1-p)\\) is called the odds. Note, you can go backwards from odds to probability with the function \\(p = O / (1 + O)\\). Odds are exactly as used in gambling. If the odds of bet at 1 to 99, then you are saying the probability is \\(1 / (99 + 1) = 1\\%\\).\nWhy use odds? There’s a couple of reasons why odds are uniquely interprettable. First, there are specific study designs where odds make more sense than probabilities, particularly retrospective ones. Secondly, odds are unique in binomial models where they work out to be particularly tractible to work with. Finally, odds have a unique gambling interpretation. That is, it gives the ratio of a one dollar risk to the return in a fair bet. (A fair bet is where the expected return is 0.) So, when a horse track gives the odds on a horse to be 99 to 1, they are saying that you would get $99 dollars if you bet one dollar and the horse won. This is an implied probability of 99 / (99 + 1) = 99% that the horse loses and 1% probability that the horse wins. Note they don’t usually express it as a fraction, they usually espress it as value to 1 or 1 to value. So they would say 99 to 1 (odds against) or 1 to 99 (odds for) so you can easily see how much you’d win for a dollar bet.\nYou can go backwards from the logit function to the probability with the expit function. That is, if \\(\\eta\\) is defined as above, then\n\\[\np = \\frac{e^{\\eta}}{1 + e^\\eta} = \\frac{1}{1 + e^{-\\eta}}.\n\\]\nThis is sometimes called the expit function or sigmoid.\nWe model the log of the odds as linear. This is called logistic regression.\n\\[\n\\eta = \\mathrm{logit}\\left\\{ P(Y = 1 ~|~ X) \\right\\}\n= \\beta_0 + \\beta_1 X.\n\\]\nThe nice part about this model is that \\(e^\\beta_1\\) has the nice interpretation of the odds ratio associated with a one unit change in \\(X\\).\nThis is great, but we still need a function of the probabilities to optimize. We’ll use the cross entropy.\n\\[\n-\\sum_{i=1}^n \\left[Y_i \\log\\{P(Y_i = 1 ~|~ X_i)\\} + (1 - Y_i) \\log\\{1 - P(Y_i = 1 ~|~ X_i)\\}\\right].\n\\]\nThis function has the interpretation of being the negative of the log of the probabilities assuming the \\(Y_i\\) are independent. This model doesn’t have to hold for the minimization to be useful.\nPlugging our logit model in, the cross entropy now looks like\n\\[\n-\\sum_{i=1}^n \\left[\n  Y_i \\eta_i + \\log\\left\\{\\frac{1}{1 + e^\\eta_i} \\right\\} \\right].\n\\]\nThis is the function that we minimize to perform logistic regression. Later on, we’ll worry about how to minimize this function. However, today, let’s fit logistic regression to some data.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model as lm\n## this sets some style parameters\nsns.set()\n\n## Read in the data and display a few rows\ndat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv\")\n\n\n## Plot the data\nsns.scatterplot(x = 'FLAIR', y = 'GOLD_Lesions', data = dat)\n\n&lt;AxesSubplot: xlabel='FLAIR', ylabel='GOLD_Lesions'&gt;\n\n\n\n\n\nLet’s now fit the model. Again we’re going to split into training and test data. But, now we’re not going to do it manually since we have to load a library that has a function to do this.\n\nx = dat[['FLAIR']]\ny = dat.GOLD_Lesions\ntrainFraction = .75\n\n## Once again hold out some data\nsample = np.random.uniform(size = 100) &lt; trainFraction\nxtrain = x[ sample]\nytrain = y[ sample]\nxtest =  x[~sample]\nytest =  y[~sample]\n\n\nlr = lm.LogisticRegression(fit_intercept=True, penalty='none')\nfit = lr.fit(xtrain, ytrain)\n\n/home/bcaffo/anaconda3/envs/ds4bio/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning:\n\n`penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n\n\n\nLet’s look at the parameters fit from the model\n\nbeta0, beta1 = [fit.intercept_[0], fit.coef_[0][0]]\n[beta0, beta1]\n\n[-4.1747144327516095, 2.5536395771485134]\n\n\n\nn = 1000\nxplot = np.linspace(-1, 5, n)\neta = beta0 + beta1 * xplot\np = 1 / (1 + np.exp(-eta))\n\nsns.scatterplot(x = 'FLAIR', y = 'GOLD_Lesions', data = dat, hue = 'GOLD_Lesions')\nsns.lineplot(x = xplot, y = p)\n\n## Of course, scikit has a predict\n## function so that you don't have to do this manually\n#yplot = fit.predict_proba(xplot.reshape((n, 1)))\n#sns.lineplot(xplot, yplot[:, 1])\n\n&lt;AxesSubplot: xlabel='FLAIR', ylabel='GOLD_Lesions'&gt;\n\n\n\n\n\nNow let’s evaluate the test set.\n\n## This predicts the classes using a 50% probability cutoff\nyhat_test = fit.predict(xtest)\n\n## double checking that if you want\n#all(yhat_test == (fit.predict_proba(xtest)[:, 1] &gt; .5))\n\naccuracy = np.mean(yhat_test == ytest)\nsensitivity = np.mean(yhat_test[ytest == 1] == ytest[ytest == 1])\nspecificity = np.mean(yhat_test[ytest == 0] == ytest[ytest == 0])\nnp.round([accuracy, sensitivity, specificity], 3)\n\narray([0.655, 0.632, 0.7  ])\n\n\n\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\n\nptest = fit.predict_proba(xtest)[:, 1]\nfpr, tpr, thresholds = roc_curve(ytest, ptest)\nroc_auc = auc(fpr, tpr)\nplt.figure()\nlw = 2 \nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "supervised_multivariable.html#aside-different-python-packages",
    "href": "supervised_multivariable.html#aside-different-python-packages",
    "title": "29  Linear separable models",
    "section": "29.1 Aside different python packages",
    "text": "29.1 Aside different python packages\nSo far we’ve explored several plotting libraries including: default pandas methods, matplotlib, seaborn and plotly. We’ve also looked at several fitting libraries including to some extent numpy, but especially scikitlearn and statsmodels. What’s the difference? Well, these packages are all mantained by different people and have different features and goals. For example, scikitlearn is more expansive than statsmodels, but statsmodels functions more like one is used to with statistical output. Matplotlib is very expansive, but seaborn has nicer default options and is a little easier. So, when doing data science with python, one has to get used to trying out a few packages, weighing the cost and benefits of each, and picking one.\n‘statsmodels’, what we’re using above, has multiple methods for fitting binary models including: sm.Logit, smf.logit, BinaryModel and glm. Here I’m just going to use Logit which does not use the formula syntax of logit. Note, by default, this does not add an intercept this way. So, I’m adding a column of ones, which adds an intercept.\nConsider the following which uses the formula API\n\nresults = smf.logit(formula = 'GOLD_Lesions ~ FLAIR + T1 + T2 + FLAIR_10 + T1_10 + T2_10 + FLAIR_20', data = trainingDat).fit()\nresults.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.246787\n         Iterations 8\n\n\n\nLogit Regression Results\n\n\nDep. Variable:\nGOLD_Lesions\nNo. Observations:\n74\n\n\nModel:\nLogit\nDf Residuals:\n66\n\n\nMethod:\nMLE\nDf Model:\n7\n\n\nDate:\nSat, 01 Apr 2023\nPseudo R-squ.:\n0.6423\n\n\nTime:\n15:54:11\nLog-Likelihood:\n-18.262\n\n\nconverged:\nTrue\nLL-Null:\n-51.049\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n1.153e-11\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-3.8197\n1.950\n-1.959\n0.050\n-7.642\n0.003\n\n\nFLAIR\n1.8228\n1.229\n1.483\n0.138\n-0.586\n4.231\n\n\nT1\n3.4443\n1.326\n2.597\n0.009\n0.845\n6.044\n\n\nT2\n2.5276\n1.262\n2.002\n0.045\n0.053\n5.002\n\n\nFLAIR_10\n8.5824\n4.147\n2.070\n0.038\n0.455\n16.710\n\n\nT1_10\n-0.7716\n2.258\n-0.342\n0.733\n-5.198\n3.655\n\n\nT2_10\n-7.0170\n3.760\n-1.866\n0.062\n-14.387\n0.353\n\n\nFLAIR_20\n-16.0704\n9.094\n-1.767\n0.077\n-33.895\n1.754"
  },
  {
    "objectID": "supervised_multivariable.html#a-classic-example",
    "href": "supervised_multivariable.html#a-classic-example",
    "title": "29  Linear separable models",
    "section": "29.2 A classic example",
    "text": "29.2 A classic example\n\nfrom sklearn.linear_model import LinearRegression\n\n\ndat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/swiss.csv\")\ndat.head()\n\n\n\n\n\n\n\n\nRegion\nFertility\nAgriculture\nExamination\nEducation\nCatholic\nInfant.Mortality\n\n\n\n\n0\nCourtelary\n80.2\n17.0\n15\n12\n9.96\n22.2\n\n\n1\nDelemont\n83.1\n45.1\n6\n9\n84.84\n22.2\n\n\n2\nFranches-Mnt\n92.5\n39.7\n5\n5\n93.40\n20.2\n\n\n3\nMoutier\n85.8\n36.5\n12\n7\n33.77\n20.3\n\n\n4\nNeuveville\n76.9\n43.5\n17\n15\n5.16\n20.6\n\n\n\n\n\n\n\n\ny = dat.Fertility\nx = dat.drop(['Region', 'Fertility'], axis=1)\nfit = LinearRegression().fit(x, y)\nyhat = fit.predict(x)\n[fit.intercept_, fit.coef_]\n\n[66.91518167896866,\n array([-0.17211397, -0.25800824, -0.87094006,  0.10411533,  1.07704814])]\n\n\n\nx2 = x\nx2['Test'] = x2.Agriculture + x2.Examination\nfit2 = LinearRegression().fit(x2, y)\nyhat2 = fit2.predict(x2)\n\n\nplt.plot(yhat, yhat2)\n\n\n\n\n\nx3 = x2.drop(['Agriculture'], axis = 1)\nfit3 = LinearRegression().fit(x3, y)\nyhat3 = fit3.predict(x3)\nplt.plot(yhat, yhat3)"
  },
  {
    "objectID": "supervised_lm_interpretation.html",
    "href": "supervised_lm_interpretation.html",
    "title": "30  Regression interpretation",
    "section": "",
    "text": "Let’s consider how adjustment works in regression by considering a so called ANCOVA (analysis of covariance) setting. Imagine, there’s treatment variable that we’re ineterested in, \\(T_i\\), and a regression variable that we have to adjust for, \\(X_i\\). Consider this specific variation of this setting:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 T_i + \\epsilon_i\n\\]\nTo tie ourselves down with a context, consider \\(Y_i\\) is blood pressure, \\(T_i\\) is a medication and \\(X_i\\) is BMI. Let’s look at different settings that could arise using plots.\nSince I’m going to be making the same plot over and over, I defined a function that\n\nfit the ANCOVA model using sklearn\nplotted the data as \\(X\\) versus \\(Y\\) with orange versus blue for treated versus not\nadded the fitted ANCOVA lines plus the marginal means (the means for each group disregarding \\(X\\)) as horizontal lines\n\nNote, the adjusted estimated treatment effect is the difference between the two parallel sloped lines. The unadjusted estimated treatment effect is the difference between the two horizontal lines. Let’s look at how adjustment changes things depending on the setting. First we’ll do our imports and then define a function that will make our plot for us and fit the ANCOVA model.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport copy\n\nsns.set()\n\n\ndef myplot(x, y, t):\n    x1 = x[t == 1]\n    x0 = x[t == 0]\n    y1 = y[t == 1]\n    y0 = y[t == 0]\n    xm1 = np.mean(x1)\n    xm0 = np.mean(x0)\n    ym1 = np.mean(y1)\n    ym0 = np.mean(y0)\n\n    X = np.array([x, t]).transpose()\n    out = LinearRegression().fit(X, y)\n    b0hat = out.intercept_\n    b1hat = out.coef_[0]\n    b2hat = out.coef_[1]\n    \n    plt.scatter(x0, y0)\n    plt.scatter(x1, y1)\n\n    col = sns.color_palette()\n\n    plt.axhline(y = ym0, c = col[0])\n    plt.axhline(y = ym1, c = col[1])\n\n    xlim = [np.min(x), np.max(x)]\n\n    ylim0 = [z * b1hat + b0hat + b2hat for z in xlim]\n    ylim1 = [z * b1hat + b0hat         for z in xlim]\n\n    plt.plot( xlim, ylim1)\n    plt.plot( xlim, ylim0) \n\n    plt.show()\n\nLet’s consider out model with \\(\\beta_0 = 0\\), \\(\\beta_1 = 1\\) and \\(\\beta_2 = 4\\). So the treated have an intercept 4 units higher. Let’s consider simulating from this model where the treatment is randomized.\n\nn = 100\nx = np.random.normal(size = n)\ne = np.random.normal(size = n)\nt = np.random.binomial(1, .5, n)\n\nbeta0 = 0\nbeta1 = 1\nbeta2 = 4\n\ny = beta0 + beta1 * x + beta2 * t + e\n\nmyplot(x, y, t)\n\n\n\n\nNotice that the marginal means (horizontal lines) are about 4 units appart, same as the lines. This is due to the randomization. A goal of randomization is to make our inference for the treatment unrelated to whether or not we adjust for the confounding variable (\\(X\\)). So, we get (up to random error) the ssame answer whether we adjust for \\(X\\) or not. Let’s consider a different setting.\n\nmyplot(x + t * 4, y, t)\n\n\n\n\nNow notice that there is a large unadjusted difference (difference between the horizontal lines) whereas there is not much of a difference between the lines. That is, when adjusting for \\(X\\), the relationship goes away. Of note, treatment assignment is highly related to the \\(X\\) variable. Orange dots tend to have a larger \\(X\\) value than the blue. Because of this, there’s pratically no area of overlap between the orange and the blue to directly compare them. The adjusted model is all model, extrapolating the blue line up to the orange and the orange down to the blue assuming that they’re parallel.\n\nmyplot(x + t * 4, y  - t * 4, t)\n\n\n\n\nAbove notice that the result is the reverse. There’s little association marginally, but a large one when conditioning. Let’s look at one final case.\n\nmyplot(x + t * 6, y  - t * 2, t)\n\n\n\n\nAbove things are even worse, the relationship has reversed itself. The marginal association is that the orange is above the blue whereas the conditional association is that the blue is above the orange. That is, if you fit the treatment model without \\(X\\) you get one answer, and with \\(X\\) you get the exact opposite answer! This is an example of so-called “Simpsons paradox”. The “paradox” isn’t that paradoxical. It simply says the relationship between two variables could reverse itself when factoring in another variable. Once again, note there’s no overlap in the distributions."
  },
  {
    "objectID": "supervised_dft.html#abstract-notations",
    "href": "supervised_dft.html#abstract-notations",
    "title": "30  DFT",
    "section": "30.1 Abstract notations",
    "text": "30.1 Abstract notations\n\n30.1.1 History\nThe Fourier transform is one of the key tools in Biomedical Data Science. Its namesake is Jean Baptiste Fourier, who was a 18th century French mathemetician who made fundamental discoveries into harmonic analysis. Its fair to say that Fourier’s discoveries are some of the most fundamental in all of a mathematics and engineering and is the foundation for signal processing.\nOne of his main discoveries was the Fourier series, the idea that a function can be decomposed into building blocks of trigonometric functions.\n\n\n30.1.2 Some notation\nLet \\(&lt;,&gt;\\) be a so-called inner product. For example \\(&lt;a, b&gt; = \\sum_{m=1}^n a_m b_m\\) if \\(a\\) and \\(b\\) are two vectors. But, \\(&lt;a, b&gt;=\\int_0^1 a(t)b(t)dt\\) if \\(a\\) and \\(b\\) are two functions on \\([0,1]\\). (There is a nice generality between Fourier results on data and Fourier results on functions and other spaces. However, we’ll largely focus on discrete data, so think of the first definition.) We can define the norm as \\(&lt;a, a&gt; = ||a||^2\\), so that, the distance between two vectors is \\(||a-b||\\).\nConsider a basis, that is a set of vectors, \\(b_k\\) so that \\(||b_k|| = 1\\) and \\(&lt;b_k, b_j&gt;= I(k=j)\\) and the set of vectors, \\({\\cal H}\\), that can be written as \\(\\sum_{k=1}^k b_k c_k\\) for some constants \\(c_k\\), then for any element \\(x\\in H\\) we have that the best approximation using any subset of the indices, \\(S\\), is of the form\n\\[\n\\sum_{k\\in S} b_k &lt;b_k, x&gt;.\n\\]\nFor real vectors and the basis we consider, every vector can be written as a sum of the basis elements. You can have weird functions that can’t be written out as sums of the basis elements, but they’re weird functions."
  },
  {
    "objectID": "supervised_dft.html#more-practically",
    "href": "supervised_dft.html#more-practically",
    "title": "30  DFT",
    "section": "30.2 More practically",
    "text": "30.2 More practically\nThe basis we’re interested in is \\(b_k\\) which has element \\(m\\) equal to \\(e^{-2\\pi i m k/n} = \\cos(2\\pi mk/n) + i \\sin (2\\pi mk / n)\\) for \\(k=0,..., n-1\\). Here, notice, we quit using the index \\(i\\) since now it stands for the complex unit. This basis satisfies our rules of \\(&lt;b_k, b_j&gt; = I(j=k)\\) and having norm 1. So that, given any vector \\(x\\), our best approximation to it is\n\\[\n\\sum_{k=0}^{n-1} b_k &lt;b_k, x&gt; = \\sum_{k=0}^{n-1} b_k F_k\n\\]\nwhere\n\\[\nF_k = \\sum_{m=0}^{n-1} x_m e^{-2\\pi i m k / n}\n= \\sum_{m=0}^{n-1} x_m [\\cos(2\\pi m k / n) + i \\sin(2\\pi m k / n)].\n\\]\nThe collection of elements, \\(F = (F_0, \\ldots F_{n-1})\\) are called the (discrete) Fourier coeficients and the operation that takes \\(x\\) and converts it into \\(F\\) is called the (discrete) Fourier transform.\nLet’s consider the case where \\(x=(1 ~4 ~9 ~16)'\\). So then\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nx = np.arange(1,5,1) ** 2\nt = np.arange(0, 4, 1)\nn = 4\nF0 = np.sum(x * np.exp(-2 * 1j * np.pi * t * 0 / n))\nF1 = np.sum(x * np.exp(-2 * 1j * np.pi * t * 1 / n))\nF2 = np.sum(x * np.exp(-2 * 1j * np.pi * t * 2 / n))\nF3 = np.sum(x * np.exp(-2 * 1j * np.pi * t * 3 / n))\n\nnp.round([F0, F1, F2, F3], 3)\n\narray([ 30. +0.j,  -8.+12.j, -10. -0.j,  -8.-12.j])\n\n\n\nF = np.fft.fft(x)\nF\n\narray([ 30. +0.j,  -8.+12.j, -10. +0.j,  -8.-12.j])\n\n\nLet’s give a more realistic example. Consider two cosine waves, one fast, one slow. Let’s add them together and see if the FFT can figure out what we’ve done.\n\nn = 1000\nt = np.arange(0, n, 1)\nc1 = np.cos(2 * np.pi * t * 5 / n)\nc2 = np.cos(2 * np.pi * t * 20 / n)\nplt.plot(t, c1)\nplt.plot(t, c2)\nplt.show\n\n&lt;function matplotlib.pyplot.show(close=None, block=None)&gt;\n\n\n\n\n\n\nx = c1 + .5 * c2\nplt.plot(t, x)\nplt.show\n\n&lt;function matplotlib.pyplot.show(close=None, block=None)&gt;\n\n\n\n\n\n\na = np.fft.fft(x)\nb = a.real ** 2 + a.imag ** 2\nplt.plot(b)\nplt.show()\nnp.where(b &gt; 1e-5)\n\n\n\n\n(array([  5,  20, 980, 995]),)\n\n\n\n30.2.1 Some notes\nWe can go backwards from the Fourier coefficients to the signal using the inverse transform. Also, for real signals sometimes people will multiply the signal by \\((-1)^t\\) in order for the plot of the norm of the coeficients (the power spectrum as its called) to look nicer.\n\na = np.fft.fft(x * (-1) ** t)\nb = a.real ** 2 + a.imag ** 2\nplt.plot(b)\nplt.show()\n\n\n\n\n\na = np.fft.fft(x)\nb = np.fft.ifft(a)\n\nplt.plot(b)\nplt.show()\n\n\n\n\n\n\n30.2.2 Filtering\nFiltering is the process of allowing certain frequency bands to be retained while others to be discarded. Imagine in our case that we want the low frequency band to pass and to get rid of the higher frequency. In this case we want a low pass filter. There’s a lot of ways to filter signals, but let’s just do it by simple thresholding. The slightly tricky thing about this in practical problems, is making sure that you’re filtering at the frequencies that you want to. As an example, we have 1,000 time points. Say one time point is 1/100 of a second so that we have ten second of data. We have two cosine functions, one that is at 5 oscillations per 10 seconds (0.5 Hz) and one at 20 oscillations per 10 seconds (2 hz). Let’s filter out anything ove 0.5 Hz.\n\n## demonstrating hard filtering\na = np.fft.fft(x)\nn = a.size\ntimestep = 1/100\n## a function that shows what the frequencies are in the units you want\nw = np.fft.fftfreq(n, timestep)\n\nb = a\nb[(abs(w) &gt; .5)] = 0\nc = np.fft.ifft(b).real\nplt.plot(c)\nplt.show()"
  },
  {
    "objectID": "supervised_multivariable.html#regression-interpretation",
    "href": "supervised_multivariable.html#regression-interpretation",
    "title": "29  Linear separable models",
    "section": "29.3 Regression interpretation",
    "text": "29.3 Regression interpretation\nLet’s consider how adjustment works in regression by considering a so called ANCOVA (analysis of covariance) setting. Imagine, there’s treatment variable that we’re ineterested in, \\(T_i\\), and a regression variable that we have to adjust for, \\(X_i\\). Consider this specific variation of this setting:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 T_i + \\epsilon_i\n\\]\nTo tie ourselves down with a context, consider \\(Y_i\\) is blood pressure, \\(T_i\\) is a medication and \\(X_i\\) is BMI. Let’s look at different settings that could arise using plots.\nSince I’m going to be making the same plot over and over, I defined a function that\n\nfit the ANCOVA model using sklearn\nplotted the data as \\(X\\) versus \\(Y\\) with orange versus blue for treated versus not\nadded the fitted ANCOVA lines plus the marginal means (the means for each group disregarding \\(X\\)) as horizontal lines\n\nNote, the adjusted estimated treatment effect is the difference between the two parallel sloped lines. The unadjusted estimated treatment effect is the difference between the two horizontal lines. Let’s look at how adjustment changes things depending on the setting. First we’ll do our imports and then define a function that will make our plot for us and fit the ANCOVA model.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport copy\n\nsns.set()\n\n\ndef myplot(x, y, t):\n    x1 = x[t == 1]\n    x0 = x[t == 0]\n    y1 = y[t == 1]\n    y0 = y[t == 0]\n    xm1 = np.mean(x1)\n    xm0 = np.mean(x0)\n    ym1 = np.mean(y1)\n    ym0 = np.mean(y0)\n\n    X = np.array([x, t]).transpose()\n    out = LinearRegression().fit(X, y)\n    b0hat = out.intercept_\n    b1hat = out.coef_[0]\n    b2hat = out.coef_[1]\n    \n    plt.scatter(x0, y0)\n    plt.scatter(x1, y1)\n\n    col = sns.color_palette()\n\n    plt.axhline(y = ym0, c = col[0])\n    plt.axhline(y = ym1, c = col[1])\n\n    xlim = [np.min(x), np.max(x)]\n\n    ylim0 = [z * b1hat + b0hat + b2hat for z in xlim]\n    ylim1 = [z * b1hat + b0hat         for z in xlim]\n\n    plt.plot( xlim, ylim1)\n    plt.plot( xlim, ylim0) \n\n    plt.show()\n\nLet’s consider out model with \\(\\beta_0 = 0\\), \\(\\beta_1 = 1\\) and \\(\\beta_2 = 4\\). So the treated have an intercept 4 units higher. Let’s consider simulating from this model where the treatment is randomized.\n\nn = 100\nx = np.random.normal(size = n)\ne = np.random.normal(size = n)\nt = np.random.binomial(1, .5, n)\n\nbeta0 = 0\nbeta1 = 1\nbeta2 = 4\n\ny = beta0 + beta1 * x + beta2 * t + e\n\nmyplot(x, y, t)\n\n\n\n\nNotice that the marginal means (horizontal lines) are about 4 units appart, same as the lines. This is due to the randomization. A goal of randomization is to make our inference for the treatment unrelated to whether or not we adjust for the confounding variable (\\(X\\)). So, we get (up to random error) the ssame answer whether we adjust for \\(X\\) or not. Let’s consider a different setting.\n\nmyplot(x + t * 4, y, t)\n\n\n\n\nNow notice that there is a large unadjusted difference (difference between the horizontal lines) whereas there is not much of a difference between the lines. That is, when adjusting for \\(X\\), the relationship goes away. Of note, treatment assignment is highly related to the \\(X\\) variable. Orange dots tend to have a larger \\(X\\) value than the blue. Because of this, there’s pratically no area of overlap between the orange and the blue to directly compare them. The adjusted model is all model, extrapolating the blue line up to the orange and the orange down to the blue assuming that they’re parallel.\n\nmyplot(x + t * 4, y  - t * 4, t)\n\n\n\n\nAbove notice that the result is the reverse. There’s little association marginally, but a large one when conditioning. Let’s look at one final case.\n\nmyplot(x + t * 6, y  - t * 2, t)\n\n\n\n\nAbove things are even worse, the relationship has reversed itself. The marginal association is that the orange is above the blue whereas the conditional association is that the blue is above the orange. That is, if you fit the treatment model without \\(X\\) you get one answer, and with \\(X\\) you get the exact opposite answer! This is an example of so-called “Simpsons paradox”. The “paradox” isn’t that paradoxical. It simply says the relationship between two variables could reverse itself when factoring in another variable. Once again, note there’s no overlap in the distributions."
  },
  {
    "objectID": "supervised_dft.html#regression-and-ffts",
    "href": "supervised_dft.html#regression-and-ffts",
    "title": "30  DFT",
    "section": "30.3 Regression and FFTs",
    "text": "30.3 Regression and FFTs\nRecall regression through the origin. If \\(y\\) and \\(x\\) are \\(n\\)-vectors of the same length, the minimizer of\n\\[\n||y - \\beta x ||^2\n\\]\nis \\(\\hat \\beta = &lt;x, y&gt; / ||x||^2\\). Note, if \\(||x|| = 1\\) then the estimate is just \\(\\hat \\beta = &lt;x, y&gt;\\). Now consider a second variable, \\(w\\), such that \\(&lt;x, w&gt; = 0\\) and \\(||w|| = 1\\). Consider now the least squares model\n\\[\n||y - \\beta x - \\gamma w||^2.\n\\]\nWe argued that the best estimate for \\(\\beta\\) now first gets rid of \\(w\\) be regressing it out of \\(y\\) and \\(x\\). So, consider that\n\\[\n||y - &lt;w, y&gt; w - \\beta (x - &lt;w, x&gt; w)||^2 =\n||y - &lt;w, y&gt; w - \\beta x||^2.\n\\]\nThus, now the best estimate of \\(\\beta\\) is\n\\[\n&lt;y - &lt;w, y&gt; w, x&gt; = &lt;y, x&gt;.\n\\]\nOr, in other words, if \\(x\\) and \\(w\\) are orthogonal then the coefficient estimate for \\(x\\) with \\(w\\) included is the same as the coefficient of \\(x\\) by itself. This extends to more than two regressors.\nIf you have a collection of \\(n\\) mutually orthogonal vectors of norm one, they are called an orthonormal basis. For an orthonomal basis, 1. the coefficients are just the inner products between the regressors and the outcome and 2. inclusion or exclusion of other elemenents of the basis doesn’t change a basis elements estimated coefficients.\nIt’s important to note, that this works quite generally. For example, for complex numbers as well as real. So, for example, consider the possibility that \\(x\\) is \\(e^{-2\\pi i m k / n}\\) for \\(m=0,\\ldots, n-1\\) for a particular value of \\(k\\). Vectors like this are orthogonal for different values of \\(k\\) and all have norm 1. We have already seen that the Fourier coefficient is\n\\[\nf_k = &lt;y, x&gt; = \\sum_{m=0}^{n-1} y_m e^{-2\\pi i m k / n} =\n\\sum_{m=0}^{n-1} y_m \\cos(-2\\pi m k / n) + i \\sum_{m=0}^{n-1} y_m \\sin(-2\\pi m k / n)\n\\]\nwhere \\(y_m\\) is element \\(m\\) of \\(y\\). Thus, the Fourier coefficients are exactly just least squares coefficients applied in the complex space. Thus we have that\n\\[\nf_k = a_k + i b_k\n\\]\nwhere \\(a_k\\) and \\(b_k\\) are the coefficients from linear models with just the sine and cosine terms. Of course, we don’t actually fit Fourier transforms this way, since there’s a much faster way to do, aptly named the fast Fourier transform (FFT). However, knowing how fast discrete Fourier transforms relate to linear models allows us to use them in creative ways, like putting them into models with other covariates, or in logistic regression models.\nLet’s numerically look at FFTs and linear models using covid case counts in Italy as an example.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n\ndat = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\ndat.head()\n\n\n\n\n\n\n\n\nProvince/State\nCountry/Region\nLat\nLong\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\n\n\n0\nNaN\nAfghanistan\n33.93911\n67.709953\n0\n0\n0\n0\n0\n0\n...\n209322\n209340\n209358\n209362\n209369\n209390\n209406\n209436\n209451\n209451\n\n\n1\nNaN\nAlbania\n41.15330\n20.168300\n0\n0\n0\n0\n0\n0\n...\n334391\n334408\n334408\n334427\n334427\n334427\n334427\n334427\n334443\n334457\n\n\n2\nNaN\nAlgeria\n28.03390\n1.659600\n0\n0\n0\n0\n0\n0\n...\n271441\n271448\n271463\n271469\n271469\n271477\n271477\n271490\n271494\n271496\n\n\n3\nNaN\nAndorra\n42.50630\n1.521800\n0\n0\n0\n0\n0\n0\n...\n47866\n47875\n47875\n47875\n47875\n47875\n47875\n47875\n47890\n47890\n\n\n4\nNaN\nAngola\n-11.20270\n17.873900\n0\n0\n0\n0\n0\n0\n...\n105255\n105277\n105277\n105277\n105277\n105277\n105277\n105277\n105288\n105288\n\n\n\n\n5 rows × 1147 columns\n\n\n\n\n## Get Italy, drop everyrthing except dates, convert to long (unstack converts to tuple)\ny=dat[dat['Country/Region'] == 'Italy'].drop([\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"], axis=1).unstack()\n## convert from tuple to array\ny = np.asarray(y)  \n## get case counts instead of cumulative counts\ny = y[1 : y.size] - y[0 : (y.size - 1)]\n## get the first non zero entry\ny =  y[np.min(np.where(y !=  0)) : y.size]\nplt.plot(y)\n\n\n\n\nLet’s look at a smoothed version of it and then take the residual. The residual is where we’d like to look at some oscillatory behavior.\n\nn = y.size\nt = np.arange(0, n, 1)\nlowess = sm.nonparametric.lowess\nyhat = lowess(y, t, frac=.05,return_sorted=False)\nplt.plot(y)\nplt.plot(yhat)\n\n\n\n\n\n## We're interested in the residual\ne = y - yhat\nplt.plot(e)\n\n\n\n\nLet’s manually create our Fourier bases. We’re just going to pick some periods to investigate. We’ll pick a fast varying and slow varying.\n\n## Create 4 elements\n## Orthonormal basis (note dividing by sqrt(n/2) makes them norm 1)\nc5  = np.cos(-2 * np.pi * t * 5 / n  ) / np.sqrt(n /2)\nc20 = np.cos(-2 * np.pi * t * 20 / n ) / np.sqrt(n /2)\ns5  = np.sin(-2 * np.pi * t * 5  / n  )/ np.sqrt(n /2)\ns20 = np.sin(-2 * np.pi * t * 20 / n  ) / np.sqrt(n /2)\n\n\nfig, axs = plt.subplots(2, 2)\naxs[0,0].plot(t, c5)\naxs[0,1].plot(t, c20)\naxs[1,0].plot(t, s5)\naxs[1,1].plot(t, s20)\nplt.show()\n\n\n\n\nLet’s verify that they are indeed orthonormal. That is, we want to show that \\(&lt;x_i, x_j&gt; = I(i =j)\\). We also show that they are all mean 0.\n\n## Verify that they are orthonormal mean 0, round to 6 decimal places\nnp.around( [\n np.sum(c5),\n np.sum(c20),\n np.sum(s5),\n np.sum(s20),\n np.sum(c5 * c5),\n np.sum(c20 * c20),\n np.sum(s5 * s5),\n np.sum(s20 * s20),\n np.sum(c5 * s5),\n np.sum(c5 * s20),\n np.sum(c5 * c20),\n np.sum(s5 * s20),\n], 6)\n\narray([-0.,  0.,  0., -0.,  1.,  1.,  1.,  1.,  0.,  0., -0., -0.])\n\n\nLet’s take the FFT, the fast (discrete) Fourier transform th way one would normally do it. First, we use FFT in numpy. Then, there’s a convenient method, fftfreq, which gives the associated frequencies with each element of the transform. Finally, we plot the spectral density, which is the sum of the real and complex Fourier coefficients. Sorting the elements first is necessary to connect the dots on the plot. Interestingly, once we remove the trend from the Italy data, there’s some very noticeable spikes in the spectral density, which implies large coefficients on that specific frequency. This is possibly some reporting issue.\n\nf = np.fft.fft(e)\nw = np.fft.fftfreq(n)\nind = w.argsort()\nf = f[ind] \nw = w[ind]\nplt.plot(w, f.real**2 + f.imag**2)\n\n\n\n\nNow let’s manually find the coefficients using our constructed bases and the formula that the coefficients.\n\n[\n np.sum(c5 * e) * np.sqrt(n / 2),\n np.sum(c20 * e) * np.sqrt(n / 2),\n np.sum(s5 * e) * np.sqrt(n / 2),\n np.sum(s20 * e) * np.sqrt(n / 2),\n] \n\n[-42402.16539776623, -961069.7936856844, 62406.76098672958, 1902286.8621751775]\n\n\n\nsreg = linear_model.LinearRegression()\nx=np.c_[c5, c20, s5, s20]\nfit = sreg.fit(x, y)\nfit.coef_ * np.sqrt(n/2)\n\narray([ 1222376.2412703 , -1286448.11961955, -4881386.48874237,\n        1827887.03044428])\n\n\n\nx=np.c_[c5, s5]\nfit = sreg.fit(x, y)\nfit.coef_ * np.sqrt(n/2)\n\narray([ 1222376.2412703 , -4881386.48874237])\n\n\n\ntest = np.where( np.abs(f.real / np.sum(c5 * y) / np.sqrt(n / 2) - 1) &lt; 1e-5) \n[test, f.real[test], w[test], 5 / n]\n\n[(array([], dtype=int64),),\n array([], dtype=float64),\n array([], dtype=float64),\n 0.004409171075837742]\n\n\n\nf.imag[test]\n\narray([], dtype=float64)"
  },
  {
    "objectID": "unsupervised_pca_ica.html#pca",
    "href": "unsupervised_pca_ica.html#pca",
    "title": "31  Unsupervised learning",
    "section": "31.1 PCA",
    "text": "31.1 PCA\nLet \\(\\{X_i\\}\\) for \\(i=1,\\ldots,n\\) be \\(p\\) random vectors with means \\((0,\\ldots,0)^t\\) and variance matrix \\(\\Sigma\\). Consider finding \\(v_1\\), a \\(p\\) dimensional vector with \\(||v_1|| = 1\\) so that \\(v_1^t \\Sigma v_1\\) is maximized. Notice this is equivalent to saying we want to maximize \\(\\mathrm{Var}( X_i^t V_1)\\). The well known solution to this equation is that \\(v_1\\) is the first eigenvector of \\(\\Sigma\\) and \\(\\lambda_1 = \\mathrm{Var}( X_i^t V_1)\\) is the associated eigenvalue. If \\(\\Sigma = V^t \\Lambda V\\) is the eigenvalue decomposition of where \\(V\\) are the eigenvectors and \\(\\Lambda\\) is a diagonal matrix of the eigenvalues ordered from greatest to least, then \\(v_1\\) corresponds to the first column of \\(V\\) and \\(\\lambda_1\\) corresponds to the first element of \\(\\Lambda\\). If one then finds \\(v_k\\) as the vector maximizing \\(v_k^t \\Sigma v_k\\) so that \\(v_k^t v_{k'} = I(k=k')\\), then the \\(v_k\\) are the columns of \\(V\\) and \\(v_k^t \\Sigma v_k = \\lambda_k\\) are the eigenvalues.\nNotice:\n\n\\(V \\Sigma V^t = \\Lambda\\) (i.e. \\(V\\) diagonalizess \\(\\Sigma\\))\n\\(\\mbox{Trace}(\\Sigma) = \\mbox{Trace}(\\Sigma V^t V) = \\mbox{Trace}(V \\Sigma V^t) = \\sum \\lambda_k\\) (i.e. the total variability is the sum of the eigenvalues)\nSince \\(V^t V = I\\), \\(V\\) is a rotation matrix. Thus, \\(V\\) rotates \\(X_i\\) in such a way that to maximize variability in the first dimension, then the second dimensions …\n\\(\\mbox{Cov}(X_i^t v_k, x_i^t v_{k'} )= \\mbox{Cov}(X_i^t v_k, x_i^t v_{k'} ) v_k^t \\mbox{Cov}(x_i, x_i^t) v_{k'} = v_k^t V v_{k'} = 0\\) if \\(k\\neq k'\\)\nAnother representation of \\(\\Sigma\\) is \\(\\sum_{k=1}^p \\lambda_i v_k v_k^t\\) by simply rewriting the matrix algebra of \\(V \\Lambda V^t\\).\nThe variables \\(U_i = V X_i\\) then: have uncorrelated elements (\\(\\mbox{Cov}(U_{ik}, U_{ik'}) = 0\\) for \\(k\\neq k'\\) by property 5), have the same total variability as the elements of \\(X_i\\) (\\(\\sum_k \\mbox{Var}(U_{ik}) = \\sum_k \\lambda_k = \\sum_k \\mbox{Var}(X_{ik})\\) by property 2), are a rotation of the \\(X_i\\), are ordered so that \\(U_{i1}\\) has the greatest amount of variability and so on.\n\nNotation:\n\nThe \\(\\lambda_k\\) are simply called the eigenvalues or principal components variation.\n\\(U_{ik} = X_i^t v_k\\) is called the principal component scores.\nThe \\(v_k\\) are called the principal component loadings or weights, with \\(v_1\\) being called the first principal component and so on.\n\nStatistical properties under the assumption that the \\(x_i\\) are iid with mean 0 and variance \\(\\Sigma\\)\n\n\\(E[U_{ik}]=0\\)\n\\(\\mbox{Var}(U_{ik}) = \\lambda_k\\)\n\\(\\mbox{Cov}(U_{ik}, U_{ik'}) = 0\\) if \\(k\\neq k'\\)\n\\(\\sum_{k=1}^p \\mbox{Var}(U_{ik}) = \\mbox{Trace}(\\Sigma)\\).\n\\(\\prod_{k=1}^p \\mbox{Var}(U_{ik}) = \\mbox{Det}(\\Sigma)\\)\n\n\n31.1.1 Sample PCA\nOf course, we’re describing PCA as a conceptual process. We realize \\(n\\) \\(p\\) dimensional vectors \\(x_1\\) to \\(x_n\\), typically organized in \\(X\\) a \\(n\\times p\\) matrix. If \\(X\\) is not mean 0, we typically demean it by calculating \\((I- J(J^t J)^{-1} J') X\\) where \\(J\\) is a vector of ones. Assume this is done. Then \\(\\frac{1}{n-1} X^t X = \\hat \\Sigma\\). Thus, our sample PCA is obtained via the eigenvalue decomposition \\(\\hat \\Sigma = \\hat V \\hat \\Lambda \\hat V^t\\) and our principal components obtained as $ X V$.\nWe can relate PCA to the SVD as follows. Let \\(\\frac{1}{\\sqrt{n-1}} X = \\hat U \\hat \\Lambda^{1/2} \\hat V^t\\) be the SVD of the scaled version of \\(X\\). Then note that \\[ \\hat \\Sigma = \\frac{1}{n-1} X^t X = \\hat V\n\\hat \\Lambda \\hat V^t \\] yields the sample covariance matrix eigenvalue decomposition.\n\n\n31.1.2 PCA with a large dimension\nConsider the case where one of \\(n\\) or \\(p\\) is large. Let’s assume \\(n\\) is large. Then \\[\n\\frac{1}{n-1} X^t X = \\frac{1}{n-1} \\sum_i x_i x_i^t\n\\] As we learned in the chapter on HDF5, we can do sums like these without loading the entirety of \\(X\\) into memory. Thus, in this case, we can calculate the eigenvectors using only the small dimension. If, on the other hand, \\(p\\) is large and \\(n\\) is smaller, then we can calculate the eigenvalue decomposition of \\[\n\\frac{1}{n-1} X X^t = \\hat U \\hat \\Lambda \\hat U^t.\n\\] In either case, whether \\(U\\) or \\(V\\) is easier to get, we can then obtain the other via vectorized multiplication.\n\n\n31.1.3 Simple example\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport numpy.linalg as la\nfrom sklearn.decomposition import PCA\nimport urllib.request\nimport PIL\nimport numpy as np\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.utils.data as data\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.decomposition import FastICA\nfrom tqdm import tqdm\nimport medmnist\nfrom medmnist import INFO, Evaluator\nimport scipy\nimport IPython\n\n\nn = 1000\nmu = (0, 0)\nSigma = np.array([[1, .5], [.5, 1]])\nX = np.random.multivariate_normal( mean = mu, cov = Sigma, size = n)\n\nplt.scatter(X[:,0], X[:,1])\n\n&lt;matplotlib.collections.PathCollection at 0x7fc632bbd390&gt;\n\n\n\n\n\n\nX = X - X.mean(0)\nprint(X.mean(0))\nSigma_hat = np.matmul(np.transpose(X), X) / (n-1) \nSigma_hat\n\n[-1.11022302e-17 -3.61932706e-17]\n\n\narray([[1.05720345, 0.51383452],\n       [0.51383452, 1.03468001]])\n\n\n\nevd = la.eig(Sigma_hat)\nlambda_ = evd[0]\nv_hat = evd[1]\nu_hat = np.matmul(X, np.transpose(v_hat))\nplt.scatter(u_hat[:,0], u_hat[:,1])\n\n&lt;matplotlib.collections.PathCollection at 0x7fc630ed4950&gt;\n\n\n\n\n\nFit using scikitlearn’s function\n\npca = PCA(n_components = 2).fit(X)\nprint(pca.explained_variance_)\nprint(lambda_ )\n\n[1.55989965 0.53198382]\n[1.55989965 0.53198382]\n\n\n\n\n31.1.4 Example\nLet’s consider the melanoma dataset that we looked at before. First we read in the data as we have done before so we don’t show that code.\n\n\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\n\n\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\n\n\nNext, let’s get the data from the torch dataloader format back into an image array and a matrix with the image part (28, 28, 3) vectorized.\n\ndef loader_to_array(dataloader):\n  ## Read one iteration to get data\n  test_input, test_target = next(iter(dataloader))\n  ## Total number of training images\n  n = np.sum([inputs.shape[0] for inputs, targets in dataloader])\n  ## The dimensions of the images\n  imgdim = (test_input.shape[2], test_input.shape[3])\n  images = np.empty( (n, imgdim[0], imgdim[1], 3))\n\n  ## Read the data from the data loader into our numpy array\n  idx = 0\n  for inputs, targets in dataloader:\n    inputs = inputs.detach().numpy()\n    for j in range(inputs.shape[0]):\n      img = inputs[j,:,:,:]\n      ## get it out of pytorch format\n      img = np.transpose(img, (1, 2, 0))\n      images[idx,:,:,:] = img\n      matrix = images.reshape(n, 3 * np.prod(imgdim))\n      idx += 1\n  return images, matrix\n\ntrain_images, train_matrix = loader_to_array(train_loader)\ntest_images, test_matrix = loader_to_array(test_loader)\n\n## Demean the matrices\ntrain_mean = train_matrix.mean(0)\ntrain_matrix = train_matrix - train_mean\ntest_mean = test_matrix.mean(0)\ntest_matrix = test_matrix - test_mean\n\nNow let’s actually perform PCA using scikitlearn. We’ll plot the eigenvalues divided by their sums, \\(\\lambda_k / \\sum_{k'} \\lambda_{k'}\\). This is called a scree plot.\n\nfrom sklearn.decomposition import PCA\nn_comp = 10\npca = PCA(n_components = n_comp).fit(train_matrix)\nplt.plot(pca.explained_variance_ratio_)\n\n\n\n\nOften this is done by plotting the cummulative sum so that you can visualize how much variance is explained by including the top \\(k\\) components. Here I fit 10 components and they explain 85% of the variation.\n\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\n\n\n\n\nNote that the weights from the eigenvectors, \\(V\\), are images. We can plot these as images.\n\neigen_moles = pca.components_\n\n\n\n\n\n\nLet’s project our testing data onto the principal component basis created by our training data and see how it does. Let \\(X_{training} = U \\Lambda^{1/2} V^t\\) is the SVD of our training data. Then, we can convert ths scores, \\(U\\) back to \\(X_{training}\\) with the map \\(U \\rightarrow U \\lambda^{1/2} V\\). Or, if our scores are normalized, \\(U \\Lambda^{1/2}\\) then we simply multiply by \\(V^t\\). If we want to represent \\(X_{training}\\) by a lower dimensional summary, we just keep fewer columns of scores, then multiply by the same columns of \\(V\\). We could write this as \\(U_s = X_{training} V_S \\lambda^{-1/2}_S\\), where \\(S\\) refers to a subset of values of \\(k\\).\nNotice that \\(\\hat X_{training} = U_{S} V^t_S \\Lambda^{-1/2}_S = X_{training} V_S V_S^t\\) , \\(\\Lambda\\) and \\(V\\). Consider then an approximation to \\(X_{test}\\) as \\(\\hat X_{test} = X_{test} V_s V_S^t\\). Written otherwise \\[\n\\hat X_{i,test} = \\sum_{k \\in S} &lt;x_{i,test}, v_k&gt; v_k\n\\] which is the projection of subject \\(i\\)’s features into the linear space spanned by the basis defined by the principal component loadings.\nLet’s try this on our mole data.\n\ntest_matrix_fit = pca.inverse_transform(pca.transform(test_matrix))\nnp.mean(np.abs( test_matrix - test_matrix_fit))\n\n0.03792390424620059"
  },
  {
    "objectID": "unsupervised_pca_ica.html#ica",
    "href": "unsupervised_pca_ica.html#ica",
    "title": "31  Unsupervised learning",
    "section": "31.2 ICA",
    "text": "31.2 ICA\nICA, independent components analysis, tries to find linear transformations of the data that are statistically independent. Usually, independence is an assumption in ICA, not actually embedded in the loss function.\nLet \\(S_t\\) be an \\(\\mathbb{R}^p\\) collection of \\(p\\) source signals. Assume that the underlying signals are independent, so that \\(S_t \\sim F = F_1 \\times F_2 \\times \\ldots \\time f_p\\). Assume that the observed data is \\(X_t = M S_t\\) and \\(X_t \\sim G\\). It is typically assumed that \\(M\\) is invertible so that \\(S_t = M^{-1} X_t\\) and \\(M\\) and \\(M^{-1}\\) are called the mixing and unmixing matrices respectively. Note that, since we observe \\(X_t\\) over many repititions of \\(t\\), we can get an estimate of \\(G\\). Typically, it is also assumed that the \\(X_t\\) are iid over \\(t\\).\nOne way to characterize the estimation problem is to parameterize \\(F_1\\), \\(F_2\\) and \\(F_3\\) and use maximum likelihood, or equivalent [citations]. Another is to minimize some distance between \\(G\\) and \\(F_1\\), \\(F_2\\) and \\(F_3\\). Yet another is to actually maximize independence between the components of \\(S_t\\) using some estimate of independence [cite Matteson].\nThe most popular approaches try to find \\(M^{-1}\\) by maximizing non-Gaussianity. The logic goes that 1) interesting features tend to be non-Gaussian and 2) an appeal to the CLT over signals suggest that the mixed signals should be more Gaussian by being linear combinations of independent things. The latter claim is heuristic relative to the formal CLT. However, maximizing non-Gaussian components tends to work well in practice, thus validating the motivation empirically.\nOne form of ICA maximizes the kurtosis. If \\(Y\\) is a random variable, then \\(E[Y^4] - 3 E[Y^2]\\) is the kurtosis. One could then find \\(M^{-1}\\) that maximizes the empirical kurtosis of the unmixed signals. Another variation of non-Gaussianity maximizes neg-entropy. The neg-entropy of a density \\(h\\) is given by \\[\n- \\int h(y) \\log(h(y)) dy = - E_h[\\log h(Y)] \\] A well known theorem states that the Gaussian distribution has the largest entropy of all distributions with the same variance. Therefore, to maximize non-Gaussianity, we can minmize entropy, or equivalently maximize neg-entropy. We could subtract the entropy of the Gaussian distribution to consider this a cross entropy problem, but that only adds a constant to the loss function. The maximization of neg-entropy can be done many ways. We need the following. For a given \\(M^{-1}\\), estimate \\(G\\) from the collection \\(M^{-1} X_t\\), then calculate the neg-entropy of \\(f_j\\). Use that to then take an opimization step of \\(M\\) is the right direction. Some versions of estimation use a polynomial expansion of the \\(f_j\\), which then typically only requires higher order moments, like kurtosis. Fast ICA is a particular implmementation of maximizing neg-entropy.\nStatistical versions of ICA don’t require \\(M\\) to be invertible. Moreover, error terms can be added in which case you can see the connection between ICA and factor analytic models. However, factor analysis models tend to assume Gaussianity.\n\n31.2.1 Example\nConsider an example that PCA would have somewhat of a hard time with. In this case, our data is from a mixture of normals with half from a normal with a strong positive correlation and half with a strong negative correlation. Because the angle between the two is not 90 degrees PCA has no chance. No rotation of the axes satisfies the obvious structure in this data.\n\nn = 1000\n\nSigma = np.array([[4, 1.8], [1.8, 1]])\na = np.random.multivariate_normal( mean = [0, 0], cov = Sigma, size = int(n/2))\nSigma = np.array([[4, -1.8], [-1.8, 1]])\nb = np.random.multivariate_normal( mean = [0, 0], cov = Sigma, size = int(n/2))\nx = np.append( a, b, axis = 0)\nplt.scatter(x[:,0], x[:,1])\nplt.xlim([-6, 6])\nplt.ylim([-6, 6])\n\n(-6.0, 6.0)\n\n\n\n\n\nLet’s try fast ICA. Notice it comes much closer to discovering the structure we’d like to discover than PCA could. It pulls appart the two components to a fair degree. Also note, there’s a random starting point of ICA, so that I get fairly different fits over re-runs of the algorithm. I had to lower the tolerance to get a good fit.\nIndpendent components are order invariant and sign invariant.\n\ntransformer = FastICA(tol = 1e-7)\nicafit = transformer.fit(x)\ns = icafit.transform(x)\nplt.scatter(s[:,0], s[:,1])\nplt.xlim( [s.min(), s.max()])\nplt.ylim( [s.min(), s.max()])\n\n/home/bcaffo/anaconda3/envs/ds4bio/lib/python3.11/site-packages/sklearn/decomposition/_fastica.py:542: FutureWarning:\n\nStarting in v1.3, whiten='unit-variance' will be used by default.\n\n\n\n(-0.1504016507767862, 0.13189604849661118)\n\n\n\n\n\n\n\n31.2.2 Cocktail party example\nThe classic ICA problem is the so called cocktail party problem. In this, you have \\(p\\) sources and \\(p\\) microphones. The microphones each pick up a mixture of signals from the different sources. The goal is to unmix the sources into the components. Independence makes sense in the cocktail party example, since logically conversations would have some independence.\n\nimport audio2numpy as a2n\ns1, i1 = a2n.audio_from_file(\"mp3s/4.mp3\")\ns2, i2 = a2n.audio_from_file(\"mp3s/2.mp3\")\ns3, i3 = a2n.audio_from_file(\"mp3s/6.mp3\")\n\n## Get everything to be the same shape and sum the two audio channels\nn = np.min((s1.shape[0], s2.shape[0], s3.shape[0]))\ns1 = s1[0:n,:].mean(axis = 1)\ns2 = s2[0:n,:].mean(axis = 1)\ns3 = s3[0:n,:].mean(axis = 1)\n\ns = np.array([s1, s2, s3])\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nMix the signals.\n\nw = np.array( [ [.7, .2, .1], [.1, .7, .2], [.2, .1, .7] ])\nx = np.transpose(np.matmul(w, s))\n\nHere’s an example mixed signal\n\nIPython.display.Audio(data = x[:,1].reshape(n), rate = i1)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nNow try to unmix using fastICA\n\ntransformer = FastICA(whiten=True, tol = 1e-7)\nicafit = transformer.fit(x)\n\n/home/bcaffo/anaconda3/envs/ds4bio/lib/python3.11/site-packages/sklearn/decomposition/_fastica.py:729: FutureWarning:\n\nStarting in v1.3, whiten=True should be specified as whiten='arbitrary-variance' (its current behaviour). This behavior is deprecated in 1.1 and will raise ValueError in 1.3.\n\n\n\n\nicafit.mixing_\n\narray([[ -12.7040735 ,  -16.5757025 ,   51.16275932],\n       [ -44.06754477,  -31.47542799,    7.49741556],\n       [  -5.50880035, -108.62795086,   14.14817169]])\n\n\nUnmixing matrix\n\nicafit.components_\n\narray([[ 0.00166487, -0.02401033,  0.00670305],\n       [ 0.0026259 ,  0.00046055, -0.00973987],\n       [ 0.02080961, -0.00581273, -0.00149111]])\n\n\nHere’s a scatterplot matrix where the real component is on the rows and the estimated component is on the columns.\n\nhat_s = np.transpose(icafit.transform(x))\n\nplt.figure(figsize=(10,10))\n\nfor i in range(3):\n  for j in range(3):\n    plt.subplot(3, 3, (3 * i + j) + 1)\n    plt.scatter(hat_s[i,:].squeeze(), np.asarray(s)[j,:])\n\n\n\n\nWe can now play the estimated sources and see how they turned out.\n\nfrom scipy.io.wavfile import write\ni = 0\ndata = (hat_s[i,:].reshape(n) / np.max(np.abs(hat_s[i,:]))) * .5\nIPython.display.Audio(data = data, rate = i1)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ni = 1\ndata = (hat_s[i,:].reshape(n) / np.max(np.abs(hat_s[i,:]))) * .5\nIPython.display.Audio(data = data, rate = i1)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ni = 2\ndata = (hat_s[i,:].reshape(n) / np.max(np.abs(hat_s[i,:]))) * .5\nIPython.display.Audio(data = data, rate = i1)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n31.2.3 Imaging example using ICA\nLet’s see what we get for the images. Logically, one would consider voxels as mixed sources and images as the iid replications. But, then the sources would not be images. Let’s try the other dimension and see what we get where subject images are mixtures of source images. This is analogous to finding a soure basis of subject images.\nThis is often done in ICA where people transpose matrices to investigate different problems.\n\ntransformer = FastICA(n_components=10, random_state=0,whiten='unit-variance', tol = 1e-7)\nicafit = transformer.fit_transform(np.transpose(train_matrix))\nicafit.shape\n\n/home/bcaffo/anaconda3/envs/ds4bio/lib/python3.11/site-packages/sklearn/decomposition/_fastica.py:123: ConvergenceWarning:\n\nFastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n\n\n\n(2352, 10)"
  },
  {
    "objectID": "nns_intro.html#basics",
    "href": "nns_intro.html#basics",
    "title": "32  Neural networks, introduction",
    "section": "32.1 Basics",
    "text": "32.1 Basics\nLet’s start by relating neural networks to regression. Consider a simple case where we have two nodes, \\(1\\) and \\(X\\) pointing to an outcome \\(Y\\). What does this mean? Let’s first put some context around the problem. Imagine that we want to use a subject’s BMI \\(X\\) to predict their blood pressure, \\(Y\\). This diagram represents that.\n\n\n\n\n\nTo interpret this diagram as a neural network, consider the following rule:\n\n\n\n\n\n\nNote\n\n\n\nParent nodes that point to a child node are multiplied by weights then added together then operated on by an activation function to form the child node.\n\n\nIf the parent nodes point to the outcome, then the nodes are combined the operated on by a known function, called the activation function to form a prediction. So, in this case, this is saying that the intercept (node labeled \\(1\\))times a weight plus BMI (node labeled \\(X\\)) times a different weight get combined to form a prediction for SBP \\(Y\\). Or, in other words\n\\[\n\\hat Y = g(w_0 \\times 1 + w_1 \\times X)\n\\]\nwhere \\(g\\) is a function that we specify. So in this case, if \\(w_0 = 120\\), \\(w_1 = .1\\) and \\(g\\) is an idenity function, \\(g(a) = a\\), and a subject had a BMI of 30, then the prediction would be\n\\[\n\\hat Y = g(120 + .1 * 30) = 120.3\n\\]\nNote \\(g\\) is not shown in the diagram (though maybe you could with the shape of the child node) or something like that0. Also not shown in the daigram is:\n\nThe loss function, i.e. how to measure the different between \\(\\hat Y\\) and \\(Y\\).\nThe way the loss function combines subjects; we have multiple BMIs and SBPs\nHow we obtain the weights, \\(W_0\\) and \\(W_1\\); this is done by minmizing the loss function using an algorithm\n\nSo, imagine the case where \\(g\\) is an identity function, our loss function for different subjects is squared error and we combine different losses by adding them up. Then, our weights are obtained by minmizing\n\\[\n\\sum_{i=1}^N (Y_i - \\hat Y_i)^2\n\\]\nand so, presuming our optimization algorithm works well, it should be idential to linear regression.\nConsider a different setting. Imagine if our \\(Y\\) is 0 or 1 based on whether or not the subject is taking anti-hypertensive mediations. Further, let \\(g\\) be the sigmoid function, \\(g(a) = 1 / \\{1 + \\exp(-a)\\}\\). Our prediction is\n\\[\n\\hat Y = \\{1 + \\exp(-W_0 - W_1 X)\\}^{-1}\n\\]\nwhich is the logistic regression prediction with intercept \\(W_0\\) and slope \\(W_1\\). Consider a case where \\(W_0 = -4\\), \\(W_1 = .1\\) and \\(X=30\\), then our \\(\\hat Y = 1 / \\{1 + \\exp[-(-4 + .1\\times 30)\\}]\\approx .27\\). Thus, this model estimates a 27% probability that a subject with a BMI of 30 has hypertension.\nFurther, if we specify that the loss function is binary cross entropy\n\\[\n- \\sum_{i=1}^n \\{ Y_i \\log(\\hat Y_i) + (1 - Y_i) \\log(1 - \\hat Y_i)\\} / N\n\\]\nthen minmizing our loss function is identical to maximizing the likelihood for logistic regression.\n\n1 / (1 + np.exp(-(-4 + .1 * 30)))\n\n0.2689414213699951"
  },
  {
    "objectID": "nns_intro.html#more-layers",
    "href": "nns_intro.html#more-layers",
    "title": "32  Neural networks, introduction",
    "section": "32.2 More layers",
    "text": "32.2 More layers\nOf course, there’d be no point in using NNs for problems that we can just solve with generalized linear models. NNs get better when we add more layers, since then they can discover interactions and non-linearities. Consider the following model. Notice we quit explicitly adding the bias (intercept) term / node. In general assume the bias term is included unless otherwise specified.\n\n\n\n\n\nUsually, the nodes are added in so called layers. \\((X_1, X_2)\\) is the input layer, \\((H_{11}, H_{12})\\) is the first hidden layer, \\((H_{21}, H_{22})\\) is the second hidden layer and \\(Y\\) is the output layer. Imagine plugging an \\(X_1\\) and \\(X_2\\) into this network. It would feed forward through the network as\n\\[\n\\begin{align}\nH_{11} = & g_1(W_{011} + W_{111} X_1 + W_{211} X_2) \\\\\nH_{12} = & g_1(W_{012} + W_{112} X_1 + W_{212} X_2) \\\\\nH_{21} = & g_2(W_{021} + W_{121} H_{11} + W_{221} H_{12}) \\\\\nH_{22} = & g_2(W_{022} + W_{122} H_{12} + W_{222} H_{12}) \\\\\n\\hat Y = & g_3(W_{031} + W_{131} H_{21} + W_{231} H_{22})\n\\end{align}\n\\]\nwhere \\(g_k\\) are specified activation functions. Typically, we would have a different activation function for the output layer than the others, and the other would have the same activation function. So, for example, if \\(Y\\) was binary, like hypertension diagnosis, then \\(g_1=g_2\\) and \\(g_3\\) would be a sigmoid."
  },
  {
    "objectID": "nns_intro.html#activation-functions",
    "href": "nns_intro.html#activation-functions",
    "title": "32  Neural networks, introduction",
    "section": "32.3 Activation functions",
    "text": "32.3 Activation functions\nThe output activation function tends to be based on the structure of the outcome. For example, a binary outcome would likely have a sigmoidal, or other function from \\(\\mathbb{R}\\) to \\([0, 1]\\) so as to model a probability. Historically, the internal activation functions were binary thresholds. This was owning to the fact that neural networks were models of (biological) neurons and the threshold was a model of an action potential being propigated. However, modern neural networks have less of a direct connection to their biological motivation and other activation functions tend to be used. The most popular right now is the rectified linear unit (RELU) function. This is simply:\n\\[\nRELU(a) = \\left\\{\n\\begin{array}{ll}\na & \\text{if $a&gt;0$} \\\\\n0 & \\text{otherwise}\n\\end{array}\n\\right.\n= a \\times I(a &gt; 0)\n\\]\nPlotted, this is:\n\nplt.plot( [-1, 0, 1], [0, 0, 1], linewidth = 4);\n\n\n\n\nIf a bias term is included, then the fact that the RELU is centered at zero isn’t important, since the intercept term effectively shifts the function around. These kinds of splin terms are incredibly flexible. Just to show you an example, let’s fit the sine function using a collection of shifted RELUs. This is just\n\\[\nY = \\sin(X) + \\epsilon\n\\]\nbeing fit with\n\\[\n\\sum_{i=1}^N \\left\\{ Y_i - W_{021} - \\sum_{j=1}^{d} W_{j21} g(W_{1j1} X_i- W_{0j1}) \\right\\}^2\n\\]\nwhere the \\(W_{kj}\\) are the weights for layer \\(k\\). Below, we’re just setting \\(W_{1j1} = 1\\) and specifying the \\(W_{0j1}\\) at a sequence of values.\n\n## Generate some data, a sine function on 0,4*pi\nn = 1000\nx = np.linspace(0, 4 * np.pi, n)\ny = np.sin(x) + .2 * np.random.normal(size = n)\n\n## Generate the spline regressors\ndf = 30\nknots = np.linspace(x.min(), x.max(), df)\nxmat = np.zeros((n, df))\nfor i in range(0, df): xmat[:,i] = (x - knots[i]) * (x &gt; knots[i])\n\n## Fit them\nfrom sklearn.linear_model import LinearRegression\nyhat = LinearRegression().fit(xmat, y).predict(xmat)\n\n## Plot them versus the data\nplt.plot(x, y);\nplt.plot(x, yhat);\n\n\n\n\nThis corresponds to a network like depicted below if there were \\(d=3\\) hidden nodes, there was a relu activation function at the first layer, then a identity activation function for the output layer and the weights for the first layer are specified.\n\n\n\n\n\nWe can actually fit this function way better using splines and a little bit more care. However, this helps show how even one layer of RELU activated nodes can start to fit complex shapes."
  },
  {
    "objectID": "nns_intro.html#optimization",
    "href": "nns_intro.html#optimization",
    "title": "32  Neural networks, introduction",
    "section": "32.4 Optimization",
    "text": "32.4 Optimization\nOne of the last bits of the puzzle we have to figure out is how to obtain the weights. A good strategy would be to minimize the loss function. However, it’s hard to minmize. If we had a derivative, we could try the following. Let \\(L(W)\\) be the loss function for weights \\(W\\). Note, we’re omitting the fact that this is a function of the data (predictors and outcome) as well, since that’s a set of fixed numbers. Consider updating parameters as\n\\[\nW^{(new)} = W^{(old}) - e * L'(W^{(old)})\n\\]\nWhat does this do? It moves the parameters by a small amount, \\(e\\), called the learning rate, in the direction the opposite of the gradient. Think of a one dimensional convex function. If the derivative at a point is positive, then that point is larger than where the minimum is. Similarily, if the derivative is negative, it’s smaller. So, the idea is to head a small amount in the opposite direction of the derivative. How much? How about along the line of the derivative? That’s all gradient descent does, just in more than one dimension.\nHow do we get the gradient? Consider the following. If \\(X\\) is our vector of predictors and \\(Y\\) is our vector of outputs, a neural network with 3 layers, can be thought of as, where \\(L_k\\) is layer \\(K\\) and \\(W_k\\) are the weights for that layer:\n\\[\nL_3(L_2(L_1(X, W_1), W_2) W_3)\n\\]\nOr a series of function compositions. Recall from calculus, if we want the derivative of composed functions we have a really simple rule called the chain rule:\n\\[\n\\frac{d}{dx}f(g(x)) = f'(g(x)) g'(x)\n\\]\nI.e. if \\(h=f(u)\\) and \\(u = g(x)\\) then \\(\\frac{dh}{dx} = \\frac{dh}{du}\\frac{du}{dx}\\). Thus, characterized this way, the chain rule formally acts like fractions (though this is a symbolic equivalence having entirely different underlying meanings).\nIf we use the chain rule on our composed loss functions, we wind up bookkeeping backwards through our neural network. That is why it’s called backwards propagation (backprop).\nSo, our algorithm goes something like this. Given, \\(W^{(new)}\\), network, \\(\\phi(X, W)\\), which depends on the predictors and the weights and loss, \\(L(Y, \\hat Y)\\), which depends on the observed and predicted outputs.\n\nSet \\(W^{(old)}=W^{(new)}\\)\nCalculate \\(\\hat Y = \\phi(X, W^{(old)})\\) and loss \\(L(Y, \\hat Y)\\).\nUse back propagation to get to get a numerical approximation to \\(\\frac{d}{dW} L\\{Y, \\phi(X, W)\\} |_{W=W^{(old)}} = L'(W^{(old)})\\)\nUpdate \\(W^{(new)} = W^{(old)} - e L'(W^{(old)})\\)\nGo to step 0."
  },
  {
    "objectID": "nns_basic_regression.html",
    "href": "nns_basic_regression.html",
    "title": "33  Basic regression as a NN",
    "section": "",
    "text": "import pandas as pd\nimport torch\nimport statsmodels.formula.api as smf\nimport statsmodels as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n## Read in the data and display a few rows\ndat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv\")\ndat.head(4)\n\n\n\n\n\n\n\n\nFLAIR\nPD\nT1\nT2\nFLAIR_10\nPD_10\nT1_10\nT2_10\nFLAIR_20\nPD_20\nT1_20\nT2_20\nGOLD_Lesions\n\n\n\n\n0\n1.143692\n1.586219\n-0.799859\n1.634467\n0.437568\n0.823800\n-0.002059\n0.573663\n0.279832\n0.548341\n0.219136\n0.298662\n0\n\n\n1\n1.652552\n1.766672\n-1.250992\n0.921230\n0.663037\n0.880250\n-0.422060\n0.542597\n0.422182\n0.549711\n0.061573\n0.280972\n0\n\n\n2\n1.036099\n0.262042\n-0.858565\n-0.058211\n-0.044280\n-0.308569\n0.014766\n-0.256075\n-0.136532\n-0.350905\n0.020673\n-0.259914\n0\n\n\n3\n1.037692\n0.011104\n-1.228796\n-0.470222\n-0.013971\n-0.000498\n-0.395575\n-0.221900\n0.000807\n-0.003085\n-0.193249\n-0.139284\n0\n\n\n\n\n\n\n\n\nsns.scatterplot(x = dat['T2'], y = dat['PD'])\n\n&lt;AxesSubplot: xlabel='T2', ylabel='PD'&gt;\n\n\n\n\n\n\nfit = smf.ols('PD ~ T2', data = dat).fit()\nfit.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nPD\nR-squared:\n0.661\n\n\nModel:\nOLS\nAdj. R-squared:\n0.657\n\n\nMethod:\nLeast Squares\nF-statistic:\n190.9\n\n\nDate:\nSun, 02 Apr 2023\nProb (F-statistic):\n9.77e-25\n\n\nTime:\n10:34:35\nLog-Likelihood:\n-57.347\n\n\nNo. Observations:\n100\nAIC:\n118.7\n\n\nDf Residuals:\n98\nBIC:\n123.9\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.3138\n0.052\n6.010\n0.000\n0.210\n0.417\n\n\nT2\n0.7832\n0.057\n13.815\n0.000\n0.671\n0.896\n\n\n\n\n\n\nOmnibus:\n1.171\nDurbin-Watson:\n1.501\n\n\nProb(Omnibus):\n0.557\nJarque-Bera (JB):\n0.972\n\n\nSkew:\n0.241\nProb(JB):\n0.615\n\n\nKurtosis:\n2.995\nCond. No.\n1.89\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# The in sample predictions\nyhat = fit.predict(dat['T2'])\n\n# Make sure that it's adding the intercept\n#test = 0.3138 + dat['T2'] * 0.7832\n#sns.scatterplot(yhat,test)\n\n## A plot of the in sample predicted values\n## versus the actual outcomes\nsns.scatterplot(x = yhat, y = dat['PD'])\nplt.plot([-1, 3], [-1, 3], linewidth=2)\n\n\n\n\n\nn = dat.shape[0]\n\n## Get the y and x from \nxtraining = torch.from_numpy(dat['T2'].values)\nytraining = torch.from_numpy(dat['PD'].values)\n\n## PT wants floats\nxtraining = xtraining.float()\nytraining = ytraining.float()\n\n## Dimension is 1xn not nx1\n## squeeze the second dimension\nxtraining = xtraining.unsqueeze(1)\nytraining = ytraining.unsqueeze(1)\n\n## Show that everything is the right size\n[xtraining.shape, \n ytraining.shape,\n [n, 1]\n ]\n\n[torch.Size([100, 1]), torch.Size([100, 1]), [100, 1]]\n\n\n\n## Show that linear regression is a pytorch \nmodel = torch.nn.Sequential(\n    torch.nn.Linear(1, 1)\n)\n\n## MSE is the loss function\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\n## Set the optimizer\n## There are lots of choices\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n\n## Loop over iterations\nfor t in range(10000):\n\n    ## Forward propagation\n  y_pred = model(xtraining)\n    \n  ## the loss for this interation\n  loss = loss_fn(y_pred, ytraining)\n\n  #print(t, loss.item() / n)\n\n  ## Zero out the gradients before adding them up \n  optimizer.zero_grad()\n  \n  ## Backprop\n  loss.backward()\n  \n  ## Optimization step\n  optimizer.step()\n\n\nytest = model(xtraining).detach().numpy().reshape(-1)\nsns.scatterplot(x = ytest, y = yhat)\nplt.plot([-1, 3], [-1, 3], linewidth=2)\n\n\n\n\n\nfor param in model.parameters():\n  print(param.data)\n\ntensor([[0.7831]])\ntensor([0.3138])"
  },
  {
    "objectID": "nns_pytorch_example.html",
    "href": "nns_pytorch_example.html",
    "title": "35  Pytorch by example",
    "section": "",
    "text": "This example from the pytorch documentation here displays generating random y ad x dat and fitting a multi-layer neural network. We’re going to consider a regression problem, but using a two layer neural network to solve it. Consider something like this. We have 5 inputs; those get passed to 3 hidden nodes, those get RELU’d, which get passed to an output.\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn as skl\n\n#plt.figure(figsize=[2, 2])\nG = nx.DiGraph()\nG.add_node(\"X1\",  pos = (0, 2) )\nG.add_node(\"X2\",  pos = (1, 2) )\nG.add_node(\"X3\",  pos = (2, 2) )\nG.add_node(\"X4\",  pos = (3, 2) )\nG.add_node(\"X5\",  pos = (4, 2) )\n\nG.add_node(\"H1\",  pos = (1, 1) )\nG.add_node(\"H2\",  pos = (2, 1) )\nG.add_node(\"H3\",  pos = (3, 1) )\nG.add_node(\"Y\" ,  pos = (2, 0) )\n\nG.add_edges_from([ (\"X1\", \"H1\"), (\"X2\", \"H1\"), (\"X3\", \"H1\"),  (\"X4\", \"H1\"), (\"X5\", \"H1\")])\nG.add_edges_from([ (\"X1\", \"H2\"), (\"X2\", \"H2\"), (\"X3\", \"H2\"),  (\"X4\", \"H2\"), (\"X5\", \"H2\")])\nG.add_edges_from([ (\"X1\", \"H3\"), (\"X2\", \"H3\"), (\"X3\", \"H3\"),  (\"X4\", \"H3\"), (\"X5\", \"H3\")])\nG.add_edges_from([ (\"H1\",  \"Y\"), (\"H2\",  \"Y\"), (\"H3\", \"Y\")])\nnx.draw(G, \n        nx.get_node_attributes(G, 'pos'), \n        with_labels=True, \n        font_weight='bold', \n        node_size = 2000,\n        node_color = \"lightblue\",\n        linewidths = 3)\nax= plt.gca()\nax.collections[0].set_edgecolor(\"#000000\")\nax.set_xlim([-.3, 4.3])\nax.set_ylim([-.3, 2.3])\nplt.show()\n\n\n\n\nBelow, we create an example. It isn’t terribly interesting, since the X and the Y aren’t related at all. But, it does show us some useful code.\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model as lm\nsns.set()\n\n\n# N is batch size; D_in is input dimension;\n# H is hidden dimension; D_out is output dimension.\nN, D_in, H, D_out = 1000, 128, 32, 8\n\n# Create random Tensors to hold inputs and outputs\nx = torch.randn(N, D_in)\ny = torch.randn(N, D_out)\n\n# Use the nn package to define our model as a sequence of layers. nn.Sequential\n# is a Module which contains other Modules, and applies them in sequence to\n# produce its output. Each Linear Module computes output from input using a\n# linear function, and holds internal Tensors for its weight and bias.\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(D_in, H),\n    torch.nn.ReLU(),\n    torch.nn.Linear(H, D_out),\n)\n\n# The nn package also contains definitions of popular loss functions; in this\n# case we will use Mean Squared Error (MSE) as our loss function.\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\nlearning_rate = 1e-4\nfor t in range(1000):\n    # Forward pass: compute predicted y by passing x to the model. Module objects\n    # override the __call__ operator so you can call them like functions. When\n    # doing so you pass a Tensor of input data to the Module and it produces\n    # a Tensor of output data.\n    y_pred = model(x)\n\n    # Compute and print loss. We pass Tensors containing the predicted and true\n    # values of y, and the loss function returns a Tensor containing the\n    # loss.\n    loss = loss_fn(y_pred, y)\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Zero the gradients before running the backward pass.\n    model.zero_grad()\n\n    # Backward pass: compute gradient of the loss with respect to all the learnable\n    # parameters of the model. Internally, the parameters of each Module are stored\n    # in Tensors with requires_grad=True, so this call will compute gradients for\n    # all learnable parameters in the model.\n    loss.backward()\n\n    # Update the weights using gradient descent. Each parameter is a Tensor, so\n    # we can access its gradients like we did before.\n    with torch.no_grad():\n        for param in model.parameters():\n            param -= learning_rate * param.grad\n\n99 4489.7412109375\n199 3196.518798828125\n299 2976.0107421875\n399 2801.79931640625\n499 2693.77685546875\n\n\n599 2667.694091796875\n699 2585.8671875\n799 2606.7880859375\n899 2427.787353515625\n999 2361.8896484375\n\n\nLet’s update that example for our setting using the voxel level data.\n\n## Read in the data and display a few rows\ndat = pd.read_csv(\"https://raw.githubusercontent.com/bcaffo/ds4bme_intro/master/data/oasis.csv\")\n\n\n\n\n\n\nLet’s set the training fraction at 75% (and thus the testing fraction at 25%). This gives us relatively little data to fit with. Thus, a relatively simple model makes sense.\n\ntrainFraction = .75\n\nsample = np.random.uniform(size = 100) &lt; trainFraction\ntrainingDat = dat[sample]\ntestingDat = dat[~sample]\n\nNext we need to get the data into a pytorch size and fram\n\nx = torch.from_numpy(dat[['PD','T1', 'T2', 'T1_10', 'T2_10', ]].values)\ny = torch.from_numpy(dat[['FLAIR']].values)\n\n##pytorch wants type as float\nx = x.float()\ny = y.float()\n\nxtraining = x[sample]\nxtesting = x[~sample]\nytraining = y[sample]\nytesting = y[~sample]\n\n[\n xtraining.size(),\n ytraining.size(),\n xtesting.size(),\n ytesting.size(),\n]\n\n[torch.Size([81, 5]),\n torch.Size([81, 1]),\n torch.Size([19, 5]),\n torch.Size([19, 1])]\n\n\n\n## Define the model\n## Dimension of the hidden layer\nH = 3\n\n## Number of predictors\nD_in = xtraining.size()[1]\nD_out = 1\n\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(D_in, H),\n    torch.nn.ReLU(),\n    torch.nn.Linear(H, D_out),\n)\n\n\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\nlearning_rate = 1e-4\nfor t in range(100000):\n    y_pred = model(xtraining)\n    loss = loss_fn(y_pred, ytraining)\n    if t % 10000 == 0:\n        print(t, loss.item())\n    model.zero_grad()\n    loss.backward()\n    with torch.no_grad():\n        for param in model.parameters():\n            param -= learning_rate * param.grad\n\n0 270.9652099609375\n\n\n10000 14.670221328735352\n\n\n20000 13.615744590759277\n\n\n30000 12.634902000427246\n\n\n40000 12.154542922973633\n\n\n50000 12.103774070739746\n\n\n60000 12.096871376037598\n\n\n70000 12.095998764038086\n\n\n80000 12.095781326293945\n\n\n90000 12.095928192138672\n\n\n\n## try prediction\nytesting_pred = model(xtesting)\na = ytesting_pred.detach().numpy()\n\nplt.scatter(a[:,0], ytesting[:,0])\n\n&lt;matplotlib.collections.PathCollection at 0x7f4c5dd58390&gt;"
  },
  {
    "objectID": "specialized_autoencoder.html",
    "href": "specialized_autoencoder.html",
    "title": "36  Autoencoders",
    "section": "",
    "text": "In this exercise, we’ll build an autoencoder to model the cryptopunks. We’ll assume that you’ve already looked at the chapter on convolutional networks, where we show how we downloaded and process the data. An autoencoder can be thought of as the following. Consider a datset with 8 features and consider a network that has 4 hidden nodes on the first layer, 2 on the second, 4 on the third and 8 on the fourth. See the picture below.\n\n\n\n\n\nLet \\(\\phi\\) be the first two layers of the network and \\(\\theta\\) be the last two. So, if we wanted to pass a data row, \\(x_i\\) through the network we would do \\(\\theta(\\phi(x_i))\\). We would call the network \\(\\phi\\) as the encoding network and \\(\\theta\\) as the decoding network. Consider training the network by minimizing\n\\[\n\\sum_{i=1}^n || x_i - \\theta(\\phi(x_i)) ||^2\n\\]\nover the weights. This sort of network is called an autoencoder. Notice that the same data is the input and output of the network. This kind of learning is called unsupervised, since we’re not trying to use \\(x\\) to predict an outcome \\(y\\). Instead, we’re trying to explore variation and find interesting features in \\(x\\) as a goal in itself without a “supervising” outcome, \\(y\\), to help out.\nNotice overfitting concerns are somewhat different in this network construction. If this model fits well, then it’s suggesting that 2 numbers can explain 8. That is, the network will have reduced the inputs to only two dimensions, that we could visualize for example. That is a form of parsimony that prevents overfitting. The middle layer is called the embedding. It is called this because an autoencoder is a form of non-linear embedding of our data into a lower dimensionional space.\nThere’s nothing to prevent us from having convolutional layers if the inputs are images. That’s what we’ll work on here. For convolutional autoencoders, it’s typical to increase the number of channels and decrease the image sizes as one works through the network.\n\nimport urllib.request\nimport PIL\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\n\nImport the image of all of the cryptopunks where we’ll try to fit a convolutional autoencoder.\n\nimgURL = \"https://raw.githubusercontent.com/larvalabs/cryptopunks/master/punks.png\"\nurllib.request.urlretrieve(imgURL, \"cryptoPunksAll.jpg\")\nimg = PIL.Image.open(\"cryptoPunksAll.jpg\").convert(\"RGB\")\nimgArray = np.asarray(img)\n\nReorder the array. I couldn’t get reshape to do this right, but I think this is a one-line command waiting to happen. See if you can figure out a better way. All images are 24x24x3 and there’s 10,000 punks. (Supposedly, there will only ever be 10k punks.) Pytorch needs this in a 10,000x3x24x24 array.\n\nfinalArray = np.empty((10000, 3, 24, 24))\nfor i in range(100):\n  for j in range(100):\n    a, b = 24 * i, 24 * (i + 1)  \n    c, d = 24 * j, 24 * (j + 1) \n    idx = j + i * (100)\n    finalArray[idx,0,:,:] = imgArray[a:b,c:d,0]\n    finalArray[idx,1,:,:] = imgArray[a:b,c:d,1]\n    finalArray[idx,2,:,:] = imgArray[a:b,c:d,2]\n\nn = finalArray.shape[0]\ntrainFraction = .75\nsample = np.random.uniform(size = n) &lt; trainFraction\nx_train = finalArray[ sample, :, :, :] / 255\nx_test =  finalArray[~sample, :, :, :] / 255\nprint([x_train.shape, x_test.shape])\n\n\n## To get the trainLoader to work, I had to just load in the torch tensor\n## in the previous example we had both data and labels. This time we just \n## have data\ntrainLoader = torch.utils.data.DataLoader(torch.Tensor(x_train), batch_size = 100, shuffle = False, num_workers = 1)\n\n[(7489, 3, 24, 24), (2511, 3, 24, 24)]\n\n\nNow let’s create our encoder/decoder. Here we’re going to use a simple approach for just flattening the images, one dense layer, they decoding. Network construction is its own thing in neural networks. The way we’re doing our network is a convolutional autoencoder. Typically, these have the number of channels increase while the image size decreases in each convolutional layer. The middle layer is called the embedding. We have to get the dimensions to match up with a combination of kernels, pooling, and varying the stride length.\nOur network construction is as follows:\nChannels: 3 -&gt; 6 -&gt; 12 -&gt; 6 -&gt; 3\nImage dims: (24, 24) -&gt; (20, 20) -&gt; (10, 10) -&gt; (6, 6) -&gt; (3, 3) -&gt; (6, 6) -&gt; (10, 10) -&gt; (24, 24)\n\nkernel_size = 5\nclass autoencoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.conv1 = nn.Conv2d(3, 6, kernel_size)\n        self.conv2 = nn.Conv2d(6, 12, kernel_size)\n        self.pool  = nn.MaxPool2d(2, 2)\n        self.iconv1 = nn.ConvTranspose2d(12, 6, kernel_size+1, stride = 2)\n        self.iconv2 = nn.ConvTranspose2d(6, 3, kernel_size+1, stride = 2)\n\n    def encode(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        return x\n    \n    def decode(self, x):\n        x = F.relu(self.iconv1(x))\n        ## Use the sigmoid as the final layer \n        ## since we've normalized pixel values to be between 0 and 1\n        x = torch.sigmoid(self.iconv2(x))\n        return(x)\n    \n    def forward(self, x):\n        return self.decode(self.encode(x))\n    \nautoencoder = autoencoder()\n\nNow we’ve constructed our network, let’s try it out. I’m going to first do the encoder, then the decoder, then through the entire network. We’ll check out the dimensions to make sure everything works.\n\n## Here's some example data by grabbing one batch\ntryItOut = next(iter(trainLoader))\nprint(tryItOut.shape)\n\n## Let's encode that data\nencoded = autoencoder.encode(tryItOut)\nprint(encoded.shape)\n\n## Now let's decode the encoded data\ndecoded = autoencoder.decode(encoded)\nprint(decoded.shape)\n\n## Now let's run the whole thing through\nfedForward = autoencoder.forward(tryItOut)\nprint(fedForward.shape)\n\ntorch.Size([100, 3, 24, 24])\ntorch.Size([100, 12, 3, 3])\ntorch.Size([100, 3, 24, 24])\ntorch.Size([100, 3, 24, 24])\n\n\n\ntest = fedForward.detach().numpy()\n\n## Plot out the first 5 images, note this isn't very interesting, since\n## all of the weights haven't been trained\nplt.figure(figsize=(10,5))\nfor i in range(5): \n  plt.subplot(1, 5,i+1)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(test[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n\n\n\nLet’ see how we do on our images. We’ll run the algorithm for 500 epochs\n\n#Optimizer\noptimizer = torch.optim.Adam(autoencoder.parameters(), lr = 0.001)\n\n#Epochs\nn_epochs = 20\n\nautoencoder.train()\n\n\nfor epoch in range(n_epochs):\n    for data in trainLoader:\n        images = data\n        optimizer.zero_grad()\n        outputs = autoencoder.forward(images)\n        loss = F.mse_loss(outputs, images)\n        loss.backward()\n        optimizer.step()\n\nNow that we’ve run it, let’s feed a collection of training images through the convnet and see how we did. The top row is the first 5 images of the last training epoch, last batch, and the bottom 5 is those images passed through the algorithm.\n\n## the data from the last iteration is called images\ntrainSample = images.detach().numpy()\n\nplt.figure(figsize=(10,5))\nfor i in range(5): \n  plt.subplot(1, 5,i+1)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(trainSample[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n## the output from the last iterations (feed forward through the network) is called outputs\ntrainOutput = outputs.detach().numpy()\n\nplt.figure(figsize=(10,5))\nfor i in range(5): \n  plt.subplot(2, 5,i+6)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(trainOutput[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n\n\n\n\n\n\n\ntestLoader  = torch.utils.data.DataLoader(torch.Tensor(x_test), batch_size = 100, shuffle = False, num_workers = 1)\ntestSample =  autoencoder.forward(next(iter(testLoader))).detach().numpy()\n\n\nplt.figure(figsize=(10,4))\n\n## Plot the original data\nfor i in range(5): \n  plt.subplot(2, 5, i + 1)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(x_test[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n# Plot the data having been run throught the convolutional autoencoder\nfor i in range(5): \n  plt.subplot(2, 5, i + 6)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(testSample[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n\n\n\nNot bad? One way to think about the convolutional autoencoder is as a compression algorithm. The data starts out as 24x24x3 = 1,728 floats. In contrast, our middle layer is 12 x 12 x 3 = 432 floats (25% of the original size). That is, it requires 75% fewer numbers to store the data as the embedding. You could imagine a sender having access to the encoder and the receiver having access to the decoder and then using the 75% reduction to send the data more efficiently."
  },
  {
    "objectID": "specialized_autoencoder2.html",
    "href": "specialized_autoencoder2.html",
    "title": "37  Autoencoder example",
    "section": "",
    "text": "Autoencoders are another unsupervised learning technique. Autoencoders take in a record and spit out a prediction of the same size. The goal is to represent the records as a NN. In an incomplete autoencoder, the model is regularized by the embedding (middle) layer being much lower than the input dimension. In this way, an autoencoder is a dimension reduction technique, reducing the input dimension size downto a much lower size (the encoder) then back out to the original size (the decoder). We can represent the autoencoder with a network diagram as below.\n\n\n\n\n\nLet \\(\\phi\\) be the first two layers of the network and \\(\\theta\\) be the last two. So, if we wanted to pass a data row, \\(x_i\\) through the network we would do \\(\\theta(\\phi(x_i))\\). We would call the network \\(\\phi\\) as the encoding network and \\(\\theta\\) as the decoding network. Consider training the network by minimizing\n\\[\n\\sum_{i=1}^n || x_i - \\theta(\\phi(x_i)) ||^2\n\\]\nover the weights. This sort of network is called an autoencoder. Notice that the same data is the input and output of the network. This kind of learning is called unsupervised, since we’re not trying to use \\(x\\) to predict an outcome \\(y\\). Instead, we’re trying to explore variation and find interesting features in \\(x\\) as a goal in itself without a “supervising” outcome, \\(y\\), to help out.\nNotice overfitting concerns are somewhat different in this network construction. If this model fits well, then it’s suggesting that 2 numbers can explain 8. That is, the network will have reduced the inputs to only two dimensions, that we could visualize for example. That is a form of parsimony that prevents overfitting. The middle layer is called the embedding. It is called this because an autoencoder is a form of non-linear embedding of our data into a lower dimensionional space.\nThere’s nothing to prevent us from having convolutional layers if the inputs are images. That’s what we’ll work on here. For convolutional autoencoders, it’s typical to increase the number of channels and decrease the image sizes as one works through the network.\n\n37.0.1 PCA and autoencoders\nWithout modification, autoencoders can be programmed that span the same space as PCA/SVD (plaut2018principal?). Enforcing the orthogonality requires something like adding Lagrange terms to the loss function. There’s no reason why you would do this, since PCA is well developed and works just fine. However, it does suggest why NNs are such a large class of models.\nLet \\(X_i\\) be a collection of features for record \\(i\\). Then, the SVD approximates the data matrix \\(X\\) with \\(UV^t\\), where we’ve absorbed the singular values into either \\(U\\) or \\(V\\). Per record, this model for \\(K\\) components and column \\(k\\) from \\(V\\) of \\(v_k\\).\n\\[\n\\hat x_i = \\sum_{k=1}^K &lt;x_i, v_k&gt; v_k\n\\]\nTherefore, consider a neural network that specifies that the first layer defines \\(K\\) hidden units as\n\\[\nh_{ik} = &lt;x_i, v_k&gt;.\n\\]\nThat is, it has a linear activation function with no bias term and weights \\(v_{jk}\\) where \\(v_{jk}\\) is element \\(j\\) of vector \\(v_k\\). Then consider an output layer that defines\n\\[\n\\hat x_{ij} = \\sum_{k=1}^K h_{ik} v_{jk},\n\\]\nAgain, this is a linear activation function with weights \\(v_{jk}\\). So, we arrive at the conclusion, that PCA is an example of an autoencoder with two layers, constraints on the weights being common to both layers, and constraints on the loss function that enforces the orthonormality of the \\(v_k\\). Of course, as we saw with ordinary regression, whether or not we can actually get gradient descent to converge remains a harder issue than just using PCA directly. Furthermore, the autoencoder wouldn’t necessarily order the PCs similarly.\nFinally, we see that a two layer autoencoder -without the constraints- contains the PCA fit as a special case and spans the same space as the PCA fit. Similarly we see that such a two layer encoder is overspecified, as most NNs are.\n\n\n37.0.2 Example on dermamnist\nFirst, let’s set up our autoencoder by defining a python class then initializing it. We assume the imports and data loading from the chapter on PCA and ICA.\n\n\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\n\n\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\n\n\n\nkernel_size = 5\n\nclass autoencoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.conv1 = nn.Conv2d(3, 6, kernel_size)\n        self.conv2 = nn.Conv2d(6, 12, kernel_size)\n        self.pool  = nn.MaxPool2d(2, 2)\n        self.iconv1 = nn.ConvTranspose2d(12, 6, kernel_size+1, stride = 2)\n        self.iconv2 = nn.ConvTranspose2d(6, 3, kernel_size+1, stride = 2)\n\n    def encode(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        return x\n    \n    def decode(self, x):\n        x = F.relu(self.iconv1(x))\n        ## Use the sigmoid as the final layer \n        ## since we've normalized pixel values to be between 0 and 1\n        x = torch.sigmoid(self.iconv2(x))\n        return(x)\n    \n    def forward(self, x):\n        return self.decode(self.encode(x))\n    \nautoencoder = autoencoder()\n\nWe can try out the autoencoder by\n\n## Here's some example data by grabbing one batch\ntryItOut, _ = next(iter(train_loader))\nprint(tryItOut.shape)\n\n## Let's encode that data\nencoded = autoencoder.encode(tryItOut)\nprint(encoded.shape)\n\n## Now let's decode the encoded data\ndecoded = autoencoder.decode(encoded)\nprint(decoded.shape)\n\n## Now let's run the whole thing through\nfedForward = autoencoder.forward(tryItOut)\nprint(fedForward.shape)\n\ntorch.Size([128, 3, 28, 28])\ntorch.Size([128, 12, 4, 4])\ntorch.Size([128, 3, 28, 28])\ntorch.Size([128, 3, 28, 28])\n\n\n\ntest = fedForward.detach().numpy()\n\n## Plot out the first 5 images, note this isn't very interesting, since\n## all of the weights haven't been trained\nplt.figure(figsize=(10,5))\nfor i in range(5): \n  plt.subplot(1, 5,i+1)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(test[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n\n\n\n\n#Optimizer\noptimizer = torch.optim.Adam(autoencoder.parameters(), lr = 0.001)\n\n#Epochs\nn_epochs = 20\n\nautoencoder.train()\n\nfor epoch in range(n_epochs):\n    for data, _ in train_loader:\n        images = data\n        optimizer.zero_grad()\n        outputs = autoencoder.forward(images)\n        loss = F.mse_loss(outputs, images)\n        loss.backward()\n        optimizer.step()\n\n\n## the data from the last iteration is called images\ntrainSample = images.detach().numpy()\n\nplt.figure(figsize=(10,5))\nfor i in range(5): \n  plt.subplot(1, 5,i+1)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(trainSample[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n## the output from the last iterations (feed forward through the network) is called outputs\ntrainOutput = outputs.detach().numpy()\n\nplt.figure(figsize=(10,5))\nfor i in range(5): \n  plt.subplot(2, 5,i+6)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(trainOutput[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n\n\n\n\n\n\nOn a test batch\n\ntest_batch, _ = next(iter(test_loader))\nx_test = test_batch.detach().numpy()\ntestSample = autoencoder.forward(test_batch).detach().numpy()\n\nplt.figure(figsize=(10,4))\n\n## Plot the original data\nfor i in range(5): \n  plt.subplot(2, 5, i + 1)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(x_test[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n# Plot the data having been run throught the convolutional autoencoder\nfor i in range(5): \n  plt.subplot(2, 5, i + 6)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(testSample[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)"
  },
  {
    "objectID": "specialized_gan.html#gan-implementations",
    "href": "specialized_gan.html#gan-implementations",
    "title": "38  GANs",
    "section": "38.1 GAN implementations",
    "text": "38.1 GAN implementations\nGANs work by two parts. I’ll describe this by imagining breaking our autoencoder into two parts. Recall, our autoencoder diagram as seen below.\n\n\n\n\n\nAgain, consider breaking this digram into two parts. One that takes the embedding and spits out images (a generator) and one that takes in images and spits out guesses as to whether or not they are real (a discriminator). See below where the generator is on the left and the discriminator is on the right.\n\n\n\n\n\nGANs do something like the following. They generate data (H21 and H22 above) using a random number generator. These are passed through the generator to obtain simulated records (H41-H48). These fake records are concatenated with real records to be passed through the discriminator, which tries to guess whether the records are real or fake. The two networks are trained adversarially. That is, the generator has higher loss when it fails to fool the discriminator and the discriminator has higher loss when it fails to discriminate between real and fake records.\nThis sort of approach can be used for data of any type. But, it’s fun especially to do it using images. Some of the images generated from GANs are wild in how realistic looking they are. Let’s try to create a GAN to generate our cryptopunks.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport urllib.request\nimport PIL\n\n\n## Read in and organize the data\nimgURL = \"https://raw.githubusercontent.com/larvalabs/cryptopunks/master/punks.png\"\nurllib.request.urlretrieve(imgURL, \"cryptoPunksAll.jpg\")\nimg = PIL.Image.open(\"cryptoPunksAll.jpg\").convert(\"RGB\")\nimgArray = np.asarray(img)\nfinalArray = np.empty((10000, 3, 24, 24))\nfor i in range(100):\n  for j in range(100):\n    a, b = 24 * i, 24 * (i + 1)  \n    c, d = 24 * j, 24 * (j + 1) \n    idx = j + i * (100)\n    finalArray[idx,0,:,:] = imgArray[a:b,c:d,0]\n    finalArray[idx,1,:,:] = imgArray[a:b,c:d,1]\n    finalArray[idx,2,:,:] = imgArray[a:b,c:d,2]\n\nn = finalArray.shape[0]\nx_real = finalArray / 255\nx_real = torch.tensor(x_real.astype(np.float32))\nx_real.shape\n\ntorch.Size([10000, 3, 24, 24])\n\n\nFor the generator, we’ll use a similar construction as the decoding layer from our autoencoder chapter. For the discriminator, let’s use a similar network to the one we used in our convolutional NN chapter.\n\n## Define our constants for our networks\nkernel_size = 5\ngenerator_input_dim = [16, 3, 3]\n\n\nclass create_generator(nn.Module):\n    def __init__(self):\n        super().__init__()        \n        self.net = nn.Sequential(\n            nn.ConvTranspose2d(16, 128, 10, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(128, 3, 4, 2, 1, bias=False), \n            nn.Sigmoid(),\n        )\n    def forward(self, x):\n        return self.net(x)\n \n## Use the discriminator from the convnet chapter\nclass create_discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 12, 5)\n        self.fc1 = nn.Linear(12 * 3 * 3, 32)\n        self.fc2 = nn.Linear(32, 1)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x\n    \n        \ngenerator = create_generator()\ndiscriminator = create_discriminator()\n\nLet’s try out our generator. First, we’re going to generate n embeddings. Then we’ll feed them through the generator to obtain n images. Notice that they don’t look so good. This is because we haven’t trained out generator yet!\n\ntest_embedding = torch.randn([5]+generator_input_dim)\nx_fake = generator(test_embedding)\n\n## Plot out the first 5 images, note this isn't very interesting, since\n## all of the weights haven't been trained\nplt.figure(figsize=(10,5))\nfor i in range(5): \n  plt.subplot(1, 5,i+1)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(x_fake.detach().numpy()[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n\n\n\n\n# Our label convention, real_label = 1, fake_label = 0\n\nlr = 1e-4\n\n## y is n real images then n fake images\ny = torch.concat( (torch.ones(n), torch.zeros(n) ) ) \n\n## Set up optimizers\noptimizerD = optim.Adam(discriminator.parameters(), lr=lr)\noptimizerG = optim.Adam(generator.parameters(), lr=lr)\n\n## Set up the loss function\nloss_function = nn.BCELoss()\n\nA couple of details. First, to speed up the algorithm, I’m using random batches of 100. You can do this directly with torch’s dataloader, but I decided just to do it manually. Secondly, you have to run the algorithm for a long time. The code below just says 20, because that was the very last size I used. The results are of 4k epochs or so. Finally, note that we save the networks in progress and I wrote in some code that lets me restart the network with the saved states. So, if my program halts for any reason, I didn’t lose all of its progress.\n\nrandomBatchSize = .1\nn_epochs = 20\ntrainFraction = .1\n\nfor epoch in range(n_epochs):          \n    ## Generate the batch sample\n    sample = np.random.uniform(size = n) &lt; trainFraction\n    n_batch = np.sum(sample)\n\n    ## Generate the simulated embedding    \n    embedding = torch.randn([n_batch]+generator_input_dim)\n    \n    \n    ## Generate new fake images\n    x_fake = generator(embedding)\n    \n    ## train the discriminator\n    ## zero out the gradient\n    discriminator.zero_grad()\n\n    ## run the generated and fake images through the discriminator\n    yhat_fake = discriminator(x_fake.detach())\n    yhat_real = discriminator(x_real[sample,:, :, :])\n    ## Note you have to concatenate them in the same order as \n    ## the previous cell. Remember we did real then fake\n    yhat = torch.concat( (yhat_real, yhat_fake) ).reshape(-1)\n\n    ## Calculate loss on all-real batch \n    y = torch.concat( (torch.ones(n_batch), torch.zeros(n_batch) ) ) \n\n    discriminator_error = loss_function(yhat, y)\n\n    # Calculate gradients for D in backward pass\n    discriminator_error.backward(retain_graph = True)\n\n    # Update the discriminator\n    optimizerD.step()\n\n    ## Train the generator\n    ## zero out the gradient\n    generator.zero_grad()\n    ## The discriminator has been udpated, so push the data through the \n    ## new discriminator\n    yhat_fake = discriminator(x_fake)\n    ## Note the outcome for the generator is all ones even\n    ## though we're classifying real as 1 and fake as 0\n    ## In other words, we want the loss for the generator to be\n    ## based on how real-like the generated data is\n    generator_error = loss_function( yhat_fake,  torch.ones( (n_batch, 1) ) )\n    ## Calculate the backwards error\n    generator_error.backward(retain_graph = False)\n    # Update the discriminator\n    optimizerG.step()\n    \n    if (epoch + 1) % 10 == 0:  \n        ## print(epoch, end = \",\")\n        ## Save the state dictionary in progress\n        torch.save(generator.state_dict(), \"generator.pt\")\n        torch.save(discriminator.state_dict(), \"discriminator.pt\")\n\nHere’s 25 of our generated punks. It’s clearly getting there. Notice, some of the punks have an earring. Also, some have a slightly green tinge, presumably because of the green (zombie) punks in the dataset.\n\nplt.figure(figsize=(10,10))\nfor i in range(25): \n  plt.subplot(5, 5,i+1)\n#  plt.xticks([])\n#  plt.yticks([])\n  img = np.transpose(x_fake.detach().numpy()[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)"
  }
]